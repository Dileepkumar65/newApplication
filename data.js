// Define topics array with "Build from Scratch" philosophy
const topics = [
    // CS Foundations
    {
        id: 'cs_foundations',
        title: 'CS Foundations',
        description: 'Basic concepts and foundations of computer science',
        content: 'Computer Science foundations include the basic principles and concepts that underpin all of computing. These foundational elements provide the groundwork for understanding more complex computer systems and software development. Our approach follows the "Build from Scratch" philosophy, teaching you to understand and implement every layer of computing from first principles.',
        parentId: null
    },
    {
        id: 'boolean_logic',
        title: 'Boolean Logic & Digital Circuits',
        description: 'The fundamental building blocks of computation',
        content: '<h3>1. Boolean Algebra Fundamentals</h3>\n<p>Boolean algebra, developed by George Boole in the 19th century, provides the mathematical foundation for all digital systems. Unlike traditional algebra which works with continuous numerical values, Boolean algebra operates on binary values (true/false, 1/0) and defines operations that manipulate these values.</p>\n<p>The three fundamental Boolean operations are:</p>\n<ul>\n<li><strong>AND</strong>: Output is true only when both inputs are true</li>\n<li><strong>OR</strong>: Output is true when at least one input is true</li>\n<li><strong>NOT</strong>: Output is the opposite of the input (inverts the value)</li>\n</ul>\n<p>In Boolean algebra, these operations are rigorously defined with truth tables and algebraic laws that allow us to analyze and manipulate logical expressions. By understanding and applying Boolean algebra, we can design digital circuits that perform specific logical functions.</p>\n\n<h3>2. Implementing Logic with Transistors</h3>\n<p>Transistors are the physical devices that implement Boolean logic in hardware. A transistor can act as an electronically controlled switch, allowing or blocking the flow of current based on a control signal. By arranging transistors in specific configurations, we can create physical implementations of logical operations.</p>\n<p>The two main types of transistor logic families are:</p>\n<ul>\n<li><strong>TTL (Transistor-Transistor Logic)</strong>: Uses bipolar junction transistors</li>\n<li><strong>CMOS (Complementary Metal-Oxide-Semiconductor)</strong>: Uses field-effect transistors in complementary pairs</li>\n</ul>\n<p>When building from scratch, we\'ll examine how transistors function at the quantum level and how their physical properties enable them to implement logical operations. Rather than taking these components as black boxes, we\'ll understand exactly how electrons flow through semiconductor materials to create switching behavior.</p>\n\n<h3>3. Basic Logic Gates</h3>\n<p>Logic gates are the fundamental building blocks of digital circuits, directly implementing Boolean functions. Each type of gate performs a specific logical operation on its inputs.</p>\n<p>The basic gates include:</p>\n<ul>\n<li><strong>AND Gate</strong>: Output is high (1) only when all inputs are high</li>\n<li><strong>OR Gate</strong>: Output is high when at least one input is high</li>\n<li><strong>NOT Gate (Inverter)</strong>: Output is the opposite of the input</li>\n<li><strong>NAND Gate</strong>: Combines AND and NOT operations (universal gate)</li>\n<li><strong>NOR Gate</strong>: Combines OR and NOT operations (universal gate)</li>\n<li><strong>XOR Gate</strong>: Output is high when inputs are different</li>\n<li><strong>XNOR Gate</strong>: Output is high when inputs are the same</li>\n</ul>\n<p>We\'ll implement each of these gates from transistors, understanding their internal structure rather than treating them as prefabricated components. This approach reveals how complex logical functions can be built from simple electronic components.</p>\n\n<h3>4. Combinational Logic Circuits</h3>\n<p>Combinational circuits perform operations where the output depends only on the current input values, without any memory of previous states. These circuits combine multiple logic gates to implement more complex functions.</p>\n<p>Key combinational circuits include:</p>\n<ul>\n<li><strong>Multiplexers (MUX)</strong>: Select one of several input signals based on control inputs</li>\n<li><strong>Demultiplexers (DEMUX)</strong>: Route an input signal to one of several outputs</li>\n<li><strong>Encoders</strong>: Convert multiple input signals into a binary code</li>\n<li><strong>Decoders</strong>: Convert a binary code into multiple output signals</li>\n<li><strong>Adders</strong>: Perform binary addition on two or more numbers</li>\n<li><strong>Comparators</strong>: Compare two binary numbers and determine their relationship</li>\n</ul>\n<p>By building these circuits from basic gates (which we\'ve already built from transistors), we\'ll gain a comprehensive understanding of how complex logical operations can be implemented in hardware.</p>\n\n<h3>5. Sequential Logic and Memory</h3>\n<p>While combinational logic computes based solely on current inputs, sequential logic allows circuits to have memory and maintain state over time. This is achieved through feedback loops and storage elements.</p>\n<p>The fundamental sequential elements include:</p>\n<ul>\n<li><strong>Latches</strong>: Basic storage elements that hold a value as long as enabled</li>\n<li><strong>Flip-flops</strong>: Edge-triggered storage elements that change state only at specific clock transitions</li>\n<li><strong>Registers</strong>: Groups of flip-flops that store multiple bits of information</li>\n<li><strong>Counters</strong>: Sequential circuits that cycle through a sequence of states</li>\n</ul>\n<p>We\'ll build these sequential components from scratch using our fundamental gates, understanding how feedback and timing create memory in electronic systems. This forms the bridge between combinational logic and more complex computational systems with state.</p>',
        parentId: 'cs_foundations'
    },
    {
        id: 'information_theory',
        title: 'Information Theory & Representation',
        description: 'How information is quantified, stored, and processed',
        content: '<h3>1. Foundations of Information Theory</h3>\n<p>Information theory, developed by Claude Shannon in the 1940s, provides a mathematical framework for quantifying information and understanding the fundamental limits of data compression and transmission. This revolutionary framework bridges mathematics, physics, and computer science.</p>\n<p>Key concepts in information theory include:</p>\n<ul>\n<li><strong>Information and Uncertainty</strong>: Information resolves uncertainty; the more uncertain an outcome, the more information is gained when learning it</li>\n<li><strong>Probability Distributions</strong>: The likelihood of different messages or symbols appearing in a communication stream</li>\n<li><strong>Surprise Value</strong>: Rare events carry more information than common ones</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, deriving the mathematical relationships rather than accepting them as given, building a strong foundation for understanding how information can be measured, stored, and communicated efficiently.</p>\n\n<h3>2. Measuring Information: Entropy</h3>\n<p>Entropy is the fundamental measure of information content, quantifying the average amount of uncertainty in a random variable or data source. It establishes theoretical limits on data compression and transmission.</p>\n<p>We\'ll explore several key measures:</p>\n<ul>\n<li><strong>Shannon Entropy</strong>: The average information content per symbol in a source, measured in bits</li>\n<li><strong>Joint Entropy</strong>: The total entropy of two or more random variables considered together</li>\n<li><strong>Conditional Entropy</strong>: The remaining uncertainty in one variable when the other is known</li>\n<li><strong>Relative Entropy (KL Divergence)</strong>: A measure of the difference between two probability distributions</li>\n<li><strong>Mutual Information</strong>: The information shared between two variables, indicating their dependence</li>\n</ul>\n<p>By implementing these measures from scratch, we\'ll develop a deep understanding of how to quantify information in any system, from text documents to quantum states, without relying on existing information theory libraries or abstractions.</p>\n\n<h3>3. Binary Representation & Coding</h3>\n<p>All information in digital systems must ultimately be represented as sequences of binary digits (bits). This section explores how various types of information are encoded in binary and the fundamental trade-offs in different coding schemes.</p>\n<p>We\'ll examine various coding systems:</p>\n<ul>\n<li><strong>Fixed-Length Codes</strong>: Each symbol uses the same number of bits (e.g., ASCII, Unicode)</li>\n<li><strong>Variable-Length Codes</strong>: Different symbols use different numbers of bits based on their probability (e.g., Morse code, Huffman coding)</li>\n<li><strong>Prefix Codes</strong>: No codeword is a prefix of another, enabling unambiguous decoding</li>\n<li><strong>Source Coding</strong>: Techniques to represent information using fewer bits while preserving meaning</li>\n</ul>\n<p>We\'ll implement these coding schemes from scratch, building encoders and decoders that transform information between human-readable forms and efficient binary representations, developing the skills to create custom encoding systems for any information type.</p>\n\n<h3>4. Data Compression Algorithms</h3>\n<p>Data compression reduces the size of information for efficient storage and transmission, applying information theory principles to practical systems. This section focuses on lossless compression techniques that allow perfect reconstruction of the original data.</p>\n<p>We\'ll implement several key compression algorithms from scratch:</p>\n<ul>\n<li><strong>Run-Length Encoding</strong>: Compressing sequences of repeated values</li>\n<li><strong>Huffman Coding</strong>: Creating optimal variable-length prefix codes based on symbol frequencies</li>\n<li><strong>Arithmetic Coding</strong>: Encoding entire messages as fractional numbers for near-optimal compression</li>\n<li><strong>Dictionary Methods</strong>: LZ77, LZ78, and LZW algorithms that build dictionaries of repeated patterns</li>\n<li><strong>Context Modeling</strong>: Using previous symbols to predict and efficiently encode the next symbol</li>\n</ul>\n<p>By building these algorithms from their mathematical foundations rather than using existing libraries, we\'ll understand exactly how each technique works, their computational complexity, and how they approach the theoretical limits established by information theory.</p>\n\n<h3>5. Digital Media Representation</h3>\n<p>This section explores how complex forms of information like text, images, audio, and video are represented in digital systems, applying information theory principles to real-world media formats.</p>\n<p>We\'ll examine representation schemes for various media types:</p>\n<ul>\n<li><strong>Text Encoding</strong>: Character sets (ASCII, Unicode), text compression, and search structures</li>\n<li><strong>Image Representation</strong>: Bitmap formats, color spaces, spatial/frequency domain representations</li>\n<li><strong>Audio Encoding</strong>: Sampling, quantization, time and frequency domain representations</li>\n<li><strong>Video Formats</strong>: Frame-based representation, motion compensation, temporal compression</li>\n</ul>\n<p>For each media type, we\'ll implement basic encoders and decoders from scratch, understanding how information theory principles lead to efficient representations. This approach avoids relying on existing media libraries, instead building the fundamental algorithms that underlie all digital media systems.</p>',
        parentId: 'cs_foundations'
    },
    {
        id: 'computation_models',
        title: 'Models of Computation',
        description: 'Theoretical frameworks that define computation',
        content: '<h3>1. Foundations of Computation Theory</h3>\n<p>Computation theory examines what problems can be solved algorithmically and how efficiently they can be solved. This foundation is essential for understanding the fundamental capabilities and limitations of computing systems.</p>\n<p>Key concepts in computation theory include:</p>\n<ul>\n<li><strong>Algorithms</strong>: Step-by-step procedures for solving problems</li>\n<li><strong>Decidability</strong>: Whether a problem has an algorithmic solution</li>\n<li><strong>Tractability</strong>: Whether a problem can be solved efficiently</li>\n<li><strong>Reduction</strong>: Transforming one problem into another to relate their complexity</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, examining the nature of algorithms and computation before introducing formal models. This approach provides a philosophical and intuitive foundation for the more rigorous mathematical structures that follow.</p>\n\n<h3>2. Finite Automata & Regular Languages</h3>\n<p>Finite automata are the simplest models of computation, capable of recognizing regular languages. They provide a foundation for understanding more complex computational models and are widely used in lexical analysis, pattern matching, and digital circuit design.</p>\n<p>We\'ll explore several types of finite automata:</p>\n<ul>\n<li><strong>Deterministic Finite Automata (DFA)</strong>: Machines where each state has exactly one transition for each input symbol</li>\n<li><strong>Nondeterministic Finite Automata (NFA)</strong>: Machines that can have multiple possible transitions for a single input</li>\n<li><strong>Regular Expressions</strong>: A compact notation for describing regular languages</li>\n<li><strong>State Minimization</strong>: Techniques for finding the smallest equivalent automaton</li>\n</ul>\n<p>We\'ll implement simulators for these machines from scratch, building a library that can define, visualize, and execute finite automata on arbitrary inputs. This implementation will avoid existing automata libraries, instead developing the algorithms and data structures directly from their mathematical definitions.</p>\n\n<h3>3. Context-Free Languages & Pushdown Automata</h3>\n<p>Context-free languages extend regular languages with recursive structures, making them appropriate for modeling programming languages, nested expressions, and natural language syntax. Pushdown automata add a stack to finite automata, providing the memory needed to recognize these languages.</p>\n<p>Key aspects we\'ll explore include:</p>\n<ul>\n<li><strong>Context-Free Grammars (CFG)</strong>: Formal systems for generating languages with recursive structure</li>\n<li><strong>Derivations and Parse Trees</strong>: Representations of how strings are generated by a grammar</li>\n<li><strong>Pushdown Automata (PDA)</strong>: Finite automata augmented with an infinite stack</li>\n<li><strong>Parsing Algorithms</strong>: Top-down (recursive descent, LL) and bottom-up (LR) approaches</li>\n</ul>\n<p>We\'ll implement parsers and pushdown automata simulators from scratch, building both theoretical models and practical parsing tools without relying on parser generators or existing libraries. This approach provides deep insight into the structure of programming languages and compilers.</p>\n\n<h3>4. Turing Machines & Computability</h3>\n<p>Turing machines represent the most powerful general model of computation, capturing the full range of algorithmic processes. They define the boundary between computable and uncomputable problems and form the theoretical foundation of modern computers.</p>\n<p>We\'ll explore several aspects of Turing machines and computability:</p>\n<ul>\n<li><strong>Turing Machine Definition</strong>: Formal structure with unlimited memory tape and finite control</li>\n<li><strong>Universal Turing Machines</strong>: Machines that can simulate any other Turing machine</li>\n<li><strong>The Church-Turing Thesis</strong>: The hypothesis that Turing machines capture all effective computation</li>\n<li><strong>Undecidable Problems</strong>: The Halting Problem and other problems with no algorithmic solution</li>\n<li><strong>Reductions</strong>: Techniques for proving problems undecidable</li>\n</ul>\n<p>We\'ll implement Turing machine simulators from scratch, building systems that can define, visualize, and execute these theoretical machines on arbitrary inputs. This implementation will provide concrete insight into these abstract models, making their theoretical properties tangible through direct experimentation.</p>\n\n<h3>5. Computational Complexity</h3>\n<p>Computational complexity theory classifies problems based on the resources (time, space) required to solve them, establishing a hierarchy of complexity classes that organizes problems by their inherent difficulty.</p>\n<p>Key concepts in complexity theory include:</p>\n<ul>\n<li><strong>Time and Space Complexity</strong>: How resource requirements scale with input size</li>\n<li><strong>Complexity Classes</strong>: P, NP, PSPACE, and their relationships</li>\n<li><strong>NP-Completeness</strong>: Problems that are as hard as any in NP</li>\n<li><strong>Polynomial-Time Reductions</strong>: Transformations that preserve complexity</li>\n<li><strong>Approximation Algorithms</strong>: Efficient approaches for intractable problems</li>\n</ul>\n<p>We\'ll implement algorithms for classic problems from different complexity classes, analyzing their performance and understanding the fundamental reasons why some problems require exponentially more resources than others. This approach grounds abstract complexity theory in concrete implementations, providing insight into the practical implications of theoretical complexity barriers.</p>',
        parentId: 'cs_foundations'
    },
    
    // Computer Hardware
    {
        id: 'hardware',
        title: 'Computer Hardware',
        description: 'Physical components and architecture of computers',
        content: 'Computer hardware encompasses all the physical components that make up a computer system. Understanding hardware is essential for optimizing performance and troubleshooting issues in computing systems. Our "Build from Scratch" approach means learning about every layer of hardware, from transistors and logic gates up to complete computer systems, emphasizing how each component can be understood from first principles.',
        parentId: null
    },
    {
        id: 'transistors',
        title: 'Transistors & Logic Gates',
        description: 'The fundamental building blocks of digital systems',
        content: '<h3>1. Semiconductor Physics Fundamentals</h3>\n<p>At the heart of all modern computing lies semiconductor physics, which enables the creation of transistors. This section explores the physical principles that make computing possible at the atomic level.</p>\n<p>Key concepts in semiconductor physics include:</p>\n<ul>\n<li><strong>Atomic Structure</strong>: Electron shells, valence electrons, and how atoms bond together</li>\n<li><strong>Band Theory</strong>: Energy bands in solids, conduction bands, valence bands, and band gaps</li>\n<li><strong>Semiconductor Materials</strong>: Silicon, germanium, and compound semiconductors</li>\n<li><strong>Doping</strong>: Adding impurities to create n-type and p-type semiconductors</li>\n<li><strong>Carrier Transport</strong>: How electrons and holes move through semiconductor materials</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, starting with quantum mechanics and atomic theory, building up to practical semiconductor behavior. This approach provides a strong foundation for understanding the physical layer of computing without relying on simplified abstractions.</p>\n\n<h3>2. Diodes and PN Junctions</h3>\n<p>Before understanding transistors, we must understand the PN junction, which forms the basis of semiconductor diodes and is a crucial component of transistors.</p>\n<p>Key aspects of PN junctions include:</p>\n<ul>\n<li><strong>Junction Formation</strong>: How p-type and n-type semiconductors interact when placed together</li>\n<li><strong>Depletion Region</strong>: The zone where free carriers are removed through diffusion</li>\n<li><strong>Forward and Reverse Bias</strong>: How applied voltage affects current flow through the junction</li>\n<li><strong>IV Characteristics</strong>: The nonlinear relationship between current and voltage</li>\n<li><strong>Junction Capacitance</strong>: How the junction stores charge and responds to changing voltages</li>\n</ul>\n<p>We\'ll analyze these junctions with mathematical models and build simple circuits to observe their behavior firsthand, understanding them from the atomic level up rather than as idealized components.</p>\n\n<h3>3. Transistor Types and Operation</h3>\n<p>Transistors are semiconductor devices that can amplify signals and act as electronic switches, forming the fundamental building blocks of all computing systems.</p>\n<p>We\'ll explore different transistor types and their operational principles:</p>\n<ul>\n<li><strong>Bipolar Junction Transistors (BJTs)</strong>: NPN and PNP structures, current amplification, operating regions</li>\n<li><strong>Field-Effect Transistors (FETs)</strong>: JFETs and MOSFETs, voltage control, channel formation</li>\n<li><strong>CMOS Technology</strong>: Complementary MOSFET pairs that form the basis of modern digital ICs</li>\n<li><strong>Transistor Parameters</strong>: Gain, frequency response, switching speed, power dissipation</li>\n<li><strong>Small-Signal Models</strong>: Mathematical representations for analyzing transistor behavior</li>\n</ul>\n<p>We\'ll implement circuits with discrete transistors, measuring their characteristics and understanding how their physical properties determine their electronic behavior, rather than simply using them as black-box components.</p>\n\n<h3>4. Digital Logic Implementation</h3>\n<p>This section explores how transistors are arranged to implement Boolean logic functions, creating the fundamental gates that form the basis of all digital systems.</p>\n<p>We\'ll examine different logic families and their implementations:</p>\n<ul>\n<li><strong>Resistor-Transistor Logic (RTL)</strong>: Simple but inefficient early implementation</li>\n<li><strong>Diode-Transistor Logic (DTL)</strong>: Improved speed with diodes for input processing</li>\n<li><strong>Transistor-Transistor Logic (TTL)</strong>: Faster switching with multiple transistors</li>\n<li><strong>CMOS Logic</strong>: Low power, high density implementation dominating modern design</li>\n<li><strong>Logic Levels and Noise Margins</strong>: How voltage ranges represent binary values reliably</li>\n</ul>\n<p>For each logic family, we\'ll build actual gates from individual transistors, measuring their performance characteristics and understanding the trade-offs between speed, power, and complexity, without relying on integrated circuits.</p>\n\n<h3>5. From Transistors to Integrated Circuits</h3>\n<p>Modern computing depends on the ability to integrate billions of transistors onto a single chip. This section explores how individual transistors evolved into large-scale integrated circuits.</p>\n<p>Key aspects of IC technology include:</p>\n<ul>\n<li><strong>Fabrication Processes</strong>: Photolithography, etching, doping, and metallization</li>\n<li><strong>Process Scaling</strong>: How transistors have shrunk from micrometers to nanometers</li>\n<li><strong>Moore\'s Law</strong>: The historical trend and physical limits of transistor scaling</li>\n<li><strong>Design Rules</strong>: Constraints that determine what can be physically manufactured</li>\n<li><strong>Packaging and Interconnects</strong>: How chips are connected to the outside world</li>\n</ul>\n<p>We\'ll examine detailed fabrication steps and design simple integrated circuits at the transistor level, understanding the physical and economic factors that shape modern chips rather than treating ICs as mysterious black boxes.</p>',
        parentId: 'hardware'
    },
    {
        id: 'digital_circuits',
        title: 'Digital Circuits & Components',
        description: 'Building useful circuits from basic logic gates',
        content: '<h3>1. Fundamentals of Digital Circuit Design</h3>\n<p>Digital circuit design is the process of creating electronic circuits that operate on binary signals, forming the foundation of all computing hardware. This section covers the essential principles and methodologies for designing reliable digital systems.</p>\n<p>Key concepts in digital design include:</p>\n<ul>\n<li><strong>Boolean Algebra in Practice</strong>: Applying Boolean algebra to simplify and optimize circuit designs</li>\n<li><strong>Circuit Minimization</strong>: Karnaugh maps and Quine-McCluskey algorithm for logic simplification</li>\n<li><strong>Timing Analysis</strong>: Propagation delay, setup/hold times, and critical paths</li>\n<li><strong>Noise and Interference</strong>: Design techniques to ensure signal integrity</li>\n<li><strong>Power Considerations</strong>: Static and dynamic power consumption optimization</li>\n</ul>\n<p>We\'ll apply these concepts by designing circuits on paper and then implementing them with physical components, focusing on understanding both theoretical and practical aspects of digital design from first principles.</p>\n\n<h3>2. Combinational Logic Circuits</h3>\n<p>Combinational circuits produce outputs that depend only on the current inputs, with no memory of past states. These circuits perform the fundamental data processing operations in a computer.</p>\n<p>We\'ll design and build key combinational components:</p>\n<ul>\n<li><strong>Adders</strong>: Half adders, full adders, and ripple-carry adders for binary arithmetic</li>\n<li><strong>Multiplexers (MUX)</strong>: Circuits that select one output from multiple inputs based on control signals</li>\n<li><strong>Demultiplexers (DEMUX)</strong>: Circuits that route a single input to one of multiple outputs</li>\n<li><strong>Encoders and Decoders</strong>: Circuits that convert between binary codes and one-hot representations</li>\n<li><strong>Comparators</strong>: Circuits that compare binary numbers and indicate relationships (equal, greater, less)</li>\n</ul>\n<p>For each component, we\'ll implement multiple design approaches using only basic gates (AND, OR, NOT) rather than using integrated circuit packages, understanding the trade-offs between design complexity, performance, and resource usage.</p>\n\n<h3>3. Sequential Logic Elements</h3>\n<p>Sequential circuits contain memory elements that store information over time, allowing systems to maintain state. These elements are essential for creating computers that can execute multi-step operations.</p>\n<p>We\'ll explore the fundamental sequential components:</p>\n<ul>\n<li><strong>Latches</strong>: SR, gated D, and other basic storage elements</li>\n<li><strong>Flip-flops</strong>: Edge-triggered storage devices (D, JK, T types)</li>\n<li><strong>Registers</strong>: Arrays of flip-flops that store multiple bits of information</li>\n<li><strong>Shift Registers</strong>: Serial-to-parallel and parallel-to-serial conversion</li>\n<li><strong>Master-Slave Designs</strong>: Preventing race conditions in sequential circuits</li>\n</ul>\n<p>We\'ll build each of these components from scratch using only basic gates, analyzing their timing behavior, metastability issues, and how their physical implementation affects their performance characteristics.</p>\n\n<h3>4. Synchronous Sequential Circuits</h3>\n<p>Synchronous sequential circuits use clock signals to coordinate state changes, enabling complex, deterministic behavior essential for computing systems. These circuits form the core of control units and data paths in processors.</p>\n<p>Key synchronous circuit types we\'ll implement include:</p>\n<ul>\n<li><strong>Counters</strong>: Asynchronous and synchronous designs (binary, decade, Johnson, etc.)</li>\n<li><strong>State Machines</strong>: Moore and Mealy machine implementations</li>\n<li><strong>Sequencers</strong>: Programmable circuits that generate control signals in sequence</li>\n<li><strong>Pulse Generators</strong>: Circuits for creating precisely timed control signals</li>\n<li><strong>Clock Distribution</strong>: Techniques for delivering reliable clock signals throughout a system</li>\n</ul>\n<p>Our implementations will focus on understanding timing requirements, hazard avoidance, and the relationship between state diagrams, state tables, and actual circuit implementations, all built from basic gates rather than using specialized ICs.</p>\n\n<h3>5. Advanced Digital Design Techniques</h3>\n<p>Beyond basic components, modern digital systems employ sophisticated techniques to improve performance, reliability, and functionality. This section explores design approaches used in complex digital systems.</p>\n<p>Advanced techniques we\'ll explore include:</p>\n<ul>\n<li><strong>Pipelining</strong>: Breaking operations into stages to increase throughput</li>\n<li><strong>Asynchronous Design</strong>: Creating systems without global clock signals</li>\n<li><strong>Hardware Description Languages</strong>: Using VHDL or Verilog to describe complex circuits</li>\n<li><strong>Fault-Tolerant Design</strong>: Creating circuits that can detect and correct errors</li>\n<li><strong>Testability</strong>: Designing circuits for ease of testing and verification</li>\n</ul>\n<p>We\'ll implement examples of these techniques with discrete components where possible and use hardware description languages for more complex designs, maintaining our focus on understanding principles rather than using pre-built components. This approach provides the foundation needed to design sophisticated digital systems from first principles.</p>',
        parentId: 'hardware'
    },
    {
        id: 'cpu_design',
        title: 'CPU Design & Architecture',
        description: 'Designing a functional CPU from basic components',
        content: '<h3>1. CPU Architecture Fundamentals</h3>\n<p>CPU architecture defines the organizational structure and operational principles of a processor. Understanding these foundational concepts is essential for designing functional processors from basic components.</p>\n<p>Key architectural concepts include:</p>\n<ul>\n<li><strong>Von Neumann Architecture</strong>: The classic stored-program computer with shared memory for instructions and data</li>\n<li><strong>Harvard Architecture</strong>: Systems with separate instruction and data memories for improved performance</li>\n<li><strong>CISC vs. RISC Designs</strong>: Trade-offs between complex and reduced instruction sets</li>\n<li><strong>Word Size and Address Space</strong>: How bit width affects addressable memory and processing capabilities</li>\n<li><strong>Execution Models</strong>: Sequential, pipelined, superscalar, and out-of-order execution approaches</li>\n</ul>\n<p>We\'ll explore these architectures by designing simple versions of each, comparing their characteristics and understanding the historical and technical factors that influenced their development, focusing on first principles rather than existing commercial architectures.</p>\n\n<h3>2. Instruction Set Architecture (ISA)</h3>\n<p>The instruction set architecture is the interface between hardware and software, defining the processor\'s capabilities from the programmer\'s perspective. Designing an effective ISA involves carefully balancing expressiveness, efficiency, and implementability.</p>\n<p>Our ISA design process will cover:</p>\n<ul>\n<li><strong>Instruction Formats</strong>: Designing fixed and variable-length encoding schemas</li>\n<li><strong>Addressing Modes</strong>: Methods for specifying operand locations (immediate, direct, indirect, indexed)</li>\n<li><strong>Operation Categories</strong>: Arithmetic, logical, data movement, control flow, and special instructions</li>\n<li><strong>Condition Codes</strong>: Status flags and their role in conditional operations</li>\n<li><strong>ISA Extensions</strong>: Specialized instructions for enhanced functionality (SIMD, crypto, etc.)</li>\n</ul>\n<p>Rather than copying existing instruction sets, we\'ll design our own minimal but complete ISA from first principles, carefully considering the trade-offs of each design decision and how it affects both hardware implementation and software development.</p>\n\n<h3>3. Arithmetic Logic Units (ALUs)</h3>\n<p>The ALU is the computational heart of a CPU, performing mathematical and logical operations on data. Creating an efficient ALU requires understanding how to implement various operations in digital logic.</p>\n<p>We\'ll design and implement ALU components including:</p>\n<ul>\n<li><strong>Integer Arithmetic</strong>: Adders, subtractors, and multipliers with various speed/area trade-offs</li>\n<li><strong>Logical Operations</strong>: AND, OR, XOR, and NOT circuits with bit-parallel processing</li>\n<li><strong>Shifters and Rotators</strong>: Barrel shifters and logic for bit manipulation operations</li>\n<li><strong>Comparators</strong>: Circuits for equality and magnitude comparison</li>\n<li><strong>Flags Generation</strong>: Circuits to detect zero, negative, carry, overflow, and parity conditions</li>\n</ul>\n<p>We\'ll build our ALU entirely from the digital components previously studied, avoiding pre-built ALU modules and understanding the trade-offs between different implementations in terms of speed, area, and power consumption.</p>\n\n<h3>4. Control Unit Design</h3>\n<p>The control unit orchestrates the operation of the CPU, generating the precise sequence of control signals needed to execute instructions. This component translates the ISA into the actual hardware behavior.</p>\n<p>We\'ll explore two main approaches to control unit design:</p>\n<ul>\n<li><strong>Hardwired Control</strong>: Implementing control logic directly with combinational circuits</li>\n<li><strong>Microprogrammed Control</strong>: Using a microcode ROM to store control sequences</li>\n<li><strong>Instruction Fetch and Decode</strong>: Circuits for retrieving and interpreting instructions</li>\n<li><strong>Pipeline Control</strong>: Managing the flow of instructions through multiple pipeline stages</li>\n<li><strong>Hazard Detection and Handling</strong>: Addressing data, control, and structural hazards</li>\n</ul>\n<p>Our implementation will include complete control units using both approaches, comparing their advantages and disadvantages, and understanding how they generate the precise timing and control signals required for correct CPU operation.</p>\n\n<h3>5. Building a Complete CPU</h3>\n<p>Bringing all components together to create a working CPU requires careful integration and validation. This final section focuses on system integration, testing, and validation of a complete processor.</p>\n<p>Key aspects of CPU integration include:</p>\n<ul>\n<li><strong>Data Path Design</strong>: Connecting ALU, registers, and memory interfaces</li>\n<li><strong>Register File Implementation</strong>: Creating efficient multi-port register storage</li>\n<li><strong>Memory Interfacing</strong>: Designing address decoders and data buses</li>\n<li><strong>System Timing</strong>: Ensuring proper clock distribution and signal synchronization</li>\n<li><strong>CPU Verification</strong>: Testing strategies to validate correctness at various abstraction levels</li>\n</ul>\n<p>We\'ll implement our complete CPU design using a hardware description language (HDL), following a systematic process from specification to simulation to synthesis. Unlike typical courses that use pre-designed CPUs, we\'ll build everything from the logic gates up, gaining deep insight into every aspect of processor operation.</p>',
        parentId: 'hardware'
    },
    {
        id: 'memory_systems',
        title: 'Memory Systems',
        description: 'From individual bits to complex memory hierarchies',
        content: '<h3>1. Memory Cell Technologies</h3>\n<p>At the heart of computer memory are the physical mechanisms for storing binary information. Different memory technologies offer various trade-offs in terms of speed, density, volatility, and cost.</p>\n<p>We\'ll explore several fundamental memory cell technologies:</p>\n<ul>\n<li><strong>Static RAM (SRAM)</strong>: Six-transistor cells that maintain state without refresh</li>\n<li><strong>Dynamic RAM (DRAM)</strong>: Single-transistor-capacitor cells requiring periodic refresh</li>\n<li><strong>Non-volatile Memories</strong>: Flash, EEPROM, and emerging technologies (MRAM, ReRAM, etc.)</li>\n<li><strong>Magnetic Storage</strong>: Hard disk drive technology and principles</li>\n<li><strong>Optical Storage</strong>: CD, DVD, and Blu-ray recording and reading mechanisms</li>\n</ul>\n<p>For each technology, we\'ll examine the physics of operation at the transistor or material level, implementing simple versions where possible and understanding their fundamental characteristics, limitations, and manufacturing processes rather than treating them as abstract components.</p>\n\n<h3>2. Memory Organization and Addressing</h3>\n<p>Memory organization determines how individual cells are arranged into addressable units and accessed by the processor. Effective organization balances capacity, speed, and power consumption.</p>\n<p>Key aspects of memory organization include:</p>\n<ul>\n<li><strong>Memory Arrays</strong>: Row and column decoders, sense amplifiers, and timing circuits</li>\n<li><strong>Address Mapping</strong>: How logical addresses translate to physical memory locations</li>\n<li><strong>Word Size and Alignment</strong>: Data organization and access restrictions</li>\n<li><strong>Error Detection and Correction</strong>: Parity, ECC, and other techniques</li>\n<li><strong>Multi-bank Organizations</strong>: Interleaved access for improved performance</li>\n</ul>\n<p>We\'ll design and implement memory subsystems from basic components, including address decoders, data buffers, and control logic, avoiding pre-built memory modules and understanding the detailed operation of memory arrays and their surrounding circuitry.</p>\n\n<h3>3. Memory Hierarchy and Caching</h3>\n<p>Memory hierarchies exploit locality of reference to bridge the speed gap between fast processors and slower main memory. Caching is the key technique that makes this hierarchy effective.</p>\n<p>Our implementation of memory hierarchies will cover:</p>\n<ul>\n<li><strong>Register Files</strong>: Processor-integrated multi-port memory systems</li>\n<li><strong>Cache Architectures</strong>: Direct-mapped, set-associative, and fully-associative designs</li>\n<li><strong>Cache Replacement Policies</strong>: LRU, FIFO, random, and other algorithms</li>\n<li><strong>Write Policies</strong>: Write-through vs. write-back strategies</li>\n<li><strong>Coherence Protocols</strong>: Maintaining consistency in multi-cache systems</li>\n</ul>\n<p>We\'ll implement complete cache controllers from digital logic components, simulating their behavior with realistic memory access patterns and measuring performance metrics like hit rate and average access time, building an intuitive understanding of locality and its exploitation.</p>\n\n<h3>4. Virtual Memory Systems</h3>\n<p>Virtual memory extends the memory hierarchy to disk storage, providing the illusion of a larger memory space than physically available and isolating processes from each other.</p>\n<p>Key aspects of virtual memory include:</p>\n<ul>\n<li><strong>Address Translation</strong>: Page tables, TLBs, and translation mechanisms</li>\n<li><strong>Page Replacement Algorithms</strong>: LRU, clock, working set, and other approaches</li>\n<li><strong>Memory Protection</strong>: Access rights enforcement and process isolation</li>\n<li><strong>Demand Paging</strong>: Loading pages only when needed</li>\n<li><strong>Memory-Mapped Files</strong>: Integrating file I/O with memory access</li>\n</ul>\n<p>We\'ll implement virtual memory controllers and memory management units (MMUs) from basic components, understanding the hardware-software interaction in modern memory systems and how operating systems use the hardware capabilities to provide protected, virtualized memory environments.</p>\n\n<h3>5. Memory Controllers and Interfaces</h3>\n<p>Memory controllers bridge the gap between processors and memory devices, handling the complex timing and protocol requirements of modern memory systems.</p>\n<p>Our exploration of memory interfaces will include:</p>\n<ul>\n<li><strong>DRAM Timing</strong>: RAS/CAS protocols, refresh scheduling, and timing parameters</li>\n<li><strong>Memory Bus Architectures</strong>: Parallel and serial interfaces (DDR, LPDDR, HBM)</li>\n<li><strong>DMA Controllers</strong>: Offloading memory transfers from the CPU</li>\n<li><strong>Memory-Mapped I/O</strong>: Interfacing with peripherals through memory addressing</li>\n<li><strong>Storage Controllers</strong>: Interfaces to persistent storage systems</li>\n</ul>\n<p>We\'ll implement memory controllers from digital logic, understanding the detailed timing requirements of various memory technologies and the complex trade-offs involved in modern memory system design. This hands-on approach reveals the challenges in bridging the speed gap between processors and memory while managing power, reliability, and cost constraints.</p>',
        parentId: 'hardware'
    },
    {
        id: 'io_systems',
        title: 'I/O Systems & Peripherals',
        description: 'Connecting computers to the outside world',
        content: '<h3>1. Fundamentals of I/O Interfaces</h3>\n<p>Input/Output interfaces form the boundary between a computer system and the external world. Understanding the principles of these interfaces is essential for designing systems that can effectively communicate with users and other devices.</p>\n<p>Key concepts in I/O interfaces include:</p>\n<ul>\n<li><strong>Signal Types and Levels</strong>: Digital vs. analog, voltage standards, and current loops</li>\n<li><strong>Data Transmission Modes</strong>: Serial, parallel, synchronous, and asynchronous</li>\n<li><strong>Addressing and Selection</strong>: Memory-mapped I/O vs. port-mapped I/O</li>\n<li><strong>Handshaking Protocols</strong>: Methods for coordinating data transfer between devices</li>\n<li><strong>Error Detection & Correction</strong>: Ensuring data integrity across interfaces</li>\n</ul>\n<p>We\'ll examine these concepts by designing and implementing basic I/O interfaces from discrete components, avoiding pre-built interface controllers and understanding how signals are physically transferred between digital systems.</p>\n\n<h3>2. Bus Architectures</h3>\n<p>Buses are the communication pathways that connect various components within a computer system. Understanding bus design involves balancing bandwidth, latency, cost, and compatibility requirements.</p>\n<p>Our exploration of bus architectures will include:</p>\n<ul>\n<li><strong>System Buses</strong>: Address, data, and control lines in computer architecture</li>\n<li><strong>Bus Topologies</strong>: Linear, star, hierarchical, and point-to-point connections</li>\n<li><strong>Arbitration Mechanisms</strong>: Resolving contention when multiple devices need access</li>\n<li><strong>Bus Timing</strong>: Synchronous vs. asynchronous protocols, wait states, and handshaking</li>\n<li><strong>Electrical Characteristics</strong>: Transmission line effects, termination, and signal integrity</li>\n</ul>\n<p>We\'ll implement simple but functional bus systems from basic digital components, understanding the timing diagrams, electrical requirements, and protocol specifications that govern reliable multi-device communication rather than using off-the-shelf bus controllers.</p>\n\n<h3>3. I/O Transaction Methods</h3>\n<p>Different methods for performing I/O transactions offer varying trade-offs between simplicity, efficiency, and CPU utilization. Understanding these methods is critical for designing systems with appropriate performance characteristics.</p>\n<p>We\'ll explore major I/O transaction methods:</p>\n<ul>\n<li><strong>Programmed I/O</strong>: CPU directly controlling data transfer through explicit instructions</li>\n<li><strong>Interrupt-Driven I/O</strong>: Devices signaling the CPU when they need service</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Peripheral-to-memory transfers without CPU intervention</li>\n<li><strong>Channel I/O</strong>: Specialized processors dedicated to managing I/O operations</li>\n<li><strong>Memory-Mapped I/O</strong>: Accessing device registers as memory locations</li>\n</ul>\n<p>For each method, we\'ll implement the necessary controllers and interface circuits from basic digital components, understanding the hardware-software interactions and measuring performance characteristics like throughput, latency, and CPU overhead.</p>\n\n<h3>4. Common Peripheral Interfaces</h3>\n<p>Modern computers support a wide range of peripherals through standardized interfaces. Understanding these interfaces involves analyzing their protocols, electrical specifications, and software interactions.</p>\n<p>We\'ll explore several key peripheral interfaces:</p>\n<ul>\n<li><strong>Serial Interfaces</strong>: UART, SPI, I2C, and USB implementation</li>\n<li><strong>Display Interfaces</strong>: VGA, DVI, HDMI, and DisplayPort protocols</li>\n<li><strong>Storage Interfaces</strong>: SATA, NVMe, SD, and storage protocol controllers</li>\n<li><strong>Network Interfaces</strong>: Ethernet, Wi-Fi, and basic physical layer implementation</li>\n<li><strong>Human Interface Devices</strong>: Keyboard, mouse, and game controller interfaces</li>\n</ul>\n<p>For each interface, we\'ll build simple controller circuits from digital logic components, understanding the precise timing requirements, protocol states, and physical layer considerations rather than using pre-built interface controllers.</p>\n\n<h3>5. I/O Software & Device Drivers</h3>\n<p>The hardware-software boundary in I/O systems requires carefully designed software that can efficiently manage devices while abstracting their complexities. This section bridges hardware design with systems programming.</p>\n<p>Key aspects of I/O software include:</p>\n<ul>\n<li><strong>Device Driver Architecture</strong>: Layered design and hardware abstraction</li>\n<li><strong>Polling vs. Interrupt Handling</strong>: Implementation strategies and trade-offs</li>\n<li><strong>Buffering Techniques</strong>: Single, double, and circular buffer implementations</li>\n<li><strong>Device Discovery & Configuration</strong>: Plug-and-play mechanisms and device enumeration</li>\n<li><strong>Error Handling & Recovery</strong>: Strategies for maintaining system reliability</li>\n</ul>\n<p>We\'ll implement basic device drivers for the peripheral controllers we\'ve built, writing software that directly manipulates hardware registers and responds to interrupts, avoiding high-level operating system APIs and understanding the complete hardware-software stack from first principles.</p>',
        parentId: 'hardware'
    },
    
    // Programming & Languages
    {
        id: 'programming',
        title: 'Programming & Languages',
        description: 'Programming concepts and language implementation',
        content: 'Programming is the process of creating instructions that tell a computer how to perform tasks. Rather than just learning to use existing programming languages, our "Build from Scratch" approach focuses on understanding how programming languages themselves work and are implemented. You\'ll learn to create your own languages, compilers, and interpreters, building a deep understanding of programming concepts from first principles.',
        parentId: null
    },
    {
        id: 'assembly',
        title: 'Assembly Language & Machine Code',
        description: 'The lowest level of programming',
        content: '<h3>1. Machine Code Fundamentals</h3>\n<p>Machine code is the raw binary instructions that a processor can directly execute. Understanding machine code requires knowledge of binary number systems, data representation, and processor architecture.</p>\n<p>Key aspects of machine code include:</p>\n<ul>\n<li><strong>Binary Representation</strong>: How instructions and data are encoded as bits</li>\n<li><strong>Instruction Formats</strong>: How operation codes, operands, and addressing modes are packed into binary</li>\n<li><strong>Endianness</strong>: Little-endian vs. big-endian byte ordering and its implications</li>\n<li><strong>Word Sizes</strong>: How the processor\'s bit width affects instruction and data formats</li>\n<li><strong>Instruction Pointer</strong>: How the processor keeps track of the next instruction to execute</li>\n</ul>\n<p>We\'ll explore these concepts by examining raw binary instructions, manually decoding them, and understanding how each bit influences processor behavior. Rather than using disassemblers or debuggers, we\'ll develop our own tools to visualize and manipulate machine code directly.</p>\n\n<h3>2. Assembly Language Syntax</h3>\n<p>Assembly language provides a human-readable representation of machine code, using mnemonics for instructions and symbolic names for memory locations. Understanding assembly syntax is the first step toward low-level programming.</p>\n<p>Our exploration of assembly syntax includes:</p>\n<ul>\n<li><strong>Instruction Mnemonics</strong>: Human-readable operation codes (e.g., MOV, ADD, JMP)</li>\n<li><strong>Operand Syntax</strong>: Register references, immediate values, and memory addressing modes</li>\n<li><strong>Directives</strong>: Special instructions to the assembler (e.g., for defining data, sections)</li>\n<li><strong>Labels</strong>: Symbolic names for memory locations and program branches</li>\n<li><strong>Comments and Documentation</strong>: Practices for making assembly code understandable</li>\n</ul>\n<p>We\'ll study multiple assembly language dialects, understanding both CISC and RISC assembly conventions, writing programs by hand rather than generating them from higher-level languages, and directly mapping the relationship between assembly instructions and the resulting machine code.</p>\n\n<h3>3. Programming in Assembly</h3>\n<p>Effective assembly programming requires understanding processor-specific instructions, registers, and memory management, as well as developing strategies for implementing higher-level constructs with low-level operations.</p>\n<p>Key assembly programming topics include:</p>\n<ul>\n<li><strong>Control Flow</strong>: Implementing conditionals, loops, and function calls</li>\n<li><strong>Data Structures</strong>: Arrays, records, stacks, and linked lists in assembly</li>\n<li><strong>Memory Management</strong>: Stack frames, heap allocation, and addressing modes</li>\n<li><strong>Bit Manipulation</strong>: Masking, shifting, and bitwise operations for efficient algorithms</li>\n<li><strong>Optimization Techniques</strong>: Instruction selection, register allocation, and pipeline awareness</li>\n</ul>\n<p>We\'ll develop complete programs in assembly language, implementing classic algorithms, data structures, and system utilities without relying on libraries or operating system services, gaining a deep understanding of how software interacts directly with hardware.</p>\n\n<h3>4. Building an Assembler</h3>\n<p>An assembler translates assembly language into machine code. By building an assembler from scratch, we gain insight into both language processing and the detailed structure of machine instructions.</p>\n<p>Our assembler implementation will cover:</p>\n<ul>\n<li><strong>Lexical Analysis</strong>: Breaking assembly source into tokens (mnemonics, registers, values)</li>\n<li><strong>Symbol Table Management</strong>: Tracking labels and their memory addresses</li>\n<li><strong>Two-Pass Assembly</strong>: Resolving forward references and calculating offsets</li>\n<li><strong>Instruction Encoding</strong>: Converting mnemonics and operands to binary opcodes</li>\n<li><strong>Output Formats</strong>: Generating executable binary files (e.g., ELF, flat binary)</li>\n</ul>\n<p>We\'ll implement a complete assembler in a high-level language, avoiding existing assembler tools or parser generators, understanding every step of the translation process and the challenges of mapping human-readable code to efficient machine instructions.</p>\n\n<h3>5. Interfacing with Hardware</h3>\n<p>Assembly language provides direct access to hardware features that are often inaccessible from higher-level languages. This capability is essential for systems programming and device control.</p>\n<p>Key aspects of hardware interfacing include:</p>\n<ul>\n<li><strong>I/O Port Access</strong>: Reading from and writing to hardware ports</li>\n<li><strong>Memory-Mapped I/O</strong>: Controlling devices through special memory addresses</li>\n<li><strong>Interrupt Handling</strong>: Writing interrupt service routines</li>\n<li><strong>DMA Control</strong>: Setting up direct memory access for efficient data transfer</li>\n<li><strong>Privileged Instructions</strong>: Managing processor modes and protection features</li>\n</ul>\n<p>We\'ll write assembly programs that directly control hardware components like displays, keyboards, and storage devices, without using operating system services or device drivers, understanding the full stack from processor instructions to physical devices.</p>',
        parentId: 'programming'
    },
    {
        id: 'language_design',
        title: 'Programming Language Design',
        description: 'Creating your own programming language',
        content: '<h3>1. Fundamental Language Concepts</h3>\n<p>Before designing a programming language, it\'s essential to understand the core concepts that underlie all languages. These fundamental principles inform the major design decisions and tradeoffs in language creation.</p>\n<p>Key language concepts include:</p>\n<ul>\n<li><strong>Syntax vs. Semantics</strong>: The distinction between how programs are written and what they mean</li>\n<li><strong>Binding and Scope</strong>: How names are associated with values and their visibility</li>\n<li><strong>Abstraction Mechanisms</strong>: Ways to hide implementation details (functions, classes, modules)</li>\n<li><strong>Evaluation Strategies</strong>: Eager vs. lazy, strict vs. non-strict evaluation</li>\n<li><strong>Memory Management Models</strong>: Manual, reference counting, garbage collection approaches</li>\n</ul>\n<p>We\'ll explore these concepts by analyzing existing languages, understanding their design decisions, and considering alternative approaches. This foundation will prepare us to make informed choices in our own language design rather than simply copying existing features.</p>\n\n<h3>2. Programming Paradigms</h3>\n<p>Programming paradigms represent fundamentally different approaches to structuring and organizing code. Understanding these paradigms is crucial for designing languages that effectively support specific styles of programming.</p>\n<p>We\'ll explore major paradigms including:</p>\n<ul>\n<li><strong>Imperative Programming</strong>: Step-by-step execution with mutable state (procedural, object-oriented)</li>\n<li><strong>Declarative Programming</strong>: Expressing logic without control flow (functional, logic-based)</li>\n<li><strong>Object-Oriented Programming</strong>: Encapsulating state and behavior in objects and classes</li>\n<li><strong>Functional Programming</strong>: Computing with pure functions and immutable data</li>\n<li><strong>Logic Programming</strong>: Specifying rules and relationships rather than algorithms</li>\n</ul>\n<p>For each paradigm, we\'ll implement small language prototypes that embody its core principles, understanding the strengths, weaknesses, and implementation challenges of each approach. This hands-on experience with multiple paradigms will inform the design of our own language, which might combine elements from different paradigms.</p>\n\n<h3>3. Syntax Design & Parsing</h3>\n<p>A language\'s syntax defines its concrete representation as text and significantly impacts readability, writability, and learnability. Well-designed syntax balances these factors while supporting the language\'s semantic model.</p>\n<p>Our syntax design process will cover:</p>\n<ul>\n<li><strong>Grammar Specification</strong>: Formal definition using BNF, EBNF, or similar notations</li>\n<li><strong>Lexical Structure</strong>: Tokens, identifiers, keywords, and literal values</li>\n<li><strong>Expression Syntax</strong>: Infix, prefix, postfix notations and precedence rules</li>\n<li><strong>Statement and Declaration Syntax</strong>: Block structure, termination, and nesting</li>\n<li><strong>Syntactic Sugar</strong>: Convenient notation that simplifies common patterns</li>\n</ul>\n<p>We\'ll design a syntax for our language from first principles, evaluating each decision against usability criteria rather than simply adopting conventions from existing languages. We\'ll also build parsers for our grammar, understanding both the theory of parsing and practical implementation techniques.</p>\n\n<h3>4. Type Systems & Semantic Analysis</h3>\n<p>Type systems provide a framework for classifying values and expressions, enabling compile-time verification of program properties. A well-designed type system balances safety, expressiveness, and simplicity.</p>\n<p>Our exploration of type systems will include:</p>\n<ul>\n<li><strong>Static vs. Dynamic Typing</strong>: Compile-time vs. run-time type checking trade-offs</li>\n<li><strong>Structural vs. Nominal Types</strong>: Classification based on structure or explicit declaration</li>\n<li><strong>Type Inference</strong>: Algorithms for deducing types without explicit annotations</li>\n<li><strong>Polymorphism</strong>: Parametric, subtype, and ad-hoc approaches to code reuse</li>\n<li><strong>Type Safety and Soundness</strong>: Ensuring program correctness through the type system</li>\n</ul>\n<p>We\'ll implement type checkers for different type systems, understanding their algorithmic foundations and how they enforce language semantics. This implementation experience will inform the design of our language\'s type system, ensuring it meets our goals for safety, expressiveness, and performance.</p>\n\n<h3>5. Language Implementation Strategies</h3>\n<p>How a language is implemented affects its performance characteristics, portability, and ease of development. Different implementation approaches offer various trade-offs that influence language design decisions.</p>\n<p>We\'ll explore several implementation strategies:</p>\n<ul>\n<li><strong>Interpreters</strong>: Direct execution of source or intermediate representations</li>\n<li><strong>Compilers</strong>: Translation to other languages or native machine code</li>\n<li><strong>Just-In-Time Compilation</strong>: Dynamic translation during program execution</li>\n<li><strong>Virtual Machines</strong>: Abstract execution environments with portable bytecode</li>\n<li><strong>Runtime Systems</strong>: Supporting libraries and services for executing programs</li>\n</ul>\n<p>We\'ll implement simple versions of different execution models, understanding their performance characteristics, implementation complexity, and suitability for different language features. This experience will prepare us to choose and develop an appropriate implementation strategy for our own language, ensuring our design decisions are practically implementable.</p>',
        parentId: 'programming'
    },
    {
        id: 'compiler_construction',
        title: 'Compiler & Interpreter Construction',
        description: 'Building tools that translate and execute code',
        content: '<h3>1. Compiler Architecture & Design</h3>\n<p>Compilers and interpreters are complex software systems that transform source code into executable forms. Understanding their overall architecture is essential for building these tools from scratch.</p>\n<p>Key architectural concepts include:</p>\n<ul>\n<li><strong>Compiler Phases</strong>: Front-end, middle-end, and back-end components and their interfaces</li>\n<li><strong>Intermediate Representations</strong>: Abstract syntax trees, three-address code, and static single assignment form</li>\n<li><strong>Symbol Tables</strong>: Data structures for managing identifiers and their attributes</li>\n<li><strong>Error Handling</strong>: Detection, reporting, and recovery strategies throughout compilation</li>\n<li><strong>Compilation Models</strong>: Single-pass vs. multi-pass, batch vs. incremental, just-in-time approaches</li>\n</ul>\n<p>We\'ll develop a modular compiler architecture from first principles, designing each component with clear interfaces and responsibilities, while avoiding existing compiler frameworks that hide these design decisions behind high-level abstractions.</p>\n\n<h3>2. Lexical Analysis & Scanning</h3>\n<p>Lexical analysis is the first phase of compilation, converting raw source text into a stream of tokens. A well-designed scanner balances correctness, error handling, and performance.</p>\n<p>Our implementation of lexical analysis will cover:</p>\n<ul>\n<li><strong>Regular Expressions</strong>: Formal notation for defining lexical patterns</li>\n<li><strong>Finite Automata</strong>: DFAs and NFAs for implementing efficient scanners</li>\n<li><strong>Scanner Construction</strong>: Manual and automated approaches to scanner implementation</li>\n<li><strong>Lexical Errors</strong>: Detection and recovery from invalid input characters</li>\n<li><strong>Lexical Conventions</strong>: Handling whitespace, comments, and language-specific rules</li>\n</ul>\n<p>We\'ll implement scanners using multiple techniques, including hand-written code and automata-based algorithms, while avoiding existing scanner generators. This approach provides deep insight into the trade-offs between different implementation strategies and the theory that underpins them.</p>\n\n<h3>3. Parsing & Syntax Analysis</h3>\n<p>Syntax analysis structures a token stream into a parse tree or abstract syntax tree according to a grammar. This phase checks that programs follow the language\'s syntactic rules and prepares them for semantic analysis.</p>\n<p>Our exploration of parsing will include:</p>\n<ul>\n<li><strong>Context-Free Grammars</strong>: Formal descriptions of language syntax</li>\n<li><strong>Top-Down Parsing</strong>: Recursive descent, LL(k), and predictive parsing techniques</li>\n<li><strong>Bottom-Up Parsing</strong>: Shift-reduce, LR, LALR, and precedence parsing approaches</li>\n<li><strong>Abstract Syntax Trees</strong>: Efficient representations of program structure</li>\n<li><strong>Error Recovery</strong>: Techniques to continue parsing after syntax errors</li>\n</ul>\n<p>We\'ll implement multiple parsers for our language, comparing different approaches and understanding their strengths and limitations. By building these parsers from scratch rather than using parser generators, we\'ll gain insight into the algorithms and data structures that drive syntax analysis.</p>\n\n<h3>4. Semantic Analysis & Type Checking</h3>\n<p>Semantic analysis verifies that programs obey the language\'s semantic rules, particularly type constraints. This phase ensures that operations are applied to appropriate operands and prepares for code generation.</p>\n<p>Key semantic analysis topics include:</p>\n<ul>\n<li><strong>Name Resolution</strong>: Connecting identifier uses with their declarations</li>\n<li><strong>Type Checking</strong>: Verifying type compatibility in expressions and statements</li>\n<li><strong>Type Inference</strong>: Deducing types without explicit annotations</li>\n<li><strong>Semantic Errors</strong>: Detecting and reporting violations of language rules</li>\n<li><strong>Static Analysis</strong>: Flow analysis, escape analysis, and other program properties</li>\n</ul>\n<p>We\'ll implement a complete semantic analyzer for our language, checking all language-specific rules and providing meaningful error messages. This implementation will avoid libraries or frameworks that hide the details of type checking, instead building understanding from first principles.</p>\n\n<h3>5. Code Generation & Execution</h3>\n<p>Code generation transforms a verified program representation into executable form, whether machine code, bytecode, or another target language. This phase determines the performance and behavior of compiled programs.</p>\n<p>Our code generation implementation will cover:</p>\n<ul>\n<li><strong>Target Architecture Selection</strong>: Native code, virtual machines, or translation targets</li>\n<li><strong>Instruction Selection</strong>: Mapping language operations to target instructions</li>\n<li><strong>Register Allocation</strong>: Efficiently assigning variables to registers</li>\n<li><strong>Memory Layout</strong>: Organizing data structures in memory</li>\n<li><strong>Runtime System Integration</strong>: Memory management, I/O, and other services</li>\n</ul>\n<p>We\'ll implement code generators for multiple targets, such as a custom virtual machine, x86 assembly, or WebAssembly, understanding the specific challenges of each. For interpreted languages, we\'ll build evaluators that directly execute abstract syntax trees or bytecode. This approach reveals the fundamental techniques of efficient code generation without relying on existing back-end frameworks.</p>',
        parentId: 'programming'
    },
    {
        id: 'systems_programming',
        title: 'Systems Programming',
        description: 'Creating software that interfaces with hardware',
        content: '<h3>1. Hardware Abstraction Layers</h3>\n<p>Hardware abstraction layers (HALs) provide a consistent interface to diverse hardware components, enabling portable and maintainable systems software. Creating effective abstractions requires understanding both hardware details and software design principles.</p>\n<p>Key aspects of hardware abstraction include:</p>\n<ul>\n<li><strong>Processor Abstraction</strong>: Managing CPU-specific features and instruction sets</li>\n<li><strong>Memory Management Units</strong>: Abstracting physical memory access and protection</li>\n<li><strong>Device Register Mapping</strong>: Creating software interfaces to hardware registers</li>\n<li><strong>Interrupt Controllers</strong>: Providing uniform access to diverse interrupt hardware</li>\n<li><strong>Platform-Specific Optimization</strong>: Balancing portability with performance</li>\n</ul>\n<p>We\'ll implement HALs for multiple hardware platforms, developing interfaces that hide hardware complexity while preserving essential capabilities. Rather than using existing HALs, we\'ll build our own from the hardware registers up, understanding the trade-offs between portability, performance, and maintainability.</p>\n\n<h3>2. Device Driver Development</h3>\n<p>Device drivers bridge the gap between hardware devices and software applications, translating abstract operations into device-specific commands. Effective driver development requires deep understanding of both hardware protocols and software architecture.</p>\n<p>Our exploration of driver development will include:</p>\n<ul>\n<li><strong>Character Devices</strong>: Implementing drivers for serial ports, keyboards, and similar devices</li>\n<li><strong>Block Devices</strong>: Creating drivers for storage systems with block-based access</li>\n<li><strong>Network Interfaces</strong>: Developing drivers for Ethernet and wireless devices</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Managing high-performance data transfers</li>\n<li><strong>Power Management</strong>: Implementing device sleep, hibernation, and wake-up</li>\n</ul>\n<p>We\'ll write complete device drivers from scratch for common hardware components, developing both the hardware-facing code and the software interfaces. This approach avoids using existing driver frameworks, instead building understanding of the entire driver stack from registers and interrupts to system calls.</p>\n\n<h3>3. Runtime Libraries & System Services</h3>\n<p>Runtime libraries provide essential services to applications, handling tasks like memory allocation, I/O, and process management. Building these libraries requires understanding both hardware capabilities and application needs.</p>\n<p>Key runtime components we\'ll implement include:</p>\n<ul>\n<li><strong>Memory Allocators</strong>: Building malloc, free, and garbage collectors from scratch</li>\n<li><strong>I/O Libraries</strong>: Implementing buffered I/O, formatting, and error handling</li>\n<li><strong>Thread Support</strong>: Creating thread creation, scheduling, and synchronization primitives</li>\n<li><strong>Time Services</strong>: Building timers, delays, and real-time scheduling support</li>\n<li><strong>Error Handling</strong>: Implementing exceptions, signals, and other error mechanisms</li>\n</ul>\n<p>We\'ll build complete runtime libraries from scratch, without using existing implementations like libc. This approach reveals the fundamental techniques required to support application execution, from low-level memory management to high-level abstractions like formatted I/O.</p>\n\n<h3>4. Embedded Systems Development</h3>\n<p>Embedded systems combine hardware and software to perform specific functions, often with constrained resources and real-time requirements. Programming these systems demands specialized techniques and careful resource management.</p>\n<p>Our embedded systems exploration will cover:</p>\n<ul>\n<li><strong>Bare-Metal Programming</strong>: Writing code that runs without an operating system</li>\n<li><strong>Real-Time Constraints</strong>: Meeting timing requirements and deadlines</li>\n<li><strong>Resource Optimization</strong>: Minimizing memory usage and power consumption</li>\n<li><strong>Firmware Development</strong>: Creating persistent code that runs on device boot</li>\n<li><strong>Hardware/Software Co-design</strong>: Balancing functionality between hardware and software</li>\n</ul>\n<p>We\'ll develop complete embedded applications for microcontroller platforms, writing code that runs directly on the hardware without operating system support. This approach builds understanding of the unique challenges in embedded development, from interrupt-driven programming to resource-constrained optimization.</p>\n\n<h3>5. Systems Programming Tools</h3>\n<p>Effective systems programming requires specialized tools for development, debugging, testing, and analysis. Understanding these tools—and sometimes building them—is essential for creating reliable systems software.</p>\n<p>Key systems programming tools include:</p>\n<ul>\n<li><strong>Linkers and Loaders</strong>: Understanding and implementing the program loading process</li>\n<li><strong>Debuggers</strong>: Building tools to inspect running systems at the hardware level</li>\n<li><strong>Binary Analysis</strong>: Creating tools to analyze and manipulate compiled code</li>\n<li><strong>Profilers and Tracers</strong>: Developing performance analysis infrastructure</li>\n<li><strong>Cross-Development Environments</strong>: Setting up build systems that target different hardware</li>\n</ul>\n<p>We\'ll implement simplified versions of essential systems programming tools, understanding their internal operation rather than just using existing ones. This approach builds insight into the entire systems development lifecycle, from code creation to deployment and analysis, revealing the foundational technologies that make modern computing possible.</p>',
        parentId: 'programming'
    },
    
    // Operating Systems
    {
        id: 'systems',
        title: 'Operating Systems',
        description: 'Building computer operating systems from first principles',
        content: 'Operating systems are the fundamental software that manages computer hardware and provides common services for computer programs. Our "Build from Scratch" approach teaches you to design and implement every component of an operating system yourself, without relying on existing implementations or high-level abstractions. You\'ll learn how to create a functional OS from basic principles, understanding every layer from hardware interaction to user interfaces.',
        parentId: null
    },
    {
        id: 'boot_process',
        title: 'Boot Process & Kernel Initialization',
        description: 'How computers start up and load the operating system',
        content: '<h3>1. Firmware and Early Boot Sequence</h3>\n<p>When a computer is powered on, it begins executing firmware code stored in non-volatile memory. This firmware initializes the hardware and prepares the system for loading an operating system. Understanding this process is essential for developing low-level system software.</p>\n<p>Key aspects of the early boot sequence include:</p>\n<ul>\n<li><strong>BIOS vs. UEFI</strong>: Traditional and modern firmware approaches to system initialization</li>\n<li><strong>Power-On Self Test (POST)</strong>: Hardware verification and initialization sequence</li>\n<li><strong>Boot Device Selection</strong>: How firmware locates and loads the bootloader</li>\n<li><strong>Memory Detection</strong>: Discovering and initializing RAM before the OS is loaded</li>\n<li><strong>Firmware Services</strong>: Interfaces provided to bootloaders and early operating systems</li>\n</ul>\n<p>We\'ll explore firmware internals by examining firmware implementations and creating minimal test environments that simulate the early boot process. This approach provides insight into the critical but often invisible first steps of system startup, without relying on black-box firmware implementations.</p>\n\n<h3>2. Bootloader Design & Implementation</h3>\n<p>Bootloaders bridge the gap between firmware and the operating system kernel, loading the kernel into memory and preparing the environment for its execution. Implementing a bootloader requires understanding both hardware constraints and OS requirements.</p>\n<p>Our bootloader implementation will cover:</p>\n<ul>\n<li><strong>Multi-Stage Loading</strong>: Breaking the boot process into size-constrained stages</li>\n<li><strong>Disk Access</strong>: Reading kernel code from storage without OS services</li>\n<li><strong>Boot Protocols</strong>: Communication between bootloader and kernel</li>\n<li><strong>Memory Maps</strong>: Providing the kernel with information about available memory</li>\n<li><strong>Boot Configuration</strong>: Passing parameters and options to the kernel</li>\n</ul>\n<p>We\'ll implement a complete bootloader from scratch, capable of loading a simple kernel from disk to memory. Rather than using existing bootloaders like GRUB, we\'ll write our own assembly and C code, understanding every step from the initial firmware handoff to the final jump to kernel code.</p>\n\n<h3>3. Processor Mode Transitions</h3>\n<p>Modern processors support different operational modes with varying capabilities and protection levels. Transitioning between these modes is a critical part of the boot process, requiring careful management of processor state and memory structures.</p>\n<p>Key mode transitions include:</p>\n<ul>\n<li><strong>Real Mode to Protected Mode</strong>: Enabling memory protection and 32-bit operation</li>\n<li><strong>Protected Mode to Long Mode</strong>: Transitioning to 64-bit operation (on x86-64)</li>\n<li><strong>Global Descriptor Tables</strong>: Setting up memory segmentation structures</li>\n<li><strong>Paging Initialization</strong>: Establishing virtual memory translation structures</li>\n<li><strong>Privilege Levels</strong>: Configuring CPU protection rings for kernel and user code</li>\n</ul>\n<p>We\'ll implement these transitions step by step, writing code that properly configures processor registers, memory structures, and control flags. This hands-on approach reveals the intricate details of processor architecture and the foundations of system security and isolation.</p>\n\n<h3>4. Early Hardware Initialization</h3>\n<p>Before a kernel can provide full functionality, it must initialize critical hardware components and establish basic services. This early initialization occurs in a minimal environment without the support of many kernel subsystems.</p>\n<p>Our exploration of early hardware initialization will include:</p>\n<ul>\n<li><strong>Interrupt Controller Setup</strong>: Configuring interrupt handling for system events</li>\n<li><strong>Timer Initialization</strong>: Establishing system timing and scheduling capabilities</li>\n<li><strong>Memory Controller Configuration</strong>: Optimizing memory access patterns and caching</li>\n<li><strong>Basic I/O Setup</strong>: Initializing essential input/output devices for early debug output</li>\n<li><strong>Multiprocessor Initialization</strong>: Bringing secondary CPU cores online</li>\n</ul>\n<p>We\'ll implement initialization routines for these critical hardware components, developing a deep understanding of hardware interfaces and the delicate sequence of operations required to bring a system to life. This implementation will avoid using existing drivers or initialization code, instead developing the minimum necessary code from hardware documentation.</p>\n\n<h3>5. Kernel Environment Preparation</h3>\n<p>Before the main kernel code can execute, the boot process must establish a suitable execution environment, including memory layouts, data structures, and essential services. This preparation bridges the gap between the bare hardware and the structured world of the kernel.</p>\n<p>Key aspects of kernel environment preparation include:</p>\n<ul>\n<li><strong>Memory Layout Establishment</strong>: Setting up kernel and user address spaces</li>\n<li><strong>C Environment Initialization</strong>: Preparing for high-level language execution</li>\n<li><strong>Core Data Structures</strong>: Creating essential kernel management structures</li>\n<li><strong>Bootstrap Memory Allocation</strong>: Implementing early memory management before the full MM system</li>\n<li><strong>Debug and Logging Services</strong>: Establishing early diagnostic capabilities</li>\n</ul>\n<p>We\'ll implement the complete sequence of environment preparation, from the first instructions executed in kernel space to the point where high-level kernel initialization can begin. This approach provides insight into the hidden foundations that support the rest of the operating system, revealing the careful orchestration required to transform bare hardware into a platform ready for complex software execution.</p>',
        parentId: 'systems'
    },
    {
        id: 'process_management',
        title: 'Process Management',
        description: 'Creating and controlling execution units',
        content: '<h3>1. Process Abstraction & Representation</h3>\n<p>Processes are the fundamental execution units in operating systems, abstracting running programs from the underlying hardware. Effectively representing processes is essential for managing and controlling their execution.</p>\n<p>Key aspects of process abstraction include:</p>\n<ul>\n<li><strong>Process Control Blocks</strong>: Data structures for tracking process state and resources</li>\n<li><strong>Process Address Space</strong>: Memory organization for code, data, stack, and heap</li>\n<li><strong>Process Lifecycle</strong>: States from creation to termination and transitions between them</li>\n<li><strong>Process Hierarchy</strong>: Parent-child relationships and inheritance of attributes</li>\n<li><strong>Process Identity</strong>: Naming, identification, and security attributes</li>\n</ul>\n<p>We\'ll implement a complete process management infrastructure from scratch, developing data structures that can track all aspects of a running process. Rather than using existing process management code, we\'ll build our own abstractions, gaining insight into the fundamental building blocks of multitasking operating systems.</p>\n\n<h3>2. Context Switching & Dispatching</h3>\n<p>Context switching is the mechanism that allows multiple processes to share CPU resources, giving the illusion of parallel execution. This complex operation requires careful preservation and restoration of processor state.</p>\n<p>Our context switching implementation will cover:</p>\n<ul>\n<li><strong>Register State Saving</strong>: Preserving CPU registers during task switches</li>\n<li><strong>Memory Context Management</strong>: Switching address spaces and translation tables</li>\n<li><strong>CPU Mode Transitions</strong>: Managing privileged operations during context switches</li>\n<li><strong>Execution Stack Switching</strong>: Managing per-process kernel and user stacks</li>\n<li><strong>Context Switch Optimization</strong>: Minimizing the performance impact of task switches</li>\n</ul>\n<p>We\'ll implement context switching in assembly language for specific processor architectures, understanding the hardware-specific details and optimizations. This hands-on approach reveals the intricate dance required to juggle multiple execution contexts on a single processor, without relying on existing context switching code.</p>\n\n<h3>3. Scheduling Algorithms & Policies</h3>\n<p>Schedulers determine which process runs next and for how long, balancing responsiveness, fairness, and efficiency. Effective scheduling is critical for system performance and requires both algorithmic understanding and practical implementation skills.</p>\n<p>Our exploration of scheduling will include:</p>\n<ul>\n<li><strong>Preemptive vs. Cooperative Scheduling</strong>: Different approaches to allocating CPU time</li>\n<li><strong>Priority-Based Algorithms</strong>: Assigning and managing execution priorities</li>\n<li><strong>Round-Robin and Time-Slicing</strong>: Fairly sharing CPU time among processes</li>\n<li><strong>Real-Time Scheduling</strong>: Meeting timing constraints for time-critical tasks</li>\n<li><strong>Multiprocessor Scheduling</strong>: Distributing processes across multiple CPU cores</li>\n</ul>\n<p>We\'ll implement multiple scheduling algorithms from scratch, comparing their behavior and performance characteristics. By building a pluggable scheduler framework, we\'ll experience the tradeoffs between different scheduling policies firsthand, rather than just using existing schedulers or simulating their behavior.</p>\n\n<h3>4. Thread Implementation & Management</h3>\n<p>Threads provide lightweight concurrency within processes, sharing an address space while executing independently. Implementing threads requires careful coordination of shared resources and execution state.</p>\n<p>Key thread implementation topics include:</p>\n<ul>\n<li><strong>User vs. Kernel Threads</strong>: Different models for thread implementation</li>\n<li><strong>Thread Control Blocks</strong>: Lightweight process-like structures for threads</li>\n<li><strong>Thread Local Storage</strong>: Per-thread data in a shared address space</li>\n<li><strong>Thread Scheduling</strong>: Balancing execution among threads within processes</li>\n<li><strong>Thread Creation and Termination</strong>: Efficiently spawning and cleaning up threads</li>\n</ul>\n<p>We\'ll implement both user-level and kernel-level threading systems from scratch, understanding the advantages and tradeoffs of each approach. This implementation will demonstrate how threads can provide concurrency while minimizing the overhead associated with full processes.</p>\n\n<h3>5. Synchronization & Communication</h3>\n<p>Processes and threads must coordinate their actions when sharing resources or exchanging information. Effective synchronization and communication mechanisms are essential for correct concurrent operation.</p>\n<p>Our implementation of synchronization will cover:</p>\n<ul>\n<li><strong>Mutual Exclusion Primitives</strong>: Locks, mutexes, and critical sections</li>\n<li><strong>Condition Synchronization</strong>: Semaphores, condition variables, and barriers</li>\n<li><strong>Message Passing</strong>: Inter-process communication via structured messages</li>\n<li><strong>Shared Memory</strong>: Directly sharing memory regions between processes</li>\n<li><strong>Deadlock Prevention</strong>: Detecting and avoiding synchronization deadlocks</li>\n</ul>\n<p>We\'ll implement these mechanisms from first principles, building synchronization primitives at both the hardware and software levels. Rather than using existing libraries, we\'ll develop a complete suite of synchronization tools, understanding the algorithms and hardware features that make safe concurrent execution possible.</p>',
        parentId: 'systems'
    },
    {
        id: 'memory_management',
        title: 'Memory Management',
        description: 'Efficiently allocating and protecting memory resources',
        content: '<h3>1. Physical Memory Management</h3>\n<p>Physical memory is the actual RAM hardware in a computer system. Managing this resource effectively requires tracking available memory, allocating it to processes, and reclaiming it when no longer needed.</p>\n<p>Key aspects of physical memory management include:</p>\n<ul>\n<li><strong>Memory Detection</strong>: Discovering and mapping available physical memory at boot time</li>\n<li><strong>Memory Partitioning</strong>: Organizing physical memory into manageable regions</li>\n<li><strong>Physical Allocation Algorithms</strong>: First-fit, best-fit, buddy system approaches</li>\n<li><strong>Physical Address Alignment</strong>: Meeting hardware constraints for memory access</li>\n<li><strong>Fragmentation Management</strong>: Strategies to combat memory fragmentation over time</li>\n</ul>\n<p>We\'ll implement a physical memory manager from scratch, developing data structures to track memory regions and algorithms for efficient allocation. This implementation will avoid using existing memory management libraries, instead building understanding from the hardware interface upward.</p>\n\n<h3>2. Virtual Memory Systems</h3>\n<p>Virtual memory creates an abstraction layer between logical addresses used by processes and physical addresses in RAM. This abstraction enables memory protection, overcommitment, and simplifies application development.</p>\n<p>Our virtual memory implementation will cover:</p>\n<ul>\n<li><strong>Address Translation</strong>: Mapping virtual addresses to physical locations</li>\n<li><strong>Page Tables</strong>: Data structures for efficient address mapping</li>\n<li><strong>Translation Lookaside Buffers (TLBs)</strong>: Hardware caching of address translations</li>\n<li><strong>Demand Paging</strong>: Loading memory contents only when needed</li>\n<li><strong>Memory-Mapped Files</strong>: Accessing files through the virtual memory system</li>\n</ul>\n<p>We\'ll build a complete virtual memory system from first principles, including page table structures optimized for different architectures. Rather than using existing virtual memory implementations, we\'ll develop our own system that interacts directly with both the MMU hardware and the physical memory manager.</p>\n\n<h3>3. Paging & Segmentation</h3>\n<p>Paging and segmentation are two approaches to implementing virtual memory, each with distinct characteristics and trade-offs. Understanding both approaches provides insight into memory system design evolution.</p>\n<p>Key aspects of paging and segmentation include:</p>\n<ul>\n<li><strong>Page Tables</strong>: Single-level, multi-level, and inverted page table designs</li>\n<li><strong>Segment Descriptors</strong>: Defining variable-sized memory regions</li>\n<li><strong>Protection Bits</strong>: Controlling read, write, and execute permissions</li>\n<li><strong>Hybrid Approaches</strong>: Combining paging and segmentation benefits</li>\n<li><strong>Hardware Support</strong>: Processor features that accelerate virtual memory</li>\n</ul>\n<p>We\'ll implement both paging and segmentation systems from scratch, comparing their performance, complexity, and memory efficiency. This hands-on experience with multiple memory organization approaches builds a deeper understanding of virtual memory concepts than studying a single implementation.</p>\n\n<h3>4. Memory Allocation & Heap Management</h3>\n<p>Memory allocators provide controlled access to memory resources, managing the details of finding and reserving memory blocks of appropriate sizes. Efficient allocators balance speed, memory utilization, and fragmentation resistance.</p>\n<p>Our allocator implementation will include:</p>\n<ul>\n<li><strong>Kernel Allocators</strong>: Memory management for operating system needs</li>\n<li><strong>User-Space Allocators</strong>: Implementing malloc, free, and related functions</li>\n<li><strong>Memory Pools</strong>: Optimized allocation for fixed-size objects</li>\n<li><strong>Garbage Collection</strong>: Automatic memory reclamation techniques</li>\n<li><strong>Memory Debugging</strong>: Tools for detecting leaks and corruption</li>\n</ul>\n<p>We\'ll build multiple memory allocators from scratch, evaluating their performance characteristics and suitability for different workloads. Rather than using existing allocation libraries, we\'ll implement our own algorithms, gaining insight into the complex trade-offs involved in heap management.</p>\n\n<h3>5. Swapping & Memory Compression</h3>\n<p>When physical memory is exhausted, systems can extend available memory by using disk space or compression techniques. These approaches trade performance for capacity, enabling systems to handle larger workloads than would fit in RAM.</p>\n<p>Key aspects of memory extension include:</p>\n<ul>\n<li><strong>Page Replacement Algorithms</strong>: Selecting which pages to evict (LRU, Clock, etc.)</li>\n<li><strong>Swap Space Management</strong>: Organizing and accessing disk-based memory extensions</li>\n<li><strong>Memory Compression</strong>: Trading CPU time for memory capacity</li>\n<li><strong>Prefetching and Anticipation</strong>: Reducing the performance impact of paging</li>\n<li><strong>Out-of-Memory Management</strong>: Handling resource exhaustion gracefully</li>\n</ul>\n<p>We\'ll implement page replacement algorithms and swapping systems from first principles, including the policy decisions, data structures, and I/O operations involved. This implementation will provide insight into how modern operating systems balance memory capacity and performance constraints, managing limited physical resources to support demanding workloads.</p>',
        parentId: 'systems'
    },
    {
        id: 'filesystem',
        title: 'File Systems',
        description: 'Organizing and storing persistent data',
        content: '<h3>1. File System Architecture & Design</h3>\n<p>File systems organize and manage persistent storage, providing a stable, hierarchical interface to an otherwise raw block device. Designing effective file systems requires balancing performance, reliability, and usability considerations.</p>\n<p>Key aspects of file system architecture include:</p>\n<ul>\n<li><strong>Overall Structure</strong>: On-disk layout, partitioning, and volume management</li>\n<li><strong>Disk Block Management</strong>: Managing physical storage units and addressing</li>\n<li><strong>Metadata Organization</strong>: Strategies for storing file and directory information</li>\n<li><strong>File System Interfaces</strong>: System calls and operations for accessing storage</li>\n<li><strong>Performance Optimizations</strong>: Caching, prefetching, and layout strategies</li>\n</ul>\n<p>We\'ll develop a file system architecture from first principles, designing the on-disk structures and in-memory representations needed for efficient storage. Rather than using existing file system implementations, we\'ll build our own from scratch, gaining insight into the fundamental trade-offs in storage system design.</p>\n\n<h3>2. Block Allocation & Management</h3>\n<p>Block allocation is how file systems assign disk space to files. The choice of allocation strategy significantly impacts performance, space efficiency, and resistance to fragmentation over time.</p>\n<p>Our exploration of block allocation will include:</p>\n<ul>\n<li><strong>Contiguous Allocation</strong>: Storing files in sequential blocks</li>\n<li><strong>Linked Allocation</strong>: Using pointers to connect blocks belonging to a file</li>\n<li><strong>Indexed Allocation</strong>: Tracking blocks through index structures (inodes)</li>\n<li><strong>Extent-Based Allocation</strong>: Managing ranges of contiguous blocks</li>\n<li><strong>Free Space Management</strong>: Bitmap, linked list, and other free block tracking methods</li>\n</ul>\n<p>We\'ll implement multiple allocation strategies from scratch, comparing their performance in different scenarios. This hands-on approach provides deeper understanding than simply studying existing file systems, revealing how allocation decisions affect file system behavior over time.</p>\n\n<h3>3. Directory Organization & Name Resolution</h3>\n<p>Directories provide the hierarchical structure and naming system that makes file systems navigable by humans. Effective directory implementation balances lookup speed, modification efficiency, and robustness.</p>\n<p>Key aspects of directory implementation include:</p>\n<ul>\n<li><strong>Directory Entry Formats</strong>: Storing filename-to-file mappings efficiently</li>\n<li><strong>Path Traversal</strong>: Resolving hierarchical paths to specific files</li>\n<li><strong>Directory Search Optimization</strong>: Hash tables, B-trees for fast lookups</li>\n<li><strong>Namespace Management</strong>: Symbolic links, hard links, and mount points</li>\n<li><strong>Directory Consistency</strong>: Maintaining integrity during updates</li>\n</ul>\n<p>We\'ll build a complete directory subsystem from scratch, including both the on-disk structures and in-memory caching. This implementation will explore the trade-offs between different directory organizations, such as linear lists versus tree structures, without relying on existing directory implementations.</p>\n\n<h3>4. File Operations & Caching</h3>\n<p>File operations translate application requests into disk operations, managing the details of reading, writing, and manipulating files. Effective implementations balance correctness, performance, and resource usage.</p>\n<p>Our file operations implementation will cover:</p>\n<ul>\n<li><strong>Basic Operations</strong>: Open, close, read, write, and seek</li>\n<li><strong>Advanced Operations</strong>: Memory mapping, truncation, and extended attributes</li>\n<li><strong>Buffer Cache</strong>: Caching disk blocks for performance</li>\n<li><strong>Read-Ahead</strong>: Prefetching blocks based on access patterns</li>\n<li><strong>Write-Behind</strong>: Delaying and coalescing writes for efficiency</li>\n</ul>\n<p>We\'ll implement a complete file operations layer from scratch, building both the system call interface and the underlying mechanisms. This approach provides insight into how abstract file operations translate to concrete disk operations, without using existing file system code that obscures these details.</p>\n\n<h3>5. Journaling & Recovery</h3>\n<p>File system reliability requires mechanisms to recover from crashes and unexpected shutdowns. Journaling provides a way to maintain consistency by tracking changes before they\'re applied to the main file system structures.</p>\n<p>Key aspects of journaling and recovery include:</p>\n<ul>\n<li><strong>Transaction Logging</strong>: Recording changes before committing them</li>\n<li><strong>Metadata Journaling</strong>: Protecting file system structures while optimizing data handling</li>\n<li><strong>Checkpointing</strong>: Periodically synchronizing journal and main file system</li>\n<li><strong>Recovery Procedures</strong>: Restoring consistency after crashes</li>\n<li><strong>Alternative Approaches</strong>: Copy-on-write, log-structured file systems</li>\n</ul>\n<p>We\'ll implement a journaling mechanism from first principles, developing both the on-disk journal format and the recovery procedures. This hands-on experience with crash recovery provides insight into the delicate balance between performance and reliability in modern file systems, revealing the complex trade-offs that existing implementations make.</p>',
        parentId: 'systems'
    },
    {
        id: 'device_drivers',
        title: 'Device Drivers & I/O',
        description: 'Interfacing with hardware devices',
        content: '<h3>1. I/O Subsystem Architecture</h3>\n<p>The I/O subsystem provides a framework for managing diverse hardware devices through consistent interfaces. Designing this architecture requires balancing abstraction with performance while supporting a wide range of device types.</p>\n<p>Key aspects of I/O subsystem architecture include:</p>\n<ul>\n<li><strong>Device Abstraction Models</strong>: Character, block, and network device paradigms</li>\n<li><strong>Driver Frameworks</strong>: Organizing driver code for modularity and maintainability</li>\n<li><strong>Device Discovery</strong>: Identifying and initializing hardware at boot and runtime</li>\n<li><strong>I/O Scheduling</strong>: Managing concurrent access to shared devices</li>\n<li><strong>Error Handling</strong>: Detecting and recovering from hardware failures</li>\n</ul>\n<p>We\'ll design and implement a complete I/O subsystem from scratch, building the framework that organizes device access. This implementation will avoid using existing driver frameworks, instead developing the architecture from first principles to gain insight into the trade-offs involved in system-level I/O management.</p>\n\n<h3>2. Device Communication Mechanisms</h3>\n<p>Hardware devices communicate with the CPU through various mechanisms, each with different performance characteristics and complexity. Understanding these communication channels is essential for effective driver development.</p>\n<p>Our exploration of device communication will include:</p>\n<ul>\n<li><strong>Port-Mapped I/O</strong>: Communicating through special CPU instructions (in/out)</li>\n<li><strong>Memory-Mapped I/O</strong>: Accessing device registers through memory addresses</li>\n<li><strong>Interrupt Handling</strong>: Responding to asynchronous device signals</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Enabling devices to access memory directly</li>\n<li><strong>Bus Protocols</strong>: Understanding PCI, USB, I2C, and other communication standards</li>\n</ul>\n<p>We\'ll implement drivers using each of these communication mechanisms, understanding their hardware interfaces and software requirements. Rather than using existing driver libraries that hide these details, we\'ll write code that directly communicates with hardware, developing a deep understanding of the CPU-device interface.</p>\n\n<h3>3. Block Device Drivers</h3>\n<p>Block devices manage data in fixed-size blocks and typically support random access. Storage devices like disks are the primary example of block devices, requiring drivers that balance performance, data integrity, and error handling.</p>\n<p>Key aspects of block driver implementation include:</p>\n<ul>\n<li><strong>Block Addressing</strong>: Mapping logical block addresses to physical locations</li>\n<li><strong>Request Queuing</strong>: Managing and optimizing I/O requests</li>\n<li><strong>Read-Ahead and Caching</strong>: Improving performance through prediction</li>\n<li><strong>Error Detection and Recovery</strong>: Handling media defects and hardware failures</li>\n<li><strong>Advanced Storage Features</strong>: TRIM, secure erase, and health monitoring</li>\n</ul>\n<p>We\'ll implement a complete block device driver from scratch, developing each component from the hardware interface upward. This approach avoids using existing block drivers, instead building understanding of the complex interactions between the block layer and both the hardware below and the file system above.</p>\n\n<h3>4. Character and Input Device Drivers</h3>\n<p>Character devices handle data as streams of bytes and often interface with input/output peripherals. These drivers must balance responsiveness with efficient resource usage while handling unpredictable timing.</p>\n<p>Our character driver implementation will cover:</p>\n<ul>\n<li><strong>Terminal Devices</strong>: Managing console and serial communication</li>\n<li><strong>Keyboard and Pointing Devices</strong>: Processing human input events</li>\n<li><strong>Buffering Strategies</strong>: Handling bursty input and output</li>\n<li><strong>Line Discipline</strong>: Processing and editing input streams</li>\n<li><strong>Device Control</strong>: Setting parameters and modes through ioctl operations</li>\n</ul>\n<p>We\'ll build drivers for common character devices from the hardware level up, implementing the complete stack from interrupt handlers to user-space interfaces. This hands-on approach reveals the unique challenges of character devices, such as real-time constraints and variable data rates, without using existing driver implementations that hide these complexities.</p>\n\n<h3>5. Graphics and Display Drivers</h3>\n<p>Graphics devices present unique challenges due to their high bandwidth requirements, complex hardware, and performance sensitivity. Implementing graphics drivers requires understanding both hardware acceleration and software rendering paths.</p>\n<p>Key aspects of graphics driver development include:</p>\n<ul>\n<li><strong>Framebuffer Management</strong>: Controlling display memory and refresh</li>\n<li><strong>Mode Setting</strong>: Configuring resolution, color depth, and timing</li>\n<li><strong>2D Acceleration</strong>: Hardware-assisted rendering operations</li>\n<li><strong>GPU Programming</strong>: Interfacing with graphics processing units</li>\n<li><strong>Multi-Display Support</strong>: Managing multiple output devices</li>\n</ul>\n<p>We\'ll implement graphics drivers from scratch, starting with basic framebuffer support and progressing to hardware acceleration features. Rather than using existing graphics stacks, we\'ll develop our own driver that directly controls the display hardware, gaining insight into the critical but often opaque world of graphics programming. This approach reveals the fundamental techniques that enable modern graphical interfaces, without relying on complex existing implementations.</p>',
        parentId: 'systems'
    },

    // Networking
    {
        id: 'networks',
        title: 'Networking',
        description: 'Building network systems and protocols from scratch',
        content: 'Computer networking is the practice of connecting computers to share resources and communicate. Our "Build from Scratch" approach means learning about every layer of the network stack, from the physical transmission of signals to application protocols, implementing each component yourself. This comprehensive understanding allows you to build, optimize, and troubleshoot network systems with insight that\'s impossible to gain by just using existing implementations.',
        parentId: null
    },
    {
        id: 'physical_layer',
        title: 'Physical Layer & Signal Transmission',
        description: 'How bits become signals on physical media',
        content: '<h3>1. Signal Theory & Electromagnetic Fundamentals</h3>\n<p>At its core, all digital communication relies on the physics of electromagnetic signals. Understanding these fundamentals is essential for designing, troubleshooting, and optimizing physical layer communication systems.</p>\n<p>Key aspects of signal theory include:</p>\n<ul>\n<li><strong>Waveform Properties</strong>: Frequency, amplitude, phase, and their manipulation</li>\n<li><strong>Spectrum Analysis</strong>: Time and frequency domain representations of signals</li>\n<li><strong>Signal Propagation</strong>: How electromagnetic waves travel through different media</li>\n<li><strong>Noise and Interference</strong>: Sources of signal degradation and their effects</li>\n<li><strong>Signal-to-Noise Ratio</strong>: Measuring and improving communication quality</li>\n</ul>\n<p>We\'ll explore these principles through hands-on experiments, using basic electronic components to generate and measure signals. This approach builds understanding from fundamental physics rather than treating signals as abstract entities, providing insight into the physical constraints that shape all communication systems.</p>\n\n<h3>2. Transmission Media & Channel Characteristics</h3>\n<p>Different transmission media have unique properties that affect how signals propagate and degrade. Understanding these characteristics is crucial for designing appropriate signaling methods for each medium.</p>\n<p>Our exploration of transmission media will include:</p>\n<ul>\n<li><strong>Copper-Based Media</strong>: Twisted pair, coaxial, and power line characteristics</li>\n<li><strong>Fiber Optic Systems</strong>: Light propagation, attenuation, and dispersion</li>\n<li><strong>Wireless Propagation</strong>: Radio waves, microwave, and infrared transmission</li>\n<li><strong>Channel Impairments</strong>: Attenuation, distortion, and multi-path effects</li>\n<li><strong>Channel Capacity</strong>: Shannon\'s theorem and fundamental limits</li>\n</ul>\n<p>We\'ll conduct experiments with different media types, measuring their properties and observing how they affect signal transmission. This hands-on approach provides direct experience with the physical constraints of communication channels, rather than just studying theoretical models.</p>\n\n<h3>3. Digital Modulation Techniques</h3>\n<p>Modulation is the process of encoding digital information onto analog carrier signals. Different modulation schemes offer various trade-offs between bandwidth efficiency, power requirements, and noise resistance.</p>\n<p>Key digital modulation techniques include:</p>\n<ul>\n<li><strong>Amplitude Shift Keying (ASK)</strong>: Encoding bits by varying signal amplitude</li>\n<li><strong>Frequency Shift Keying (FSK)</strong>: Using different frequencies to represent bits</li>\n<li><strong>Phase Shift Keying (PSK)</strong>: Encoding data through phase variations</li>\n<li><strong>Quadrature Amplitude Modulation (QAM)</strong>: Combining amplitude and phase modulation</li>\n<li><strong>Spread Spectrum Techniques</strong>: DSSS and FHSS for noise resistance</li>\n</ul>\n<p>We\'ll implement these modulation schemes from scratch using basic signal processing components, understanding their mathematical foundations and practical implementations. Rather than using existing modulation libraries, we\'ll build our own modulators and demodulators, gaining insight into the algorithms that convert between bits and physical signals.</p>\n\n<h3>4. Line Coding & Signal Encoding</h3>\n<p>Line coding translates binary data into signal patterns suitable for specific transmission media. Effective encoding schemes balance multiple objectives including clock recovery, error detection, and spectral efficiency.</p>\n<p>Our study of line coding will cover:</p>\n<ul>\n<li><strong>Non-Return to Zero (NRZ)</strong>: Basic binary voltage level representations</li>\n<li><strong>Manchester Encoding</strong>: Self-clocking schemes for synchronization</li>\n<li><strong>Multi-Level Transmissions</strong>: 2B1Q, PAM-n, and other bandwidth-efficient codes</li>\n<li><strong>Run-Length Limited Codes</strong>: Controlling signal patterns for physical constraints</li>\n<li><strong>8b/10b and Similar Encodings</strong>: Balancing DC levels and error detection</li>\n</ul>\n<p>We\'ll implement these encoding schemes from first principles, building encoders and decoders that translate between raw binary data and physical signal representations. This implementation-focused approach reveals the practical challenges of reliable bit transmission, without relying on existing encoding libraries.</p>\n\n<h3>5. Physical Layer Protocols & Interfaces</h3>\n<p>Physical layer protocols define the electrical, mechanical, and procedural standards for activating, maintaining, and deactivating physical connections. These standards ensure interoperability between equipment from different vendors.</p>\n<p>Key physical layer standards and interfaces include:</p>\n<ul>\n<li><strong>Ethernet Physical Layer</strong>: 10BASE-T, 100BASE-TX, 1000BASE-T standards</li>\n<li><strong>Serial Communication</strong>: RS-232, RS-485, UART, SPI, and I2C interfaces</li>\n<li><strong>Wireless PHY Layers</strong>: 802.11 (Wi-Fi), Bluetooth, and cellular protocols</li>\n<li><strong>Digital Subscriber Line (DSL)</strong>: Techniques for high-speed transmission over copper</li>\n<li><strong>Clock Recovery</strong>: Synchronizing transmitter and receiver timing</li>\n</ul>\n<p>We\'ll implement simplified versions of these physical layer protocols, focusing on the signaling, synchronization, and timing aspects. Rather than using existing protocol stacks, we\'ll build our own physical layer interfaces from basic components, developing insight into how abstract bits become reliable physical signals in real-world communication systems.</p>',
        parentId: 'networks'
    },
    {
        id: 'data_link_layer',
        title: 'Data Link Layer & MAC Protocols',
        description: 'Creating reliable links between directly connected devices',
        content: '<h3>1. Framing & Packet Structure</h3>\n<p>Framing is the process of organizing raw bits into structured data units that can be reliably transmitted and received. Effective frame designs balance efficiency, reliability, and compatibility with physical layer constraints.</p>\n<p>Key aspects of framing include:</p>\n<ul>\n<li><strong>Frame Delimiting</strong>: Marking the beginning and end of frames</li>\n<li><strong>Character/Bit Stuffing</strong>: Preventing delimiter confusion in data</li>\n<li><strong>Frame Synchronization</strong>: Aligning receiver with incoming frame boundaries</li>\n<li><strong>Address Fields</strong>: Identifying source and destination devices</li>\n<li><strong>Control Information</strong>: Frame types, sequence numbers, and flags</li>\n</ul>\n<p>We\'ll implement multiple framing methods from scratch, developing software that transforms raw data streams into properly structured frames. Rather than using existing framing libraries, we\'ll build our own implementations, gaining insight into the challenges of maintaining frame integrity across noisy channels.</p>\n\n<h3>2. Error Detection & Correction</h3>\n<p>Communication channels introduce errors that corrupt transmitted data. Error detection and correction codes allow receivers to identify and potentially repair these corruptions, ensuring data reliability.</p>\n<p>Our exploration of error control will include:</p>\n<ul>\n<li><strong>Parity Checks</strong>: Simple single-bit error detection</li>\n<li><strong>Cyclic Redundancy Checks (CRC)</strong>: Polynomial division for robust detection</li>\n<li><strong>Checksums</strong>: Addition-based error detection methods</li>\n<li><strong>Hamming Codes</strong>: Detecting and correcting single-bit errors</li>\n<li><strong>Forward Error Correction</strong>: Reed-Solomon and other correction techniques</li>\n</ul>\n<p>We\'ll implement these error detection and correction algorithms from first principles, building encoders and decoders that can protect data from corruption. This approach reveals the mathematical foundations and practical trade-offs in error control, without relying on existing implementations that hide these details.</p>\n\n<h3>3. Medium Access Control Protocols</h3>\n<p>When multiple devices share a communication medium, they need protocols to coordinate access and prevent interference. MAC protocols balance fairness, efficiency, and determinism in channel sharing.</p>\n<p>Key MAC protocols we\'ll explore include:</p>\n<ul>\n<li><strong>CSMA/CD</strong>: Ethernet\'s approach to shared medium coordination</li>\n<li><strong>CSMA/CA</strong>: Collision avoidance techniques used in wireless networks</li>\n<li><strong>Token Passing</strong>: Deterministic approaches to medium access</li>\n<li><strong>Time Division Multiple Access (TDMA)</strong>: Scheduled time slot allocation</li>\n<li><strong>Frequency Division Multiple Access (FDMA)</strong>: Dividing the frequency spectrum</li>\n</ul>\n<p>We\'ll implement these protocols from scratch, developing systems that can coordinate multiple devices on a shared channel. This hands-on approach provides insight into the fundamental problem of distributed coordination without centralized control, revealing how network designs balance performance and reliability.</p>\n\n<h3>4. Link Control Protocols</h3>\n<p>Link control protocols manage the communication flow between directly connected devices, handling acknowledgments, retransmissions, and sequencing to ensure reliable data transfer.</p>\n<p>Our implementation of link control will cover:</p>\n<ul>\n<li><strong>Stop-and-Wait ARQ</strong>: Simple acknowledgment-based reliability</li>\n<li><strong>Sliding Window Protocols</strong>: Go-Back-N and Selective Repeat</li>\n<li><strong>Flow Control</strong>: Preventing receiver overflow</li>\n<li><strong>Connection Management</strong>: Establishing and terminating links</li>\n<li><strong>Protocol Efficiency Analysis</strong>: Throughput and utilization optimization</li>\n</ul>\n<p>We\'ll build these protocols from first principles, creating software that can reliably transfer data over unreliable links. This implementation-focused approach reveals the algorithmic and engineering challenges in ensuring delivery guarantees, without using existing protocol stacks that hide the underlying mechanisms.</p>\n\n<h3>5. Switching & Bridging</h3>\n<p>Switches and bridges extend the data link layer beyond point-to-point connections, connecting multiple network segments while operating at the MAC address level. These devices form the foundation of local area networks.</p>\n<p>Key switching and bridging topics include:</p>\n<ul>\n<li><strong>MAC Address Learning</strong>: Building and maintaining forwarding tables</li>\n<li><strong>Frame Forwarding</strong>: Directing frames to appropriate ports</li>\n<li><strong>Spanning Tree Protocol</strong>: Preventing loops in redundant topologies</li>\n<li><strong>VLAN Implementation</strong>: Logical network segmentation</li>\n<li><strong>Switch Architectures</strong>: Cut-through vs. store-and-forward designs</li>\n</ul>\n<p>We\'ll implement switching algorithms from scratch, building software that can intelligently forward frames between network segments. Rather than using existing switch firmware or simulators, we\'ll develop our own switching logic, gaining insight into the self-learning mechanisms that allow switches to automatically build network topologies. This approach reveals the elegant solutions that enable modern local networks to scale efficiently while maintaining plug-and-play simplicity.</p>',
        parentId: 'networks'
    },
    {
        id: 'network_layer',
        title: 'Network Layer & Routing',
        description: 'Finding paths through interconnected networks',
        content: '<h3>1. Network Layer Architecture & Protocols</h3>\n<p>The network layer provides end-to-end packet delivery across multiple network segments, creating the illusion of a unified network despite physical boundaries. Designing effective network layer protocols requires balancing scalability, efficiency, and robustness.</p>\n<p>Key aspects of network layer architecture include:</p>\n<ul>\n<li><strong>Connectionless vs. Connection-Oriented Service</strong>: Delivery guarantees and state management</li>\n<li><strong>Network Layer Addressing</strong>: Global identification of hosts across interconnected networks</li>\n<li><strong>Protocol Structures</strong>: Packet formats, header designs, and extension mechanisms</li>\n<li><strong>Fragmentation and Reassembly</strong>: Handling different maximum transfer units</li>\n<li><strong>Error Handling and Reporting</strong>: Detecting and responding to delivery failures</li>\n</ul>\n<p>We\'ll design and implement a complete network layer protocol from scratch, developing the packet formats, addressing schemes, and core services. This approach avoids existing implementations like IPv4/IPv6, instead building understanding of fundamental design decisions and trade-offs in internetworking protocols.</p>\n\n<h3>2. Addressing & Subnetting</h3>\n<p>Network addressing schemes allow devices to be uniquely identified across vast interconnected systems. Subnetting divides address spaces into manageable segments, enabling efficient routing and address allocation.</p>\n<p>Our exploration of addressing will include:</p>\n<ul>\n<li><strong>Address Structure</strong>: Designing hierarchical addressing schemes</li>\n<li><strong>Subnet Division</strong>: CIDR notation and subnet mask calculations</li>\n<li><strong>Address Resolution</strong>: Mapping between network and link layer addresses</li>\n<li><strong>Address Assignment</strong>: Static, dynamic, and automatic configuration</li>\n<li><strong>Special Purpose Addresses</strong>: Broadcast, multicast, and reserved ranges</li>\n</ul>\n<p>We\'ll implement addressing and subnetting from first principles, building tools that can divide address spaces, perform subnet calculations, and manage address allocation. This hands-on approach reveals the mathematical and design foundations of network addressing, without relying on existing tools that hide these mechanisms.</p>\n\n<h3>3. Routing Algorithms</h3>\n<p>Routing algorithms determine the best paths for packets to travel through a network. Effective algorithms balance optimality, convergence speed, and resource usage while adapting to changing network conditions.</p>\n<p>Key routing algorithms we\'ll implement include:</p>\n<ul>\n<li><strong>Distance Vector Routing</strong>: Bellman-Ford approach to distributed path finding</li>\n<li><strong>Link State Routing</strong>: Dijkstra-based approaches with complete topology knowledge</li>\n<li><strong>Path Vector Routing</strong>: BGP-like policy-based routing with path attributes</li>\n<li><strong>Hierarchical Routing</strong>: Area-based approaches for large-scale networks</li>\n<li><strong>Multicast Routing</strong>: Efficiently delivering to multiple destinations</li>\n</ul>\n<p>We\'ll implement these algorithms from scratch, building routing protocols that can dynamically discover network topologies and compute optimal paths. This implementation approach provides insight into the distributed algorithms that make global communication possible, revealing how millions of routers coordinate without centralized control.</p>\n\n<h3>4. Packet Forwarding & Switching</h3>\n<p>Packet forwarding is the process of moving packets from input to output interfaces based on routing decisions. Efficient forwarding implementations balance speed, flexibility, and resource utilization.</p>\n<p>Our implementation of forwarding will cover:</p>\n<ul>\n<li><strong>Forwarding Table Organization</strong>: Data structures for rapid lookup</li>\n<li><strong>Longest Prefix Matching</strong>: Algorithms for finding the most specific route</li>\n<li><strong>Switching Fabrics</strong>: Hardware architectures for high-speed forwarding</li>\n<li><strong>Quality of Service</strong>: Prioritization and traffic management</li>\n<li><strong>Network Address Translation</strong>: Mapping between address domains</li>\n</ul>\n<p>We\'ll build packet forwarding engines from first principles, implementing the algorithms and data structures used in modern routers. Rather than using existing forwarding implementations, we\'ll develop our own, gaining insight into the performance optimizations and design trade-offs in high-speed networking equipment.</p>\n\n<h3>5. Internet Protocol Implementation</h3>\n<p>The Internet Protocol (IP) is the foundation of the global internet, providing a standardized internetworking layer that enables worldwide communication. Understanding IP requires implementing its key mechanisms and behaviors.</p>\n<p>Key aspects of IP implementation include:</p>\n<ul>\n<li><strong>IPv4 and IPv6 Packet Structure</strong>: Header formats and extension mechanisms</li>\n<li><strong>IP Routing Tables</strong>: Managing forwarding information</li>\n<li><strong>ICMP Implementation</strong>: Error reporting and network diagnostics</li>\n<li><strong>IP Security</strong>: Authentication and encryption at the network layer</li>\n<li><strong>IP Mobility</strong>: Maintaining connections across network changes</li>\n</ul>\n<p>We\'ll implement a simplified but functional version of IP from scratch, including packet handling, routing integration, and support services. This approach avoids using existing IP stacks, instead building deep understanding of how the internet\'s fundamental protocol works by creating our own implementation. This reveals the elegant design decisions that have allowed IP to scale from a small research network to a global communication system connecting billions of devices.</p>',
        parentId: 'networks'
    },
    {
        id: 'transport_layer',
        title: 'Transport Layer Protocols',
        description: 'Creating reliable end-to-end communication',
        content: '<h3>1. Transport Protocol Fundamentals</h3>\n<p>Transport protocols bridge the gap between application requirements and network-layer services, providing end-to-end communication with appropriate guarantees. Effective transport design requires balancing reliability, efficiency, and compatibility with diverse network conditions.</p>\n<p>Key aspects of transport protocol fundamentals include:</p>\n<ul>\n<li><strong>Connectionless vs. Connection-Oriented</strong>: Service models and their trade-offs</li>\n<li><strong>Transport Addressing</strong>: Ports and service identifiers</li>\n<li><strong>Multiplexing and Demultiplexing</strong>: Handling multiple application streams</li>\n<li><strong>Protocol Header Design</strong>: Information needed for transport functions</li>\n<li><strong>Error Detection</strong>: End-to-end validation beyond network checks</li>\n</ul>\n<p>We\'ll design and implement both connectionless and connection-oriented transport protocols from scratch, developing the fundamental mechanisms required for end-to-end communication. This approach avoids existing implementations like TCP/UDP, instead building understanding of the core design decisions in transport protocol architecture.</p>\n\n<h3>2. Reliable Data Transfer</h3>\n<p>Reliable data transfer ensures that information arrives at its destination intact, in order, and without duplication, despite network-layer limitations. Building reliable protocols requires sophisticated mechanisms for error detection and recovery.</p>\n<p>Our implementation of reliable transfer will cover:</p>\n<ul>\n<li><strong>Acknowledgment Schemes</strong>: Positive and negative acknowledgment approaches</li>\n<li><strong>Retransmission Strategies</strong>: Timeout-based and fast retransmission</li>\n<li><strong>Sequence Numbers</strong>: Tracking and ordering segments</li>\n<li><strong>Checksumming</strong>: End-to-end error detection</li>\n<li><strong>Window Management</strong>: Limiting in-flight data</li>\n</ul>\n<p>We\'ll implement reliable data transfer protocols from first principles, building both simple stop-and-wait approaches and more sophisticated sliding window mechanisms. This hands-on experience reveals the challenges of maintaining reliability over unreliable networks, without relying on existing implementations that hide these details.</p>\n\n<h3>3. Flow Control & Buffering</h3>\n<p>Flow control prevents senders from overwhelming receivers with data, ensuring that each endpoint operates within its processing and memory constraints. Effective flow control balances throughput with resource consumption.</p>\n<p>Key flow control mechanisms include:</p>\n<ul>\n<li><strong>Receiver Windows</strong>: Advertising available buffer space</li>\n<li><strong>Dynamic Window Adjustment</strong>: Adapting to changing conditions</li>\n<li><strong>Buffer Management</strong>: Organizing and prioritizing incoming data</li>\n<li><strong>Zero Window Handling</strong>: Managing fully congested receivers</li>\n<li><strong>Application Integration</strong>: Balancing transport and application buffering</li>\n</ul>\n<p>We\'ll implement flow control algorithms from scratch, building mechanisms that dynamically adjust transmission rates based on receiver feedback. This implementation-focused approach reveals the practical challenges of coordinating transmission between endpoints with different capabilities, without using existing flow control implementations.</p>\n\n<h3>4. Congestion Control</h3>\n<p>Congestion control prevents network overload by adjusting transmission rates in response to network conditions. Effective congestion control ensures fair sharing of network resources while maximizing utilization.</p>\n<p>Our exploration of congestion control will include:</p>\n<ul>\n<li><strong>Congestion Detection</strong>: Identifying network saturation</li>\n<li><strong>Slow Start</strong>: Gradually increasing transmission rate</li>\n<li><strong>Congestion Avoidance</strong>: Linear growth in stable conditions</li>\n<li><strong>Fast Recovery</strong>: Optimized response to packet loss</li>\n<li><strong>Explicit Congestion Notification</strong>: Network-assisted congestion signaling</li>\n</ul>\n<p>We\'ll implement multiple congestion control algorithms from first principles, including traditional approaches like TCP Reno and more modern variants. This hands-on experience reveals the delicate balance between aggressive transmission and network-friendly behavior, without relying on existing implementations that hide the underlying mathematics and heuristics.</p>\n\n<h3>5. Protocol Implementation & Optimization</h3>\n<p>Implementing a complete transport protocol requires integrating various mechanisms into a cohesive system while optimizing for performance and robustness. This process involves both algorithmic design and practical engineering considerations.</p>\n<p>Key aspects of transport protocol implementation include:</p>\n<ul>\n<li><strong>State Machine Design</strong>: Managing protocol phases and transitions</li>\n<li><strong>Timer Management</strong>: Scheduling and handling various timeouts</li>\n<li><strong>Performance Optimization</strong>: Minimizing overhead and maximizing throughput</li>\n<li><strong>Specialized Mechanisms</strong>: Delayed ACKs, Nagle\'s algorithm, selective acknowledgments</li>\n<li><strong>Protocol Extension</strong>: Supporting options and negotiated features</li>\n</ul>\n<p>We\'ll implement a complete transport protocol stack from scratch, developing both the individual components and the integration framework. Rather than using existing protocol implementations, we\'ll build our own, gaining insight into the complex interactions between various transport mechanisms. This approach reveals how transport protocols manage to provide simple, reliable interfaces to applications despite the complex and unreliable nature of underlying networks.</p>',
        parentId: 'networks'
    },
    {
        id: 'application_protocols',
        title: 'Application Layer Protocols',
        description: 'Building protocols for specific applications',
        content: '<h3>1. Protocol Design Principles</h3>\n<p>Application protocols define how applications exchange data over a network. Effective protocol design balances simplicity, extensibility, performance, and interoperability while addressing the specific needs of an application domain.</p>\n<p>Key protocol design principles include:</p>\n<ul>\n<li><strong>Message Formats</strong>: Structuring data for efficient processing</li>\n<li><strong>State Management</strong>: Tracking conversations between clients and servers</li>\n<li><strong>Error Handling</strong>: Responding to protocol and application errors</li>\n<li><strong>Versioning</strong>: Supporting evolution while maintaining compatibility</li>\n<li><strong>Security Considerations</strong>: Authentication, encryption, and access control</li>\n</ul>\n<p>We\'ll explore protocol design from first principles, developing frameworks for creating application-specific communication formats. This approach avoids using existing protocol specifications, instead focusing on the fundamental design decisions that shape effective networked applications.</p>\n\n<h3>2. Web Protocol Implementation</h3>\n<p>Web protocols enable the retrieval and manipulation of resources across the internet. Understanding these protocols requires implementing both client and server components to exchange structured information.</p>\n<p>Our web protocol implementation will cover:</p>\n<ul>\n<li><strong>HTTP Protocol</strong>: Request/response structure and methods</li>\n<li><strong>Header Processing</strong>: Metadata for content negotiation and control</li>\n<li><strong>Content Handling</strong>: Different media types and encodings</li>\n<li><strong>Caching Mechanisms</strong>: Optimizing repeated access</li>\n<li><strong>RESTful Design</strong>: Resource-oriented API architecture</li>\n</ul>\n<p>We\'ll implement a complete HTTP client and server from scratch, developing the protocol parsers, content handlers, and state management. Rather than using existing web frameworks or libraries, we\'ll build our own HTTP stack, gaining insight into the protocol details that enable the world wide web.</p>\n\n<h3>3. Real-Time Communication Protocols</h3>\n<p>Real-time communication protocols enable synchronous interaction between distributed applications. These protocols must balance immediacy, reliability, and efficiency while adapting to network conditions.</p>\n<p>Key aspects of real-time protocols include:</p>\n<ul>\n<li><strong>Websocket Implementation</strong>: Full-duplex communication channels</li>\n<li><strong>Event Notification</strong>: Push-based information delivery</li>\n<li><strong>Media Streaming</strong>: Audio and video transmission</li>\n<li><strong>Timing Synchronization</strong>: Coordinating distributed events</li>\n<li><strong>Reliability Mechanisms</strong>: Handling packet loss in time-sensitive applications</li>\n</ul>\n<p>We\'ll implement multiple real-time protocols from first principles, building both the transport mechanisms and the application logic. This hands-on approach reveals the unique challenges of time-sensitive communications without relying on existing implementations that hide the underlying complexity.</p>\n\n<h3>4. File Transfer & Content Distribution</h3>\n<p>File transfer protocols enable the reliable and efficient exchange of data between systems. Effective protocols balance reliability, efficiency, and functionality while handling files of varying sizes and types.</p>\n<p>Our implementation of file transfer will cover:</p>\n<ul>\n<li><strong>FTP-like Protocols</strong>: Command and data channel architecture</li>\n<li><strong>Chunked Transfer</strong>: Handling large files in manageable pieces</li>\n<li><strong>Resumable Transfers</strong>: Recovering from interruptions</li>\n<li><strong>Integrity Verification</strong>: Ensuring uncorrupted delivery</li>\n<li><strong>Peer-to-Peer Distribution</strong>: Decentralized content sharing</li>\n</ul>\n<p>We\'ll build complete file transfer protocols from scratch, implementing both client and server components. This implementation-focused approach provides insight into the challenges of moving large data sets across networks, without using existing file transfer implementations that hide these complexities.</p>\n\n<h3>5. Application Protocol Security</h3>\n<p>Security is essential in application protocols to protect data confidentiality, integrity, and availability. Implementing secure protocols requires understanding both cryptographic principles and practical security mechanisms.</p>\n<p>Key aspects of protocol security include:</p>\n<ul>\n<li><strong>TLS Implementation</strong>: Transport layer security from scratch</li>\n<li><strong>Authentication Protocols</strong>: Verifying identity securely</li>\n<li><strong>Authorization Frameworks</strong>: Controlling resource access</li>\n<li><strong>Secure Message Formats</strong>: Protecting content with encryption and signatures</li>\n<li><strong>Threat Modeling</strong>: Identifying and mitigating protocol vulnerabilities</li>\n</ul>\n<p>We\'ll implement security mechanisms from first principles, building our own encryption, authentication, and authorization systems. Rather than using existing security libraries that hide implementation details, we\'ll develop our own secure protocol components, gaining insight into the cryptographic foundations and security engineering practices that protect networked applications. This approach reveals the challenges in balancing security with usability and performance in real-world applications.</p>',
        parentId: 'networks'
    },

    // Algorithms & Data Structures
    {
        id: 'algorithms',
        title: 'Algorithms & Data Structures',
        description: 'Implementing efficient solutions from first principles',
        content: 'Algorithms are step-by-step procedures for solving problems, while data structures organize data for efficient access and modification. Our "Build from Scratch" approach means implementing every algorithm and data structure yourself, understanding their mathematical foundations, and analyzing their efficiency. Rather than using existing libraries, you\'ll gain the skills to design and optimize solutions for any computational problem.',
        parentId: null
    },
    {
        id: 'basic_data_structures',
        title: 'Basic Data Structures',
        description: 'Fundamental ways to organize data',
        content: '<h3>1. Arrays & Memory Management</h3>\n<p>Arrays are the most fundamental data structure, providing direct access to elements through indices. Understanding arrays requires knowledge of how memory is allocated, addressed, and managed in computer systems.</p>\n<p>Key aspects of array implementation include:</p>\n<ul>\n<li><strong>Memory Allocation</strong>: Static vs. dynamic allocation techniques</li>\n<li><strong>Addressing Schemes</strong>: Computing element locations from indices</li>\n<li><strong>Multi-dimensional Arrays</strong>: Row-major vs. column-major storage</li>\n<li><strong>Resizing Strategies</strong>: Dynamic arrays and growth factors</li>\n<li><strong>Memory Management</strong>: Allocation, deallocation, and garbage collection</li>\n</ul>\n<p>We\'ll implement arrays from first principles, building both static and dynamic array types with different memory allocation strategies. Rather than using language-provided array types, we\'ll develop our own implementation from basic memory operations, gaining insight into the trade-offs between different array designs.</p>\n\n<h3>2. Linked Lists & Pointer Structures</h3>\n<p>Linked lists organize data in nodes with references to other nodes, enabling efficient insertion and deletion operations. These structures form the basis for many more complex data structures and algorithms.</p>\n<p>Our implementation of linked lists will cover:</p>\n<ul>\n<li><strong>Singly-Linked Lists</strong>: Forward traversal with minimal memory</li>\n<li><strong>Doubly-Linked Lists</strong>: Bidirectional traversal capabilities</li>\n<li><strong>Circular Lists</strong>: Special cases and applications</li>\n<li><strong>Sentinel Nodes</strong>: Simplifying edge cases with dummy nodes</li>\n<li><strong>Pointer Manipulation</strong>: Safe techniques for maintaining list integrity</li>\n</ul>\n<p>We\'ll build these list structures from scratch, implementing all operations including traversal, insertion, deletion, and searching. This hands-on approach reveals the subtle challenges of pointer manipulation and memory management, without relying on existing list implementations that hide these complexities.</p>\n\n<h3>3. Stacks & Queues</h3>\n<p>Stacks and queues are abstract data types with specific access patterns that appear throughout computing. These structures enforce particular ordering disciplines that solve numerous algorithmic problems.</p>\n<p>Key stack and queue implementations include:</p>\n<ul>\n<li><strong>Array-Based Stack</strong>: Using arrays for LIFO access</li>\n<li><strong>Linked Stack</strong>: Implementing stacks with linked nodes</li>\n<li><strong>Array Circular Queue</strong>: Efficient FIFO with fixed arrays</li>\n<li><strong>Linked Queue</strong>: Flexible FIFO with linked structures</li>\n<li><strong>Priority Queues</strong>: Ordering elements by priority values</li>\n</ul>\n<p>We\'ll implement multiple versions of stacks and queues from first principles, using both array-based and linked approaches. This implementation approach highlights the trade-offs between different implementation strategies and reveals how the choice of underlying structure affects performance characteristics.</p>\n\n<h3>4. Hash Tables & Associative Arrays</h3>\n<p>Hash tables enable rapid lookup of values based on keys, providing average-case constant time access. These structures form the foundation for numerous algorithms and higher-level data structures.</p>\n<p>Our hash table implementation will cover:</p>\n<ul>\n<li><strong>Hash Functions</strong>: Designing and analyzing effective functions</li>\n<li><strong>Collision Resolution</strong>: Chaining, open addressing, and hybrid strategies</li>\n<li><strong>Load Factor Analysis</strong>: Understanding occupancy and performance</li>\n<li><strong>Rehashing</strong>: Dynamically resizing hash tables</li>\n<li><strong>Perfect Hashing</strong>: Guaranteed worst-case performance techniques</li>\n</ul>\n<p>We\'ll build hash tables from scratch, developing multiple hash functions and collision resolution strategies. This hands-on experience reveals the mathematics of hashing, the subtle trade-offs in hash table design, and the techniques for achieving reliable performance, without using existing implementations that hide these important details.</p>\n\n<h3>5. Sets & Maps</h3>\n<p>Sets and maps are abstract data types that manage collections of unique elements and key-value pairs, respectively. Their efficient implementation requires careful consideration of the underlying storage mechanisms.</p>\n<p>Key aspects of set and map implementations include:</p>\n<ul>\n<li><strong>Hash-Based Sets</strong>: Using hash tables for membership testing</li>\n<li><strong>Tree-Based Sets</strong>: Maintaining element ordering</li>\n<li><strong>Hash Maps</strong>: Optimizing key-value association</li>\n<li><strong>Ordered Maps</strong>: Supporting key-based range operations</li>\n<li><strong>Multi-Maps</strong>: Handling multiple values per key</li>\n</ul>\n<p>We\'ll implement these abstract data types from the ground up, building on our previous data structure implementations. Rather than using language-provided collections, we\'ll develop our own set and map variants, gaining insight into how these high-level abstractions translate to efficient concrete implementations. This approach reveals the algorithmic foundations that make modern collection libraries possible.</p>',
        parentId: 'algorithms'
    },
    {
        id: 'advanced_data_structures',
        title: 'Advanced Data Structures',
        description: 'Sophisticated structures for complex problems',
        content: '<h3>1. Self-Balancing Search Trees</h3>\n<p>Self-balancing binary search trees maintain balanced height during insertions and deletions, ensuring logarithmic time operations. These structures provide efficient implementations for ordered sets and maps.</p>\n<p>Key self-balancing tree implementations include:</p>\n<ul>\n<li><strong>AVL Trees</strong>: Height-balanced trees with strict balance factors</li>\n<li><strong>Red-Black Trees</strong>: Color-balanced trees with relaxed constraints</li>\n<li><strong>Splay Trees</strong>: Self-adjusting trees that move frequently accessed items near the root</li>\n<li><strong>Treaps</strong>: Randomized binary search trees with heap priorities</li>\n<li><strong>Weight-Balanced Trees</strong>: Trees balanced by node subtree sizes</li>\n</ul>\n<p>We\'ll implement multiple self-balancing tree variants from scratch, developing the complex rotation and rebalancing operations that maintain their properties. This implementation approach provides insight into the mathematical foundations of tree balancing and the algorithmic techniques that ensure efficient operations.</p>\n\n<h3>2. B-Trees & External Memory Structures</h3>\n<p>B-trees and related structures efficiently organize data for storage systems where access times are dominated by disk I/O operations. These structures minimize disk accesses while supporting searches, insertions, and deletions.</p>\n<p>Our exploration of B-tree variants will include:</p>\n<ul>\n<li><strong>B-Trees</strong>: Multi-way search trees with guaranteed minimum fill</li>\n<li><strong>B+ Trees</strong>: B-trees with sequential leaf access for range queries</li>\n<li><strong>B* Trees</strong>: B-trees with higher minimum fill guarantees</li>\n<li><strong>Buffer Trees</strong>: B-tree variants optimized for batched operations</li>\n<li><strong>Fractal Trees</strong>: B-tree alternatives with optimized write performance</li>\n</ul>\n<p>We\'ll implement these external memory structures from first principles, building systems that efficiently manage large datasets stored on disk. This hands-on approach reveals the special considerations required when optimizing for external storage rather than RAM, without relying on existing implementations that hide these important design decisions.</p>\n\n<h3>3. Heap Structures & Priority Queues</h3>\n<p>Heaps efficiently maintain the minimum or maximum element in a collection, supporting priority queue operations. These structures appear throughout algorithms and systems that require ordering by priority.</p>\n<p>Key heap implementations include:</p>\n<ul>\n<li><strong>Binary Heaps</strong>: Complete binary trees with heap property</li>\n<li><strong>d-ary Heaps</strong>: Generalized heaps with tunable branching factor</li>\n<li><strong>Fibonacci Heaps</strong>: Heap structures with amortized efficiency</li>\n<li><strong>Pairing Heaps</strong>: Self-adjusting heaps with simple implementation</li>\n<li><strong>Leftist and Skew Heaps</strong>: Mergeable priority queues</li>\n</ul>\n<p>We\'ll implement multiple heap variants from scratch, developing the key operations like insertion, extraction, and decrease-key. This implementation-focused approach highlights the trade-offs between different heap structures and reveals how theoretical operation counts translate to practical performance characteristics.</p>\n\n<h3>4. Trie Structures & String Processing</h3>\n<p>Tries efficiently store and search collections of strings, providing operations that depend on key length rather than collection size. These structures are fundamental to text processing, auto-completion, and string matching algorithms.</p>\n<p>Our trie implementation will cover:</p>\n<ul>\n<li><strong>Standard Tries</strong>: Character-by-character string indexing</li>\n<li><strong>Compressed Tries</strong>: Path compression for space efficiency</li>\n<li><strong>Ternary Search Tries</strong>: Hybrid trie/BST structures</li>\n<li><strong>Suffix Trees</strong>: Compressed tries containing all suffixes</li>\n<li><strong>Burst Tries</strong>: Cache-conscious string indexing structures</li>\n</ul>\n<p>We\'ll build these string-optimized data structures from first principles, implementing efficient storage, search, and pattern matching operations. This hands-on experience reveals the specialized techniques that make string processing efficient, without using existing libraries that hide these algorithmic details.</p>\n\n<h3>5. Advanced Graph Representations</h3>\n<p>Graph data structures represent relationships between objects, supporting efficient traversal and connectivity operations. Different representations offer various trade-offs depending on graph density, access patterns, and memory constraints.</p>\n<p>Key graph representations include:</p>\n<ul>\n<li><strong>Adjacency Matrices</strong>: Dense representations with constant-time edge lookup</li>\n<li><strong>Adjacency Lists</strong>: Space-efficient sparse representations</li>\n<li><strong>Compressed Sparse Row (CSR)</strong>: Memory-efficient static graphs</li>\n<li><strong>Dynamic Graph Structures</strong>: Supporting efficient modifications</li>\n<li><strong>Specialized Representations</strong>: Planar graphs, implicit graphs, etc.</li>\n</ul>\n<p>We\'ll implement multiple graph representations from scratch, building the foundational structures needed for graph algorithms. Rather than using existing graph libraries, we\'ll develop our own implementations, gaining insight into how representation choices affect algorithm performance. This approach reveals the connection between abstract graph theory and concrete computational representations, building the skills to design custom graph structures for specific application domains.</p>',
        parentId: 'algorithms'
    },
    {
        id: 'searching_sorting',
        title: 'Searching & Sorting Algorithms',
        description: 'Finding and organizing data efficiently',
        content: '<h3>1. Search Algorithm Fundamentals</h3>\n<p>Search algorithms locate specific items within collections, forming the foundation for many computational tasks. Effective implementations balance average and worst-case performance for different data distributions and access patterns.</p>\n<p>Key search algorithms include:</p>\n<ul>\n<li><strong>Linear Search</strong>: Sequential examination of each element</li>\n<li><strong>Binary Search</strong>: Divide-and-conquer approach for sorted data</li>\n<li><strong>Interpolation Search</strong>: Position estimation based on values</li>\n<li><strong>Jump Search</strong>: Block-based approach for sorted arrays</li>\n<li><strong>Exponential Search</strong>: Bounded binary search with exponential jumps</li>\n</ul>\n<p>We\'ll implement these search algorithms from scratch, analyzing their time and space complexity across various input distributions. This hands-on approach reveals the practical performance characteristics that complement theoretical analysis, without relying on existing search implementations.</p>\n\n<h3>2. Basic Sorting Algorithms</h3>\n<p>Basic sorting algorithms organize data in a specific order through simple, direct approaches. While not always the most efficient, these algorithms are often suitable for small datasets and provide insight into sorting fundamentals.</p>\n<p>Our implementation of basic sorting will cover:</p>\n<ul>\n<li><strong>Bubble Sort</strong>: Adjacent element comparisons and swaps</li>\n<li><strong>Selection Sort</strong>: Finding and positioning the minimum element</li>\n<li><strong>Insertion Sort</strong>: Building a sorted array incrementally</li>\n<li><strong>Shell Sort</strong>: Generalized insertion sort with gap sequences</li>\n<li><strong>Cycle Sort</strong>: Minimizing memory writes for in-place sorting</li>\n</ul>\n<p>We\'ll implement these sorting algorithms from first principles, measuring their performance on various input types and sizes. This implementation experience provides insight into how theoretical complexity translates to actual runtime behavior, revealing the situations where simpler algorithms may outperform asymptotically superior methods.</p>\n\n<h3>3. Advanced Sorting Algorithms</h3>\n<p>Advanced sorting algorithms achieve better asymptotic efficiency through more sophisticated strategies. These algorithms form the backbone of high-performance systems that process large datasets.</p>\n<p>Key advanced sorting implementations include:</p>\n<ul>\n<li><strong>Merge Sort</strong>: Divide-and-conquer with stable merging</li>\n<li><strong>Quick Sort</strong>: Partition-based sorting with various pivot strategies</li>\n<li><strong>Heap Sort</strong>: Selection sort using heap data structures</li>\n<li><strong>Tim Sort</strong>: Hybrid sorting algorithm combining merge and insertion sorts</li>\n<li><strong>Radix Sort</strong>: Non-comparative sorting using positional digits</li>\n</ul>\n<p>We\'ll build these advanced algorithms from scratch, exploring optimizations that improve practical performance. This hands-on approach reveals the engineering considerations that turn theoretical algorithms into efficient implementations, without using existing sorting libraries that hide these important details.</p>\n\n<h3>4. String & Special-Purpose Searching</h3>\n<p>String searching and pattern matching require specialized algorithms optimized for the unique characteristics of text and sequential data. These techniques form the foundation for text editors, compilers, and search engines.</p>\n<p>Our exploration of string searching will include:</p>\n<ul>\n<li><strong>Naive String Matching</strong>: Direct character-by-character comparison</li>\n<li><strong>Knuth-Morris-Pratt</strong>: Precomputing pattern shifts to avoid redundant comparisons</li>\n<li><strong>Boyer-Moore</strong>: Skip-based techniques for efficient matching</li>\n<li><strong>Rabin-Karp</strong>: Hash-based substring search</li>\n<li><strong>Suffix Arrays & Trees</strong>: Precomputed indexes for extensive text searching</li>\n</ul>\n<p>We\'ll implement these string searching algorithms from first principles, analyzing their behavior with different text and pattern characteristics. This implementation-focused approach highlights how specialized algorithms can dramatically outperform generic approaches for specific problem domains.</p>\n\n<h3>5. Algorithm Analysis & Optimization</h3>\n<p>Algorithm analysis provides the theoretical and empirical tools to evaluate and improve computational procedures. This foundation enables informed selection and optimization of algorithms for specific contexts.</p>\n<p>Key aspects of algorithm analysis include:</p>\n<ul>\n<li><strong>Time Complexity</strong>: Mathematical analysis of operation counts</li>\n<li><strong>Space Complexity</strong>: Memory usage patterns and efficiency</li>\n<li><strong>Empirical Analysis</strong>: Measuring actual performance on representative data</li>\n<li><strong>Algorithm Tuning</strong>: Adjusting implementations for specific conditions</li>\n<li><strong>Lower Bounds</strong>: Fundamental limits on problem complexity</li>\n</ul>\n<p>We\'ll apply rigorous analysis techniques to our algorithm implementations, comparing theoretical predictions with measured performance. This analytical approach develops the skills to reason about algorithmic efficiency beyond simple big-O notation, revealing the nuanced performance characteristics that affect real-world applications. By understanding both the theoretical and practical aspects of algorithm performance, you\'ll be equipped to design and optimize solutions for novel computational problems.</p>',
        parentId: 'algorithms'
    },
    {
        id: 'graph_algorithms',
        title: 'Graph Algorithms',
        description: 'Solving problems on interconnected data',
        content: '<h3>1. Graph Traversal Algorithms</h3>\n<p>Graph traversal algorithms systematically visit every vertex in a graph, forming the foundation for many more complex graph operations. Effective traversal requires careful tracking of visited vertices to avoid cycles.</p>\n<p>Key graph traversal techniques include:</p>\n<ul>\n<li><strong>Breadth-First Search (BFS)</strong>: Level-by-level exploration using queues</li>\n<li><strong>Depth-First Search (DFS)</strong>: Path-based exploration using recursion or stacks</li>\n<li><strong>Iterative Deepening</strong>: Combining BFS thoroughness with DFS efficiency</li>\n<li><strong>Bidirectional Search</strong>: Meeting in the middle for faster path finding</li>\n<li><strong>Topological Sort</strong>: Ordering vertices based on dependencies</li>\n</ul>\n<p>We\'ll implement these traversal algorithms from scratch for various graph representations, analyzing their behavior on different graph structures. This hands-on approach reveals how abstract traversal techniques translate to concrete code that can explore complex networks efficiently.</p>\n\n<h3>2. Shortest Path Algorithms</h3>\n<p>Shortest path algorithms find optimal routes between vertices in weighted graphs. These fundamental algorithms appear in navigation systems, network routing, and many optimization problems.</p>\n<p>Our implementation of shortest path algorithms will cover:</p>\n<ul>\n<li><strong>Dijkstra\'s Algorithm</strong>: Greedy approach for non-negative weights</li>\n<li><strong>Bellman-Ford Algorithm</strong>: Dynamic programming for graphs with negative weights</li>\n<li><strong>Floyd-Warshall Algorithm</strong>: All-pairs shortest paths through matrix operations</li>\n<li><strong>Johnson\'s Algorithm</strong>: Efficient all-pairs computation for sparse graphs</li>\n<li><strong>A* Search</strong>: Heuristic-guided pathfinding for spatial problems</li>\n</ul>\n<p>We\'ll build these algorithms from first principles, implementing priority queues, distance tables, and path reconstruction. This implementation-focused approach highlights the engineering decisions that affect practical performance, without relying on existing implementations that hide these important details.</p>\n\n<h3>3. Minimum Spanning Trees</h3>\n<p>Minimum spanning tree algorithms find connected subgraphs that include all vertices with minimum total edge weight. These algorithms appear in network design, clustering, and approximation algorithms.</p>\n<p>Key minimum spanning tree algorithms include:</p>\n<ul>\n<li><strong>Kruskal\'s Algorithm</strong>: Greedy edge selection with union-find</li>\n<li><strong>Prim\'s Algorithm</strong>: Greedy vertex expansion with priority queues</li>\n<li><strong>Borůvka\'s Algorithm</strong>: Parallel approach that merges components</li>\n<li><strong>Reverse-Delete Algorithm</strong>: Complement to Kruskal\'s using edge removal</li>\n<li><strong>Approximation Algorithms</strong>: Near-optimal solutions for constrained problems</li>\n</ul>\n<p>We\'ll implement these algorithms from scratch, exploring their behavior on different graph structures. This hands-on experience reveals the elegant mathematical properties of spanning trees and their practical applications in network design and optimization.</p>\n\n<h3>4. Network Flow Algorithms</h3>\n<p>Network flow algorithms solve problems involving capacity-constrained edges, appearing in transportation, resource allocation, and matching problems. These algorithms find maximum flow or minimum-cost flow in networks.</p>\n<p>Our exploration of network flow will include:</p>\n<ul>\n<li><strong>Ford-Fulkerson Method</strong>: Augmenting path approach to maximum flow</li>\n<li><strong>Edmonds-Karp Algorithm</strong>: BFS-based implementation of Ford-Fulkerson</li>\n<li><strong>Push-Relabel Algorithm</strong>: Vertex-based approach for maximum flow</li>\n<li><strong>Min-Cost Max-Flow</strong>: Finding optimal flows with minimal cost</li>\n<li><strong>Bipartite Matching</strong>: Assignment problems using flow techniques</li>\n</ul>\n<p>We\'ll implement these flow algorithms from first principles, building residual graphs, augmenting paths, and flow tracking mechanisms. This implementation-focused approach reveals how abstract flow concepts translate to algorithms that solve diverse practical problems in resource allocation and optimization.</p>\n\n<h3>5. Advanced Graph Algorithms</h3>\n<p>Advanced graph algorithms solve specialized problems with sophisticated techniques, extending beyond basic traversal and path-finding. These algorithms address complex graph problems in various domains.</p>\n<p>Key advanced graph algorithms include:</p>\n<ul>\n<li><strong>Strongly Connected Components</strong>: Identifying cohesive groups in directed graphs</li>\n<li><strong>Articulation Points & Bridges</strong>: Finding critical elements in graph connectivity</li>\n<li><strong>Euler Paths & Circuits</strong>: Traversing each edge exactly once</li>\n<li><strong>Hamilton Paths & Circuits</strong>: Visiting each vertex exactly once</li>\n<li><strong>Graph Coloring</strong>: Assigning labels under constraints</li>\n</ul>\n<p>We\'ll implement these advanced algorithms from scratch, exploring their applications in real-world problems. This hands-on approach develops both theoretical understanding and practical implementation skills, revealing how graph theory provides elegant solutions to complex problems in areas like network reliability, route planning, and resource scheduling. By building these algorithms yourself rather than using existing libraries, you\'ll gain deep insight into the mathematical structures that underlie interconnected systems in many domains.</p>',
        parentId: 'algorithms'
    },
    {
        id: 'algorithm_design',
        title: 'Algorithm Design Techniques',
        description: 'Strategies for creating efficient algorithms',
        content: '<h3>1. Divide and Conquer Techniques</h3>\n<p>Divide and conquer is a paradigm that breaks problems into smaller subproblems, solves them independently, and combines the results. This approach often leads to efficient recursive algorithms for a wide range of problems.</p>\n<p>Key aspects of divide and conquer include:</p>\n<ul>\n<li><strong>Problem Decomposition</strong>: Strategies for breaking problems into subproblems</li>\n<li><strong>Recurrence Relations</strong>: Analyzing algorithm complexity through recursive equations</li>\n<li><strong>Efficient Combining</strong>: Techniques for merging subsolutions effectively</li>\n<li><strong>Master Theorem</strong>: Mathematical tools for solving common recurrences</li>\n<li><strong>Parallel Opportunities</strong>: Exploiting independent subproblems for concurrency</li>\n</ul>\n<p>We\'ll implement divide and conquer algorithms from scratch for problems like sorting (merge sort, quick sort), multiplication (Karatsuba, Strassen), closest pair, and convex hull. This hands-on approach reveals how elegant recursive structures can lead to highly efficient algorithms.</p>\n\n<h3>2. Dynamic Programming</h3>\n<p>Dynamic programming solves complex problems by breaking them into overlapping subproblems and storing intermediate results. This technique converts exponential brute-force approaches into polynomial-time algorithms for many important problems.</p>\n<p>Our exploration of dynamic programming will cover:</p>\n<ul>\n<li><strong>Optimal Substructure</strong>: Identifying when problems can be decomposed optimally</li>\n<li><strong>Overlapping Subproblems</strong>: Recognizing and exploiting repeated computation</li>\n<li><strong>Memoization</strong>: Top-down approach with recursive caching</li>\n<li><strong>Tabulation</strong>: Bottom-up approach with iterative table filling</li>\n<li><strong>State Space Design</strong>: Efficiently representing subproblems</li>\n</ul>\n<p>We\'ll implement dynamic programming solutions from first principles for classic problems like knapsack, sequence alignment, shortest paths, and optimal tree structures. This implementation-focused approach develops the intuition to recognize when dynamic programming is applicable and the skills to design efficient solutions.</p>\n\n<h3>3. Greedy Algorithms</h3>\n<p>Greedy algorithms make locally optimal choices at each step with the hope of finding a global optimum. When applicable, they often provide simple and efficient solutions to complex optimization problems.</p>\n<p>Key aspects of greedy algorithm design include:</p>\n<ul>\n<li><strong>Greedy Choice Property</strong>: Proving local optimality leads to global optimality</li>\n<li><strong>Optimal Substructure</strong>: Ensuring subproblems maintain optimality</li>\n<li><strong>Exchange Arguments</strong>: Mathematical techniques for proving correctness</li>\n<li><strong>Matroid Theory</strong>: Mathematical foundations of greedy optimality</li>\n<li><strong>Approximation Ratios</strong>: Analyzing how close to optimal greedy solutions are</li>\n</ul>\n<p>We\'ll implement greedy algorithms from scratch for problems like activity selection, Huffman coding, minimum spanning trees, and set covering. This hands-on experience develops the skills to identify when greedy approaches are appropriate and how to prove their correctness.</p>\n\n<h3>4. Backtracking & Branch and Bound</h3>\n<p>Backtracking and branch and bound are systematic methods for exploring solution spaces, particularly useful for combinatorial problems where exhaustive search is necessary but can be optimized.</p>\n<p>Our implementation of these techniques will cover:</p>\n<ul>\n<li><strong>State Space Trees</strong>: Organizing and traversing potential solutions</li>\n<li><strong>Constraint Satisfaction</strong>: Filtering invalid partial solutions early</li>\n<li><strong>Bounding Functions</strong>: Eliminating unpromising solution branches</li>\n<li><strong>Search Strategies</strong>: Depth-first, breadth-first, and best-first exploration</li>\n<li><strong>Pruning Techniques</strong>: Reducing the search space effectively</li>\n</ul>\n<p>We\'ll build backtracking and branch and bound algorithms from first principles for problems like N-Queens, graph coloring, traveling salesman, and integer programming. This implementation-focused approach reveals how to efficiently navigate enormous solution spaces without examining every possibility.</p>\n\n<h3>5. Advanced Algorithmic Paradigms</h3>\n<p>Advanced algorithmic paradigms provide powerful frameworks for solving particularly challenging computational problems. These techniques extend beyond standard approaches to address problems that would otherwise be intractable.</p>\n<p>Key advanced paradigms include:</p>\n<ul>\n<li><strong>Amortized Analysis</strong>: Averaging operation costs over sequences</li>\n<li><strong>Randomized Algorithms</strong>: Using randomness for efficiency or simplicity</li>\n<li><strong>Approximation Algorithms</strong>: Finding near-optimal solutions to hard problems</li>\n<li><strong>Online Algorithms</strong>: Making decisions with incomplete future information</li>\n<li><strong>Parallel Algorithms</strong>: Designing for concurrent execution</li>\n</ul>\n<p>We\'ll implement algorithms based on these advanced paradigms from scratch, applying them to real-world problems. This hands-on experience with cutting-edge algorithmic techniques provides the tools to approach problems that resist traditional solutions. Rather than simply using existing implementations, you\'ll gain the insights needed to design novel algorithms for the most challenging computational problems you encounter.</p>',
        parentId: 'algorithms'
    },

    // Databases
    {
        id: 'databases',
        title: 'Databases',
        description: 'Building database systems from first principles',
        content: 'Database systems provide organized mechanisms to store, manage, and retrieve information. Our "Build from Scratch" approach means implementing every component of a database system yourself, from storage engines and query processors to transaction managers. Rather than just using existing database systems, you\'ll understand how they work internally and be able to design storage solutions for any data management problem.',
        parentId: null
    },
    {
        id: 'storage_engines',
        title: 'Storage Engines & Data Organization',
        description: 'How databases physically store and retrieve data',
        content: '<h3>1. Disk-Based Storage Fundamentals</h3>\n<p>Database systems must efficiently organize data on persistent storage devices with fundamentally different performance characteristics than memory. Understanding these physical constraints is essential for designing effective storage engines.</p>\n<p>Key aspects of disk-based storage include:</p>\n<ul>\n<li><strong>I/O Characteristics</strong>: Understanding seek times, rotational latency, and transfer rates</li>\n<li><strong>File System Interaction</strong>: Working with underlying operating system abstractions</li>\n<li><strong>Block-Based Access</strong>: Organizing data for efficient block reads and writes</li>\n<li><strong>Buffer Management</strong>: Caching strategies for frequently accessed data</li>\n<li><strong>Storage Hierarchies</strong>: Balancing performance across memory, SSD, and HDD</li>\n</ul>\n<p>We\'ll implement a block-based storage manager from scratch, designing low-level I/O operations that efficiently transfer data between disk and memory. This implementation approach builds fundamental understanding of the physical limitations that shape database architecture.</p>\n\n<h3>2. Record & Page Formats</h3>\n<p>Database systems organize information into records (rows) that are grouped into pages (blocks) for efficient storage and retrieval. The design of these structures significantly impacts performance and space utilization.</p>\n<p>Our implementation of record and page formats will cover:</p>\n<ul>\n<li><strong>Fixed-Length Records</strong>: Simple but space-inefficient storage</li>\n<li><strong>Variable-Length Records</strong>: Efficient storage with more complex management</li>\n<li><strong>Page Headers</strong>: Metadata for managing page contents</li>\n<li><strong>Slot-Based Addressing</strong>: Indirection for flexible record placement</li>\n<li><strong>Record Versions</strong>: Supporting multi-version concurrency control</li>\n</ul>\n<p>We\'ll build record and page managers from first principles, implementing serialization, deserialization, and page layout algorithms. This hands-on experience reveals the intricate trade-offs between space efficiency, access speed, and implementation complexity in database storage design.</p>\n\n<h3>3. File Organizations & Access Methods</h3>\n<p>File organizations determine how records are physically arranged on disk, profoundly affecting query performance for different access patterns. Effective design requires balancing competing requirements for various query types.</p>\n<p>Key file organization techniques include:</p>\n<ul>\n<li><strong>Heap Files</strong>: Unordered collections with efficient insertion</li>\n<li><strong>Sequential Files</strong>: Ordered storage for range queries</li>\n<li><strong>Hash-Based Organization</strong>: Direct access for equality predicates</li>\n<li><strong>Multi-Level Organizations</strong>: Combining approaches for diverse workloads</li>\n<li><strong>Partitioning Strategies</strong>: Distributing data for parallel access</li>\n</ul>\n<p>We\'ll implement multiple file organization techniques from scratch, developing the algorithms for record insertion, deletion, and retrieval under each approach. This implementation-focused exploration reveals how physical organization dramatically affects database performance for different query patterns.</p>\n\n<h3>4. Indexing Data Structures</h3>\n<p>Indexes accelerate data retrieval by creating auxiliary data structures that allow the database to locate records without scanning entire tables. Different index structures optimize for different query patterns.</p>\n<p>Our implementation of indexing structures will cover:</p>\n<ul>\n<li><strong>B-Tree Indexes</strong>: Balanced tree structures for ordered data</li>\n<li><strong>B+ Tree Variants</strong>: Optimizations for database-specific requirements</li>\n<li><strong>Hash Indexes</strong>: Constant-time lookup for equality queries</li>\n<li><strong>Bitmap Indexes</strong>: Compact representations for low-cardinality data</li>\n<li><strong>Spatial Indexes</strong>: R-trees and quadtrees for geometric data</li>\n</ul>\n<p>We\'ll build these index structures from first principles, implementing the insertion, deletion, and search algorithms specific to each structure. This hands-on approach provides deep insight into the complex balancing act between maintenance overhead and query acceleration that indexes present.</p>\n\n<h3>5. Storage Engine Architecture</h3>\n<p>A complete storage engine integrates multiple components into a cohesive system that efficiently manages database files, pages, records, and indexes. This integration requires careful attention to interfaces, resource management, and system behavior.</p>\n<p>Key aspects of storage engine architecture include:</p>\n<ul>\n<li><strong>Component Layering</strong>: Organizing disk, buffer, file, and index managers</li>\n<li><strong>Catalog Management</strong>: Tracking schema and storage metadata</li>\n<li><strong>Recovery Integration</strong>: Supporting durability through logging</li>\n<li><strong>Concurrency Management</strong>: Coordinating multi-user access</li>\n<li><strong>Extensibility</strong>: Pluggable storage engines for different workloads</li>\n</ul>\n<p>We\'ll design and implement a complete storage engine architecture from scratch, integrating our page formats, file organizations, and index structures into a cohesive system. This implementation experience reveals how the components of a storage engine interact to provide the foundation for database functionality, without relying on existing implementations that hide these important architectural decisions.</p>',
        parentId: 'databases'
    },
    {
        id: 'query_processing',
        title: 'Query Processing & Execution',
        description: 'Translating queries into efficient operations',
        content: '<h3>1. Query Language Design & Parsing</h3>\n<p>Query languages provide the interface between users and database systems, allowing declarative descriptions of desired data. Translating these descriptions into executable operations begins with language design and parsing.</p>\n<p>Key aspects of query language processing include:</p>\n<ul>\n<li><strong>SQL Grammar Design</strong>: Formal specification of language syntax</li>\n<li><strong>Lexical Analysis</strong>: Breaking queries into tokens</li>\n<li><strong>Syntactic Parsing</strong>: Building parse trees from token streams</li>\n<li><strong>Semantic Analysis</strong>: Validating references and type checking</li>\n<li><strong>Internal Representations</strong>: Converting parse trees to logical query plans</li>\n</ul>\n<p>We\'ll implement a query parser from scratch, building lexers, parsers, and semantic analyzers for a SQL-like language. This implementation approach reveals the complexities of language processing that form the foundation of database query systems.</p>\n\n<h3>2. Relational Algebra & Logical Query Plans</h3>\n<p>Relational algebra provides the mathematical foundation for query processing, defining operations that manipulate relations (tables) to produce results. Logical query plans represent queries as trees of these operations.</p>\n<p>Our implementation of relational algebra will cover:</p>\n<ul>\n<li><strong>Selection</strong>: Filtering rows based on predicates</li>\n<li><strong>Projection</strong>: Selecting and transforming columns</li>\n<li><strong>Join Operations</strong>: Combining relations (inner, outer, semi-joins)</li>\n<li><strong>Set Operations</strong>: Union, intersection, difference</li>\n<li><strong>Aggregation</strong>: Grouping and applying functions to groups</li>\n</ul>\n<p>We\'ll build a logical query plan representation from first principles, implementing operators and transformations that express the semantics of database queries. This hands-on approach provides insight into the abstract representations that bridge human-readable queries and executable plans.</p>\n\n<h3>3. Query Optimization</h3>\n<p>Query optimization transforms logical query plans into equivalent but more efficient forms, potentially improving performance by orders of magnitude. Effective optimization requires both rule-based transformations and cost-based selection.</p>\n<p>Key query optimization techniques include:</p>\n<ul>\n<li><strong>Algebraic Transformations</strong>: Applying equivalence rules to rewrite queries</li>\n<li><strong>Predicate Pushdown</strong>: Moving filters earlier in the plan</li>\n<li><strong>Join Ordering</strong>: Finding optimal sequences for multiple joins</li>\n<li><strong>Cost Estimation</strong>: Predicting execution costs using statistics</li>\n<li><strong>Plan Enumeration</strong>: Efficiently exploring the space of possible plans</li>\n</ul>\n<p>We\'ll implement a query optimizer from scratch, developing both rule-based transformations and cost-based search algorithms. This implementation-focused approach reveals the sophisticated techniques that enable database systems to execute complex queries efficiently.</p>\n\n<h3>4. Physical Query Execution</h3>\n<p>Physical query execution transforms logical plans into concrete algorithms that process data. Different physical implementations of logical operations have varying performance characteristics depending on data and system properties.</p>\n<p>Our implementation of physical query execution will cover:</p>\n<ul>\n<li><strong>Access Methods</strong>: Table scans, index scans, and sampling</li>\n<li><strong>Join Algorithms</strong>: Nested loops, sort-merge, and hash joins</li>\n<li><strong>Sorting Techniques</strong>: External sorting for large datasets</li>\n<li><strong>Aggregation Implementations</strong>: Sorting vs. hashing approaches</li>\n<li><strong>Expression Evaluation</strong>: Interpreting and compiling predicates</li>\n</ul>\n<p>We\'ll build physical operators from first principles, implementing the algorithms that actually manipulate data during query execution. This hands-on experience provides insight into how logical operations translate to concrete processing steps and how operator selection dramatically affects performance.</p>\n\n<h3>5. Query Execution Engines</h3>\n<p>Query execution engines coordinate the evaluation of query plans, managing resources and data flow between operators. Different execution models offer various trade-offs in performance, resource usage, and implementation complexity.</p>\n<p>Key aspects of query execution engines include:</p>\n<ul>\n<li><strong>Iterator (Volcano) Model</strong>: Pull-based operator execution</li>\n<li><strong>Materialization Model</strong>: Fully materializing intermediate results</li>\n<li><strong>Vectorized Execution</strong>: Processing batches of tuples at once</li>\n<li><strong>Just-In-Time Compilation</strong>: Generating query-specific code</li>\n<li><strong>Parallel Execution</strong>: Distributing work across multiple cores</li>\n</ul>\n<p>We\'ll implement multiple execution engine architectures from scratch, developing the frameworks that coordinate operator execution. This implementation approach reveals the system-level concerns in query processing, including memory management, scheduling, and data flow control. By building these components yourself rather than using existing database engines, you\'ll gain insight into the engineering decisions that enable efficient execution of complex queries over large datasets.</p>',
        parentId: 'databases'
    },
    {
        id: 'transaction_management',
        title: 'Transaction Management',
        description: 'Ensuring data consistency during concurrent operations',
        content: '<h3>1. Transaction Fundamentals & ACID Properties</h3>\n<p>Transactions provide logical units of work that must be processed reliably and independently of other transactions. The ACID properties (Atomicity, Consistency, Isolation, Durability) define the guarantees that database systems must provide.</p>\n<p>Key aspects of transaction fundamentals include:</p>\n<ul>\n<li><strong>Atomicity</strong>: All-or-nothing execution guarantees</li>\n<li><strong>Consistency</strong>: Preserving database invariants</li>\n<li><strong>Isolation</strong>: Preventing interference between concurrent transactions</li>\n<li><strong>Durability</strong>: Ensuring committed changes survive failures</li>\n<li><strong>Transaction Models</strong>: Flat, nested, and distributed transactions</li>\n</ul>\n<p>We\'ll explore the theoretical foundations of transactions and implement a transaction manager interface from first principles. This implementation approach reveals the fundamental challenges of maintaining correctness in concurrent database systems.</p>\n\n<h3>2. Concurrency Control & Isolation Levels</h3>\n<p>Concurrency control mechanisms prevent transactions from interfering with each other while maximizing parallel execution. Different isolation levels provide varying trade-offs between correctness guarantees and performance.</p>\n<p>Our implementation of concurrency control will cover:</p>\n<ul>\n<li><strong>Pessimistic Control</strong>: Locking-based approaches (2PL, strict 2PL)</li>\n<li><strong>Optimistic Control</strong>: Validation-based approaches (OCC)</li>\n<li><strong>Multi-Version Control</strong>: Snapshot isolation and MVCC</li>\n<li><strong>Isolation Levels</strong>: Read uncommitted through serializable</li>\n<li><strong>Anomalies</strong>: Preventing dirty reads, non-repeatable reads, phantoms</li>\n</ul>\n<p>We\'ll implement multiple concurrency control methods from scratch, exploring their performance characteristics and correctness guarantees. This hands-on approach reveals the subtle interplay between concurrency, consistency, and performance in database systems.</p>\n\n<h3>3. Lock Management & Deadlock Handling</h3>\n<p>Lock management coordinates access to shared database objects, while deadlock handling addresses situations where transactions are permanently blocked waiting for each other. These mechanisms are critical for pessimistic concurrency control.</p>\n<p>Key aspects of lock management include:</p>\n<ul>\n<li><strong>Lock Granularity</strong>: Page-level vs. record-level vs. field-level</li>\n<li><strong>Lock Types</strong>: Shared, exclusive, update, intention locks</li>\n<li><strong>Lock Compatibility</strong>: Matrices defining legal concurrent locks</li>\n<li><strong>Deadlock Detection</strong>: Wait-for graph algorithms</li>\n<li><strong>Deadlock Resolution</strong>: Victim selection and transaction abortion</li>\n</ul>\n<p>We\'ll implement a comprehensive lock manager from scratch, including deadlock detection and resolution. This implementation-focused approach highlights the system-level challenges of managing concurrency in transactional systems effectively.</p>\n\n<h3>4. Recovery & Durability Mechanisms</h3>\n<p>Recovery mechanisms ensure that database systems can restore consistent state after failures, while durability guarantees that committed transactions persist. These components are essential for reliable database operation.</p>\n<p>Our implementation of recovery systems will cover:</p>\n<ul>\n<li><strong>Write-Ahead Logging</strong>: Ensuring changes are logged before applied</li>\n<li><strong>Physical vs. Logical Logging</strong>: Trading size for recovery complexity</li>\n<li><strong>Checkpoint Mechanisms</strong>: Limiting recovery time after failures</li>\n<li><strong>ARIES Recovery</strong>: Analysis, redo, and undo phases</li>\n<li><strong>Shadow Paging</strong>: Alternative approach to logging</li>\n</ul>\n<p>We\'ll build a recovery manager from first principles, implementing logging, checkpointing, and failure recovery. This hands-on experience reveals the sophisticated techniques required to maintain durability and recover correctly from various types of failures.</p>\n\n<h3>5. Distributed Transaction Management</h3>\n<p>Distributed transaction management extends transaction processing across multiple database systems or nodes, maintaining ACID properties despite network partitions, independent failures, and message delays.</p>\n<p>Key aspects of distributed transaction management include:</p>\n<ul>\n<li><strong>Two-Phase Commit</strong>: Ensuring atomic outcomes across sites</li>\n<li><strong>Three-Phase Commit</strong>: Addressing coordinator failure limitations</li>\n<li><strong>Distributed Deadlock Detection</strong>: Finding cycles across sites</li>\n<li><strong>Global Serializability</strong>: Ensuring consistent transaction ordering</li>\n<li><strong>Failure Models</strong>: Handling various types of system failures</li>\n</ul>\n<p>We\'ll implement distributed transaction protocols from scratch, building coordinators and participants that can maintain consistency across distributed sites. This implementation approach reveals the additional complexities introduced when transaction processing spans multiple independent systems, and the techniques developed to address these challenges while maintaining ACID guarantees.</p>',
        parentId: 'databases'
    },
    {
        id: 'database_design',
        title: 'Database Design & Normalization',
        description: 'Structuring data for consistency and efficiency',
        content: '<h3>1. Entity-Relationship Modeling</h3>\n<p>Entity-Relationship (ER) modeling provides a conceptual framework for representing real-world objects and their relationships in database systems. Effective ER modeling forms the foundation for logical database design.</p>\n<p>Key aspects of ER modeling include:</p>\n<ul>\n<li><strong>Entity Types & Attributes</strong>: Identifying objects and their properties</li>\n<li><strong>Relationship Types</strong>: Representing associations between entities</li>\n<li><strong>Cardinality Constraints</strong>: One-to-one, one-to-many, many-to-many</li>\n<li><strong>Weak Entities</strong>: Entities dependent on others for identification</li>\n<li><strong>Specialization & Generalization</strong>: Modeling hierarchical relationships</li>\n</ul>\n<p>We\'ll develop ER diagrams from scratch for various application domains, implementing notation systems and transformation rules. This hands-on approach builds the conceptual design skills that precede logical database implementation.</p>\n\n<h3>2. Relational Model Theory</h3>\n<p>The relational model provides the mathematical foundation for modern database systems, defining principles for organizing and manipulating data in tables with rows and columns. Understanding this model is essential for effective database design.</p>\n<p>Our exploration of relational theory will cover:</p>\n<ul>\n<li><strong>Relations, Tuples & Attributes</strong>: Mathematical basis of tables</li>\n<li><strong>Relational Algebra</strong>: Formal operations on relations</li>\n<li><strong>Relational Calculus</strong>: Declarative specification of queries</li>\n<li><strong>Integrity Constraints</strong>: Domain, key, and referential constraints</li>\n<li><strong>View Theory</strong>: Virtual relations derived from base relations</li>\n</ul>\n<p>We\'ll implement elements of the relational model from first principles, developing systems to represent and manipulate relations mathematically. This implementation-focused approach provides deep insight into the theoretical underpinnings of relational databases.</p>\n\n<h3>3. Normalization & Functional Dependencies</h3>\n<p>Normalization eliminates redundancy and anomalies in relational database designs through systematic decomposition based on functional dependencies. This process ensures data integrity while minimizing storage requirements.</p>\n<p>Key normalization concepts include:</p>\n<ul>\n<li><strong>Functional Dependencies</strong>: Constraints between attributes</li>\n<li><strong>Normal Forms</strong>: 1NF through 5NF and BCNF</li>\n<li><strong>Decomposition Algorithms</strong>: Splitting relations while preserving dependencies</li>\n<li><strong>Dependency Preservation</strong>: Maintaining constraint enforcement ability</li>\n<li><strong>Lossless Join Property</strong>: Ensuring correct reconstruction of data</li>\n</ul>\n<p>We\'ll implement normalization algorithms from scratch, developing systems to analyze functional dependencies, determine normal forms, and decompose relations. This hands-on approach reveals the mathematical foundations of database design that guide structured schema development.</p>\n\n<h3>4. Schema Refinement & Physical Design</h3>\n<p>Schema refinement translates normalized logical designs into physical database implementations, considering performance requirements, access patterns, and system constraints. This process bridges theoretical design and practical implementation.</p>\n<p>Our implementation of schema refinement will cover:</p>\n<ul>\n<li><strong>Denormalization Strategies</strong>: Trading redundancy for performance</li>\n<li><strong>Indexes & Partitioning</strong>: Physical access optimization</li>\n<li><strong>Data Types & Constraints</strong>: Implementation-specific choices</li>\n<li><strong>Storage Layout Decisions</strong>: Clustering and organization</li>\n<li><strong>Performance Modeling</strong>: Estimating query costs on schemas</li>\n</ul>\n<p>We\'ll develop schema refinement methodologies from first principles, implementing tools to transform logical models into optimized physical designs. This implementation-focused approach balances theoretical correctness with practical performance considerations.</p>\n\n<h3>5. Advanced Schema Design Patterns</h3>\n<p>Advanced schema design patterns address complex data management requirements beyond basic normalization, providing specialized structures for particular application domains and access patterns.</p>\n<p>Key advanced design patterns include:</p>\n<ul>\n<li><strong>Temporal Data Modeling</strong>: Representing time-varying information</li>\n<li><strong>Hierarchical Data in Relations</strong>: Trees and graphs in tables</li>\n<li><strong>Polymorphic Associations</strong>: Flexibility in entity relationships</li>\n<li><strong>Data Warehousing Schemas</strong>: Star and snowflake designs</li>\n<li><strong>Domain-Specific Patterns</strong>: Inventory, accounting, reservation systems</li>\n</ul>\n<p>We\'ll implement these advanced patterns from scratch, developing specialized schema structures for various application domains. This hands-on approach reveals how theoretical design principles can be adapted to address specific practical challenges, without relying on off-the-shelf solutions that hide the reasoning behind specialized database structures.</p>',
        parentId: 'databases'
    },
    {
        id: 'distributed_databases',
        title: 'Distributed Database Systems',
        description: 'Building databases that span multiple machines',
        content: '<h3>1. Distributed Database Architecture</h3>\n<p>Distributed database architectures extend traditional database systems across multiple machines, introducing new challenges and opportunities for scalability, reliability, and performance. Effective design requires balancing competing distributed system requirements.</p>\n<p>Key aspects of distributed database architecture include:</p>\n<ul>\n<li><strong>System Models</strong>: Shared-nothing, shared-disk, and shared-memory architectures</li>\n<li><strong>Data Distribution</strong>: Centralized, partitioned, and replicated approaches</li>\n<li><strong>Query Processing</strong>: Distributed execution and optimization</li>\n<li><strong>Transaction Management</strong>: Coordination across distributed nodes</li>\n<li><strong>Heterogeneity</strong>: Managing diverse systems in federation</li>\n</ul>\n<p>We\'ll design distributed database architectures from first principles, implementing the core components that enable multi-node database operation. This hands-on approach reveals the fundamental challenges of distributed data management and the architectural decisions that address them.</p>\n\n<h3>2. Data Partitioning Strategies</h3>\n<p>Data partitioning (sharding) distributes data across multiple nodes to improve scalability and performance. Effective partitioning strategies balance load distribution, query efficiency, and maintenance complexity.</p>\n<p>Our implementation of partitioning will cover:</p>\n<ul>\n<li><strong>Horizontal Partitioning</strong>: Row-based distribution techniques</li>\n<li><strong>Vertical Partitioning</strong>: Column-based distribution approaches</li>\n<li><strong>Partitioning Functions</strong>: Range, hash, list, and composite methods</li>\n<li><strong>Dynamic Repartitioning</strong>: Adapting to changing workloads</li>\n<li><strong>Directory Management</strong>: Locating data across partitions</li>\n</ul>\n<p>We\'ll implement multiple partitioning strategies from scratch, developing the algorithms for data distribution and location. This implementation-focused approach reveals the trade-offs between different partitioning schemes and their impact on query performance and scalability.</p>\n\n<h3>3. Replication & Consistency Models</h3>\n<p>Replication maintains multiple copies of data across different nodes to improve availability and read performance. Consistency models define the guarantees provided when multiple replicas are updated concurrently.</p>\n<p>Key aspects of replication and consistency include:</p>\n<ul>\n<li><strong>Replication Topologies</strong>: Master-slave, multi-master, peer-to-peer</li>\n<li><strong>Consistency Models</strong>: Strong, eventual, causal, and session consistency</li>\n<li><strong>Consistency Protocols</strong>: Synchronous and asynchronous approaches</li>\n<li><strong>Conflict Detection</strong>: Identifying conflicting updates</li>\n<li><strong>Conflict Resolution</strong>: Strategies for merging divergent replicas</li>\n</ul>\n<p>We\'ll implement multiple replication protocols from scratch, developing mechanisms for maintaining consistency across distributed copies. This hands-on approach highlights the fundamental trade-offs between consistency, availability, and partition tolerance described by the CAP theorem.</p>\n\n<h3>4. Distributed Consensus Algorithms</h3>\n<p>Distributed consensus algorithms enable agreement among nodes in the presence of failures, providing the foundation for reliable distributed systems. These algorithms are essential for maintaining consistency in replicated databases.</p>\n<p>Our implementation of consensus algorithms will cover:</p>\n<ul>\n<li><strong>Paxos</strong>: Classic approach to distributed consensus</li>\n<li><strong>Raft</strong>: More understandable alternative to Paxos</li>\n<li><strong>Viewstamped Replication</strong>: State machine replication approach</li>\n<li><strong>Byzantine Fault Tolerance</strong>: Consensus with malicious actors</li>\n<li><strong>Leader Election</strong>: Selecting coordinators in distributed systems</li>\n</ul>\n<p>We\'ll build these consensus protocols from first principles, implementing the message exchanges and state machines that enable reliable agreement. This implementation experience provides insight into the sophisticated algorithms that form the foundation of fault-tolerant distributed systems.</p>\n\n<h3>5. NoSQL & NewSQL Architectures</h3>\n<p>NoSQL and NewSQL systems represent modern approaches to distributed database design, offering different trade-offs in consistency, availability, partitioning, and query capabilities compared to traditional distributed relational databases.</p>\n<p>Key aspects of modern distributed databases include:</p>\n<ul>\n<li><strong>Key-Value Stores</strong>: Simple distributed hash tables</li>\n<li><strong>Document Databases</strong>: Schema-flexible JSON storage</li>\n<li><strong>Column-Family Stores</strong>: Wide-column distributed tables</li>\n<li><strong>Graph Databases</strong>: Specialized storage for connected data</li>\n<li><strong>NewSQL</strong>: Scalable databases with ACID guarantees</li>\n</ul>\n<p>We\'ll implement components of these modern architectures from scratch, developing the specialized data structures and distribution mechanisms that power contemporary distributed databases. This hands-on approach reveals how different data models and consistency requirements lead to different architectural decisions in distributed database design, providing insight into when to choose particular approaches for specific application requirements.</p>',
        parentId: 'databases'
    },

    // Cybersecurity
    {
        id: 'security',
        title: 'Cybersecurity',
        description: 'Building security systems and controls from first principles',
        content: 'Cybersecurity is the practice of protecting systems, networks, and programs from digital attacks. Our "Build from Scratch" approach means implementing security mechanisms yourself, understanding the mathematical foundations of cryptography, and learning to think like both defenders and attackers. Rather than just using existing security tools, you\'ll gain the knowledge to design secure systems from the ground up and evaluate the security of any digital system.',
        parentId: null
    },
    {
        id: 'crypto_fundamentals',
        title: 'Cryptographic Fundamentals',
        description: 'Mathematical foundations of secure communication',
        content: '<h3>1. Mathematical Foundations</h3>\n<p>Cryptography is built on mathematical principles that provide the theoretical foundation for secure communication. Understanding these principles is essential for implementing and analyzing cryptographic systems.</p>\n<p>Key mathematical foundations include:</p>\n<ul>\n<li><strong>Number Theory</strong>: Prime numbers, factorization, and modular arithmetic</li>\n<li><strong>Probability Theory</strong>: Random variables, distributions, and statistical tests</li>\n<li><strong>Information Theory</strong>: Entropy, perfect secrecy, and minimum key lengths</li>\n<li><strong>Complexity Theory</strong>: One-way functions and computational hardness</li>\n<li><strong>Abstract Algebra</strong>: Groups, fields, and algebraic structures</li>\n</ul>\n<p>We\'ll explore these mathematical concepts from first principles, implementing fundamental algorithms like primality testing, modular exponentiation, and entropy calculation. This hands-on approach reveals the mathematical structures that underpin cryptographic security.</p>\n\n<h3>2. Historical & Classical Ciphers</h3>\n<p>Classical ciphers provide historical context and introduce fundamental encryption concepts, illustrating both the evolution of cryptographic thinking and the weaknesses that modern systems address.</p>\n<p>Our implementation of classical ciphers will cover:</p>\n<ul>\n<li><strong>Substitution Ciphers</strong>: Caesar, Vigenère, and polyalphabetic systems</li>\n<li><strong>Transposition Ciphers</strong>: Rail fence, columnar, and route ciphers</li>\n<li><strong>Rotor Machines</strong>: Enigma-like mechanical encryption</li>\n<li><strong>Cryptanalysis Techniques</strong>: Frequency analysis and pattern recognition</li>\n<li><strong>One-Time Pad</strong>: Information-theoretically secure encryption</li>\n</ul>\n<p>We\'ll implement these historical systems from scratch, developing both the encryption algorithms and the corresponding cryptanalysis techniques. This implementation approach provides insight into the evolution of cryptographic thinking and the fundamental principles of encryption.</p>\n\n<h3>3. Symmetric Encryption</h3>\n<p>Symmetric encryption uses the same key for both encryption and decryption, offering efficient security for bulk data protection. Modern symmetric algorithms incorporate sophisticated mathematical operations to achieve strong security guarantees.</p>\n<p>Key symmetric encryption topics include:</p>\n<ul>\n<li><strong>Block Ciphers</strong>: DES, AES, and their internal structures</li>\n<li><strong>Block Cipher Modes</strong>: ECB, CBC, CTR, and authenticated modes</li>\n<li><strong>Stream Ciphers</strong>: RC4, ChaCha, and LFSR-based designs</li>\n<li><strong>Padding Schemes</strong>: PKCS#7 and other block completion methods</li>\n<li><strong>Key Management</strong>: Generation, exchange, and rotation</li>\n</ul>\n<p>We\'ll implement modern symmetric algorithms from first principles, developing the mathematical operations, permutations, and substitutions that form secure ciphers. This hands-on experience reveals the complex design decisions in modern encryption algorithms without relying on existing cryptographic libraries.</p>\n\n<h3>4. Cryptographic Hash Functions</h3>\n<p>Cryptographic hash functions map arbitrary data to fixed-size outputs with special security properties, serving as fundamental building blocks for numerous security applications.</p>\n<p>Our implementation of hash functions will cover:</p>\n<ul>\n<li><strong>Hash Constructions</strong>: MD-style, SHA-family, and sponge functions</li>\n<li><strong>Security Properties</strong>: Collision resistance, preimage resistance, and avalanche effects</li>\n<li><strong>Message Authentication</strong>: HMAC and MAC constructions</li>\n<li><strong>Password Hashing</strong>: Key derivation and salting techniques</li>\n<li><strong>Hash Trees</strong>: Merkle structures for data integrity</li>\n</ul>\n<p>We\'ll build cryptographic hash functions from scratch, implementing the compression functions, padding schemes, and iteration structures. This implementation-focused approach reveals the careful balance between security, performance, and mathematical properties in hash function design.</p>\n\n<h3>5. Cryptographic Protocols</h3>\n<p>Cryptographic protocols combine primitive operations into secure systems that solve specific security problems, addressing challenges like key exchange, authentication, and secure communication.</p>\n<p>Key cryptographic protocols include:</p>\n<ul>\n<li><strong>Key Exchange</strong>: Diffie-Hellman and its variants</li>\n<li><strong>Authentication</strong>: Challenge-response and zero-knowledge proofs</li>\n<li><strong>Commitment Schemes</strong>: Binding values without revealing them</li>\n<li><strong>Secret Sharing</strong>: Distributing secrets across multiple parties</li>\n<li><strong>Secure Multi-Party Computation</strong>: Computing on private inputs</li>\n</ul>\n<p>We\'ll implement these cryptographic protocols from first principles, developing the message formats, state machines, and verification steps. This hands-on approach reveals how primitive operations combine to solve complex security problems, providing insight into protocol design without relying on existing implementations that hide the underlying cryptographic machinery.</p>',
        parentId: 'security'
    },
    {
        id: 'public_key_crypto',
        title: 'Public Key Cryptography',
        description: 'Building asymmetric encryption systems',
        content: '<h3>1. Asymmetric Cryptography Foundations</h3>\n<p>Public key cryptography enables secure communication without requiring a pre-shared secret, revolutionizing secure communications. Its mathematical foundations differ fundamentally from symmetric cryptography.</p>\n<p>Key foundations of asymmetric cryptography include:</p>\n<ul>\n<li><strong>Trapdoor Functions</strong>: Easy to compute in one direction, hard to reverse without special information</li>\n<li><strong>Key Pairs</strong>: Mathematically related public and private keys</li>\n<li><strong>One-Way Functions</strong>: Problems believed to be computationally difficult</li>\n<li><strong>Hard Problems</strong>: Integer factorization, discrete logarithms, lattice problems</li>\n<li><strong>Security Models</strong>: Formal definitions for different attack scenarios</li>\n</ul>\n<p>We\'ll explore the mathematical foundations of public key systems from first principles, implementing algorithms to generate and validate key pairs. This implementation approach reveals the elegant mathematical structures that enable secure asymmetric communication.</p>\n\n<h3>2. RSA Cryptosystem</h3>\n<p>RSA is one of the oldest and most widely used public key cryptosystems, based on the difficulty of factoring large numbers. Understanding RSA provides insight into the fundamental principles of asymmetric encryption.</p>\n<p>Our implementation of RSA will cover:</p>\n<ul>\n<li><strong>Prime Generation</strong>: Finding large, secure primes</li>\n<li><strong>Key Generation</strong>: Creating public and private key pairs</li>\n<li><strong>Encryption & Decryption</strong>: Modular exponentiation operations</li>\n<li><strong>Digital Signatures</strong>: Authentication and non-repudiation</li>\n<li><strong>Padding Schemes</strong>: PKCS#1 and other secure padding methods</li>\n</ul>\n<p>We\'ll implement the RSA cryptosystem from scratch, developing the algorithms for generating keys, encrypting and decrypting messages, and creating digital signatures. This hands-on approach provides deep insight into the number theory that enables secure asymmetric communication.</p>\n\n<h3>3. Elliptic Curve Cryptography</h3>\n<p>Elliptic curve cryptography (ECC) provides the same security level as RSA with much smaller keys, making it ideal for resource-constrained environments. Its mathematical foundations involve algebraic curves over finite fields.</p>\n<p>Key aspects of ECC include:</p>\n<ul>\n<li><strong>Elliptic Curve Mathematics</strong>: Point addition, scalar multiplication</li>\n<li><strong>Curve Selection</strong>: Standards and security considerations</li>\n<li><strong>ECDH Key Exchange</strong>: Establishing shared secrets</li>\n<li><strong>ECDSA Signatures</strong>: Digital signatures using elliptic curves</li>\n<li><strong>Side-Channel Protections</strong>: Preventing physical attacks</li>\n</ul>\n<p>We\'ll implement elliptic curve cryptosystems from first principles, developing the mathematical operations and protocols that make these systems secure. This implementation-focused approach reveals the elegant algebraic structures that provide high security with efficient computation.</p>\n\n<h3>4. Digital Signatures & Authentication</h3>\n<p>Digital signatures provide authentication, integrity, and non-repudiation for electronic messages and documents. These mechanisms are essential for secure communication and transactions in distributed systems.</p>\n<p>Our implementation of digital signatures will cover:</p>\n<ul>\n<li><strong>Signature Algorithms</strong>: RSA, DSA, ECDSA, and EdDSA</li>\n<li><strong>Hash-and-Sign Paradigm</strong>: Combining hash functions with asymmetric cryptography</li>\n<li><strong>Certificate Generation</strong>: Creating X.509 certificates</li>\n<li><strong>Trust Models</strong>: Web of trust vs. hierarchical PKI</li>\n<li><strong>Signature Verification</strong>: Validating authenticity and integrity</li>\n</ul>\n<p>We\'ll build digital signature systems from scratch, implementing the algorithms for generating and verifying signatures across different cryptosystems. This hands-on experience reveals how cryptographic primitives combine to provide essential security services for modern communication systems.</p>\n\n<h3>5. Advanced Public Key Primitives</h3>\n<p>Advanced public key cryptographic primitives extend beyond basic encryption and signatures to provide specialized security properties for complex applications and protocols.</p>\n<p>Key advanced primitives include:</p>\n<ul>\n<li><strong>Post-Quantum Cryptography</strong>: Lattice, hash, code, and multivariate systems</li>\n<li><strong>Identity-Based Encryption</strong>: Using identities as public keys</li>\n<li><strong>Homomorphic Encryption</strong>: Computing on encrypted data</li>\n<li><strong>Zero-Knowledge Proofs</strong>: Proving knowledge without revealing it</li>\n<li><strong>Threshold Cryptography</strong>: Distributing cryptographic operations</li>\n</ul>\n<p>We\'ll implement selected advanced primitives from first principles, developing the mathematical operations and protocols that enable these sophisticated cryptographic capabilities. This implementation approach reveals the cutting-edge research and innovation in public key cryptography that addresses emerging security challenges and applications, from privacy-preserving computation to post-quantum security.</p>',
        parentId: 'security'
    },
    {
        id: 'secure_systems',
        title: 'Secure Systems Design',
        description: 'Building systems with security from the ground up',
        content: '<h3>1. Security Engineering Fundamentals</h3>\n<p>Security engineering establishes methodologies for designing systems with security built in from the beginning, rather than added as an afterthought. This foundational approach improves system resilience against diverse threats.</p>\n<p>Key aspects of security engineering include:</p>\n<ul>\n<li><strong>Security Requirements</strong>: Identifying and prioritizing protection needs</li>\n<li><strong>Threat Modeling</strong>: Systematically identifying potential threats</li>\n<li><strong>Risk Assessment</strong>: Evaluating likelihood and impact of threats</li>\n<li><strong>Security Architecture</strong>: Designing protective structures and boundaries</li>\n<li><strong>Security Principles</strong>: Applying fundamental security concepts</li>\n</ul>\n<p>We\'ll implement security engineering methodologies from first principles, developing tools for threat modeling and risk assessment. This hands-on approach reveals how systematic analysis guides secure architecture and implementation decisions.</p>\n\n<h3>2. Authentication Systems</h3>\n<p>Authentication systems verify the identity of users, services, and devices, providing the foundation for access control and accountability. Effective authentication requires balancing security with usability and scalability.</p>\n<p>Our implementation of authentication systems will cover:</p>\n<ul>\n<li><strong>Password Systems</strong>: Secure storage and validation</li>\n<li><strong>Multi-Factor Authentication</strong>: Something you know, have, and are</li>\n<li><strong>Challenge-Response Protocols</strong>: Interactive verification</li>\n<li><strong>Single Sign-On</strong>: Centralized authentication services</li>\n<li><strong>Biometric Authentication</strong>: Fingerprint, face, and behavior recognition</li>\n</ul>\n<p>We\'ll build authentication frameworks from scratch, implementing password hashing, token management, and multi-factor verification. This implementation-focused approach reveals the cryptographic foundations and security considerations in identity verification systems.</p>\n\n<h3>3. Access Control Mechanisms</h3>\n<p>Access control mechanisms enforce security policies by limiting actions and access to resources based on identity and privileges. These systems translate authentication into authorization, ensuring appropriate access boundaries.</p>\n<p>Key access control mechanisms include:</p>\n<ul>\n<li><strong>Access Control Models</strong>: DAC, MAC, RBAC, and ABAC</li>\n<li><strong>Permission Systems</strong>: Designing flexible capability structures</li>\n<li><strong>Authorization Frameworks</strong>: Policy definition and enforcement</li>\n<li><strong>Privilege Management</strong>: Granting and revoking access rights</li>\n<li><strong>Privilege Escalation Defense</strong>: Preventing unauthorized elevation</li>\n</ul>\n<p>We\'ll implement multiple access control systems from scratch, developing the data structures, enforcement points, and administrative interfaces. This hands-on experience reveals how theoretical security models translate to practical implementations that protect system resources.</p>\n\n<h3>4. Secure Communication</h3>\n<p>Secure communication protects data during transmission between systems, addressing threats like eavesdropping, tampering, and impersonation. These mechanisms are essential for distributed systems security.</p>\n<p>Our implementation of secure communication will cover:</p>\n<ul>\n<li><strong>Secure Channels</strong>: Building encrypted tunnels</li>\n<li><strong>Transport Layer Security</strong>: Implementing TLS-like protocols</li>\n<li><strong>Certificate Management</strong>: Validation and revocation</li>\n<li><strong>Secure RPC</strong>: Authentication and encryption for remote calls</li>\n<li><strong>Message Security</strong>: End-to-end protection for communications</li>\n</ul>\n<p>We\'ll build secure communication protocols from scratch, implementing the handshakes, key exchanges, and encrypted data transfers. This implementation approach reveals the complex interactions between cryptographic primitives that create secure channels in untrusted networks.</p>\n\n<h3>5. Secure Software Development</h3>\n<p>Secure software development integrates security practices throughout the development lifecycle, producing code that resists attacks and protects data. These practices address vulnerabilities at their source—during development.</p>\n<p>Key aspects of secure development include:</p>\n<ul>\n<li><strong>Secure Coding Practices</strong>: Preventing common vulnerabilities</li>\n<li><strong>Input Validation</strong>: Defending against injection attacks</li>\n<li><strong>Security Testing</strong>: Finding vulnerabilities before deployment</li>\n<li><strong>Memory Safety</strong>: Preventing buffer overflows and use-after-free</li>\n<li><strong>Secure Design Patterns</strong>: Reusable security solutions</li>\n</ul>\n<p>We\'ll implement secure development practices from first principles, building libraries and frameworks that enforce security properties. This hands-on approach reveals how careful implementation prevents common vulnerabilities and creates software resilient against attacks, without relying on existing security frameworks that hide the underlying security considerations.</p>',
        parentId: 'security'
    },
    {
        id: 'security_analysis',
        title: 'Security Analysis & Penetration Testing',
        description: 'Finding and exploiting vulnerabilities',
        content: '<h3>1. Vulnerability Theory & Classification</h3>\n<p>Vulnerability theory provides a framework for understanding, categorizing, and addressing security weaknesses in systems. This foundation enables systematic analysis and mitigation of security risks.</p>\n<p>Key aspects of vulnerability theory include:</p>\n<ul>\n<li><strong>Vulnerability Taxonomies</strong>: Categorizing weakness types</li>\n<li><strong>Attack Surfaces</strong>: Identifying potential entry points</li>\n<li><strong>Attack Vectors</strong>: Methods for exploiting vulnerabilities</li>\n<li><strong>Vulnerability Lifecycle</strong>: From discovery to remediation</li>\n<li><strong>Exploit Development</strong>: Understanding attack mechanics</li>\n</ul>\n<p>We\'ll explore vulnerability theory from first principles, implementing classification systems and vulnerability databases. This hands-on approach reveals the patterns that unite seemingly different security weaknesses, providing insight into systematic discovery and mitigation approaches.</p>\n\n<h3>2. Static Analysis Techniques</h3>\n<p>Static analysis examines code without executing it, identifying potential security vulnerabilities through pattern matching, data flow analysis, and symbolic execution. These techniques find issues early in the development process.</p>\n<p>Our implementation of static analysis will cover:</p>\n<ul>\n<li><strong>Lexical Analysis</strong>: Tokenizing and parsing code</li>\n<li><strong>Pattern Matching</strong>: Finding vulnerable code patterns</li>\n<li><strong>Data Flow Analysis</strong>: Tracking information through programs</li>\n<li><strong>Taint Analysis</strong>: Following untrusted data</li>\n<li><strong>Symbolic Execution</strong>: Analyzing all possible execution paths</li>\n</ul>\n<p>We\'ll build static analyzers from scratch for various languages, implementing the algorithms that find potential vulnerabilities without execution. This implementation-focused approach reveals how automated tools can discover security issues that manual inspection might miss.</p>\n\n<h3>3. Dynamic Analysis & Fuzzing</h3>\n<p>Dynamic analysis examines running systems, identifying vulnerabilities through runtime testing, observation, and manipulation. Fuzzing automatically generates malformed inputs to trigger unexpected behavior and security vulnerabilities.</p>\n<p>Key dynamic analysis techniques include:</p>\n<ul>\n<li><strong>Fuzzing Strategies</strong>: Mutation, generation, and grammar-based</li>\n<li><strong>Instrumentation</strong>: Monitoring program execution</li>\n<li><strong>Runtime Vulnerability Detection</strong>: Finding issues during execution</li>\n<li><strong>Feedback-Driven Testing</strong>: Using execution data to guide testing</li>\n<li><strong>Memory Corruption Detection</strong>: Finding memory safety issues</li>\n</ul>\n<p>We\'ll implement various dynamic analysis tools from scratch, including fuzzing engines and runtime monitors. This hands-on experience reveals how observing program behavior under unusual conditions can uncover security vulnerabilities not visible through static analysis.</p>\n\n<h3>4. Web Application Security</h3>\n<p>Web application security addresses the unique vulnerabilities and attack vectors of web-based systems, from client-side issues to server-side weaknesses. These techniques are essential for securing modern internet applications.</p>\n<p>Our exploration of web security will cover:</p>\n<ul>\n<li><strong>Injection Attacks</strong>: SQL, command, and template injection</li>\n<li><strong>Cross-Site Scripting (XSS)</strong>: Reflected, stored, and DOM-based</li>\n<li><strong>Cross-Site Request Forgery</strong>: Exploiting user authentication</li>\n<li><strong>Authentication Weaknesses</strong>: Session management flaws</li>\n<li><strong>Client-Side Vulnerabilities</strong>: Browser security models</li>\n</ul>\n<p>We\'ll build web security testing tools from scratch, implementing scanners and exploit frameworks for web applications. This implementation approach reveals the complex interaction between web technologies that creates unique security challenges not present in traditional applications.</p>\n\n<h3>5. Penetration Testing Methodology</h3>\n<p>Penetration testing methodology provides a structured approach to evaluating system security through simulated attacks. These methodologies ensure comprehensive testing and clear communication of security findings.</p>\n<p>Key aspects of penetration testing include:</p>\n<ul>\n<li><strong>Reconnaissance Techniques</strong>: Gathering system information</li>\n<li><strong>Vulnerability Scanning</strong>: Automated discovery of weaknesses</li>\n<li><strong>Exploitation</strong>: Leveraging vulnerabilities to gain access</li>\n<li><strong>Post-Exploitation</strong>: Pivoting and privilege escalation</li>\n<li><strong>Reporting</strong>: Documenting findings and recommendations</li>\n</ul>\n<p>We\'ll implement penetration testing frameworks from first principles, developing tools for each phase of a security assessment. This hands-on approach reveals how attackers chain together multiple techniques to compromise systems, providing security professionals with insight into defensive priorities and strategies.</p>',
        parentId: 'security'
    },
    {
        id: 'network_security',
        title: 'Network Security',
        description: 'Protecting communications and network infrastructure',
        content: '<h3>1. Network Threats & Attack Vectors</h3>\n<p>Network threats exploit vulnerabilities in communication protocols, infrastructure components, and network architectures. Understanding these threats provides the foundation for building effective defenses.</p>\n<p>Key network threats and attacks include:</p>\n<ul>\n<li><strong>Packet Sniffing</strong>: Passive interception of network traffic</li>\n<li><strong>Man-in-the-Middle Attacks</strong>: Intercepting and potentially modifying communications</li>\n<li><strong>Denial of Service</strong>: Overwhelming networks with traffic</li>\n<li><strong>ARP Poisoning</strong>: Redirecting traffic through address spoofing</li>\n<li><strong>DNS Attacks</strong>: Hijacking or poisoning domain resolution</li>\n</ul>\n<p>We\'ll analyze these attacks from first principles, implementing tools to demonstrate and detect network-based threats. This hands-on approach reveals the fundamental vulnerabilities in network protocols and architectures that attackers exploit.</p>\n\n<h3>2. Packet Filtering & Firewalls</h3>\n<p>Packet filtering and firewalls control network traffic based on rules about source, destination, protocol, and content. These defensive mechanisms form the first line of protection for network boundaries.</p>\n<p>Our implementation of filtering systems will cover:</p>\n<ul>\n<li><strong>Stateless Packet Filters</strong>: Rule-based traffic decisions</li>\n<li><strong>Stateful Inspection</strong>: Tracking connection context</li>\n<li><strong>Application Layer Filtering</strong>: Content-based decisions</li>\n<li><strong>Next-Generation Firewalls</strong>: Deep packet inspection</li>\n<li><strong>Network Address Translation</strong>: Hiding internal addresses</li>\n</ul>\n<p>We\'ll build packet filtering systems from scratch, implementing the algorithms for inspecting and controlling network traffic. This implementation-focused approach reveals how firewalls balance security with performance and usability to protect network boundaries.</p>\n\n<h3>3. Intrusion Detection & Prevention</h3>\n<p>Intrusion detection and prevention systems monitor networks for suspicious activity, providing alerts or actively blocking threats. These systems form a critical layer of defense for identifying attacks that bypass perimeter security.</p>\n<p>Key aspects of intrusion detection include:</p>\n<ul>\n<li><strong>Signature-Based Detection</strong>: Matching known attack patterns</li>\n<li><strong>Anomaly Detection</strong>: Identifying unusual network behavior</li>\n<li><strong>Network Traffic Analysis</strong>: Protocol and behavior monitoring</li>\n<li><strong>Alert Correlation</strong>: Connecting related security events</li>\n<li><strong>Response Automation</strong>: Reacting to detected threats</li>\n</ul>\n<p>We\'ll implement intrusion detection systems from first principles, developing algorithms for traffic analysis and threat recognition. This hands-on experience reveals the challenges of distinguishing legitimate activity from attacks and the techniques used to achieve high detection rates with low false positives.</p>\n\n<h3>4. Virtual Private Networks</h3>\n<p>Virtual Private Networks (VPNs) create secure encrypted tunnels over untrusted networks, enabling private communication across public infrastructure. These systems extend private network boundaries across the internet.</p>\n<p>Our implementation of VPN technologies will cover:</p>\n<ul>\n<li><strong>Tunneling Protocols</strong>: Encapsulating traffic</li>\n<li><strong>Key Exchange</strong>: Establishing secure session keys</li>\n<li><strong>Authentication Mechanisms</strong>: Verifying endpoints</li>\n<li><strong>Traffic Encryption</strong>: Protecting data in transit</li>\n<li><strong>Split Tunneling</strong>: Selective traffic routing</li>\n</ul>\n<p>We\'ll build VPN components from scratch, implementing tunneling, encryption, and authentication mechanisms. This implementation approach reveals how cryptographic primitives combine with networking protocols to create secure communication channels across untrusted networks.</p>\n\n<h3>5. Secure Network Protocols</h3>\n<p>Secure network protocols add cryptographic protection to communication, addressing the inherent lack of security in traditional internet protocols. These protocols form the foundation for secure modern communications.</p>\n<p>Key secure protocols include:</p>\n<ul>\n<li><strong>Transport Layer Security</strong>: Securing application communications</li>\n<li><strong>IPsec</strong>: Network layer protection</li>\n<li><strong>Secure Shell (SSH)</strong>: Protected remote access</li>\n<li><strong>Secure Email Protocols</strong>: S/MIME and PGP</li>\n<li><strong>Secure DNS</strong>: DNSSEC and DNS over HTTPS</li>\n</ul>\n<p>We\'ll implement components of these secure protocols from scratch, developing the handshakes, encryption, and authentication mechanisms that protect different network layers. This hands-on approach reveals how protocol design addresses security requirements at different layers of the network stack, providing comprehensive protection for modern communications.</p>',
        parentId: 'security'
    },
    
    // AI & Machine Learning
    {
        id: 'ai_ml',
        title: 'AI & Machine Learning',
        description: 'Building intelligent systems from mathematical foundations',
        content: 'Artificial Intelligence enables machines to simulate human intelligence, while Machine Learning allows systems to learn from data without explicit programming. Our "Build from Scratch" approach means implementing AI algorithms and ML models yourself, starting from their mathematical foundations. Rather than using existing frameworks and libraries, you\'ll gain the knowledge to design, train, and evaluate intelligent systems from first principles.',
        parentId: null
    },
    {
        id: 'ml_fundamentals',
        title: 'Machine Learning Fundamentals',
        description: 'Mathematical foundations of learning from data',
        content: '<h3>1. Mathematical Foundations for ML</h3>\n<p>Machine learning rests on mathematical foundations that provide the tools for designing, analyzing, and implementing learning algorithms. These mathematical principles are essential for understanding how ML algorithms work and why they succeed or fail.</p>\n<p>Key mathematical foundations include:</p>\n<ul>\n<li><strong>Linear Algebra</strong>: Vectors, matrices, eigendecomposition</li>\n<li><strong>Probability Theory</strong>: Random variables, distributions, Bayes\' theorem</li>\n<li><strong>Calculus</strong>: Gradients, partial derivatives, chain rule</li>\n<li><strong>Optimization Theory</strong>: Convexity, gradient descent, constrained optimization</li>\n<li><strong>Information Theory</strong>: Entropy, mutual information, KL divergence</li>\n</ul>\n<p>We\'ll implement these mathematical operations from scratch, building libraries for vector/matrix operations, probability calculations, and optimization algorithms. This hands-on approach reveals the mathematical structures that enable machines to learn from data.</p>\n\n<h3>2. Supervised Learning</h3>\n<p>Supervised learning algorithms learn mappings from inputs to outputs based on labeled examples. These algorithms form the foundation for many practical ML applications, from spam filtering to medical diagnosis.</p>\n<p>Our implementation of supervised learning will cover:</p>\n<ul>\n<li><strong>Linear Regression</strong>: Least squares and regularization</li>\n<li><strong>Logistic Regression</strong>: Binary and multinomial classification</li>\n<li><strong>Decision Trees</strong>: Recursive partitioning algorithms</li>\n<li><strong>Support Vector Machines</strong>: Maximal margin classifiers</li>\n<li><strong>Ensemble Methods</strong>: Bagging, boosting, and random forests</li>\n</ul>\n<p>We\'ll build these algorithms from first principles, implementing learning rules, optimization procedures, and prediction functions. This implementation-focused approach reveals how relatively simple mathematical formulations can extract powerful predictive patterns from data.</p>\n\n<h3>3. Unsupervised Learning</h3>\n<p>Unsupervised learning algorithms discover structure in unlabeled data, revealing patterns, groupings, and representations without external guidance. These methods provide insights into data structure and generate useful features for downstream tasks.</p>\n<p>Key unsupervised learning methods include:</p>\n<ul>\n<li><strong>Clustering Algorithms</strong>: K-means, hierarchical, DBSCAN</li>\n<li><strong>Dimensionality Reduction</strong>: PCA, t-SNE, UMAP</li>\n<li><strong>Density Estimation</strong>: Kernel density, mixture models</li>\n<li><strong>Association Rule Learning</strong>: Apriori and FP-growth</li>\n<li><strong>Autoencoders</strong>: Representational learning</li>\n</ul>\n<p>We\'ll implement these unsupervised methods from scratch, developing algorithms to find patterns without labeled examples. This hands-on experience reveals how machines can discover structure independently, forming the basis for knowledge discovery and representation learning.</p>\n\n<h3>4. Model Evaluation & Validation</h3>\n<p>Model evaluation and validation techniques assess learning algorithm performance, enabling comparison between models and providing estimates of real-world effectiveness. These methods are critical for building reliable machine learning systems.</p>\n<p>Our implementation of evaluation methods will cover:</p>\n<ul>\n<li><strong>Performance Metrics</strong>: Accuracy, precision, recall, F1, ROC</li>\n<li><strong>Cross-Validation</strong>: K-fold, leave-one-out, stratified</li>\n<li><strong>Bias-Variance Analysis</strong>: Understanding error decomposition</li>\n<li><strong>Learning Curves</strong>: Diagnosing underfitting and overfitting</li>\n<li><strong>Statistical Significance</strong>: Hypothesis testing for model comparison</li>\n</ul>\n<p>We\'ll build evaluation frameworks from first principles, implementing metrics, validation schemes, and diagnostic tools. This implementation approach reveals the statistical foundations of model assessment and the techniques for estimating generalization performance.</p>\n\n<h3>5. Feature Engineering & Selection</h3>\n<p>Feature engineering and selection transform raw data into representations that improve learning algorithm performance. These techniques bridge the gap between data collection and model training, often determining success or failure.</p>\n<p>Key feature engineering methods include:</p>\n<ul>\n<li><strong>Feature Extraction</strong>: Creating derived features</li>\n<li><strong>Feature Transformation</strong>: Scaling, normalization, encoding</li>\n<li><strong>Feature Selection</strong>: Filter, wrapper, and embedded methods</li>\n<li><strong>Regularization</strong>: Controlling model complexity via features</li>\n<li><strong>Automated Feature Learning</strong>: Representation extraction</li>\n</ul>\n<p>We\'ll implement feature engineering techniques from scratch, developing transformations, selection algorithms, and evaluation methods. This hands-on approach reveals how data representation fundamentally impacts learning performance and how domain knowledge can be encoded into features that enable effective machine learning.</p>',
        parentId: 'ai_ml'
    },
    {
        id: 'neural_networks',
        title: 'Neural Networks From Scratch',
        description: 'Building brain-inspired learning systems',
        content: '<h3>1. Neuron Models & Network Architecture</h3>\n<p>Neural network architecture defines the structure and organization of artificial neurons into interconnected layers. This foundational design determines the network\'s representational capacity and learning abilities.</p>\n<p>Key aspects of neural architecture include:</p>\n<ul>\n<li><strong>Neuron Models</strong>: Activation functions and inputs summation</li>\n<li><strong>Layer Types</strong>: Input, hidden, and output layers</li>\n<li><strong>Connection Patterns</strong>: Fully connected, sparse, skip connections</li>\n<li><strong>Weight Initialization</strong>: Strategies for starting parameters</li>\n<li><strong>Computational Graphs</strong>: Representing network operations</li>\n</ul>\n<p>We\'ll implement neural network components from first principles, building neurons and connecting them into networks. This hands-on approach reveals how simple computational units combine to form powerful function approximators.</p>\n\n<h3>2. Forward Propagation</h3>\n<p>Forward propagation computes network outputs from inputs, passing data through layers of transformations. This process applies the current network parameters to make predictions on new data.</p>\n<p>Our implementation of forward propagation will cover:</p>\n<ul>\n<li><strong>Vector-Matrix Operations</strong>: Efficient computation of layer outputs</li>\n<li><strong>Activation Functions</strong>: Sigmoid, tanh, ReLU, and their variants</li>\n<li><strong>Layer Connectivity</strong>: Information flow through the network</li>\n<li><strong>Computational Complexity</strong>: Analyzing operation counts</li>\n<li><strong>Numerical Stability</strong>: Preventing overflow and underflow</li>\n</ul>\n<p>We\'ll build forward propagation algorithms from scratch, implementing the mathematical operations that transform inputs to outputs. This implementation-focused approach reveals how networks compute predictions and the critical role of activation functions in introducing non-linearity.</p>\n\n<h3>3. Backpropagation & Gradient Descent</h3>\n<p>Backpropagation calculates gradients of the loss function with respect to network weights, enabling gradient-based optimization. This algorithm efficiently computes how each parameter influences network performance.</p>\n<p>Key aspects of backpropagation include:</p>\n<ul>\n<li><strong>Chain Rule Application</strong>: Computing parameter gradients</li>\n<li><strong>Loss Functions</strong>: Mean squared error, cross-entropy, hinge loss</li>\n<li><strong>Gradient Descent Variants</strong>: SGD, momentum, RMSprop, Adam</li>\n<li><strong>Learning Rate Schedules</strong>: Dynamic parameter adjustment</li>\n<li><strong>Batch Processing</strong>: Mini-batch gradient estimation</li>\n</ul>\n<p>We\'ll implement backpropagation and gradient descent from first principles, developing the algorithms for computing gradients and updating weights. This hands-on experience reveals the elegant application of calculus that enables neural networks to learn from data.</p>\n\n<h3>4. Regularization & Optimization</h3>\n<p>Regularization techniques prevent overfitting by constraining model complexity, while optimization strategies improve training efficiency and final performance. These methods are essential for developing neural networks that generalize well.</p>\n<p>Our implementation of regularization will cover:</p>\n<ul>\n<li><strong>Weight Penalties</strong>: L1 and L2 regularization</li>\n<li><strong>Dropout</strong>: Random neuron deactivation</li>\n<li><strong>Early Stopping</strong>: Validation-based training termination</li>\n<li><strong>Batch Normalization</strong>: Normalizing layer inputs</li>\n<li><strong>Hyperparameter Tuning</strong>: Grid and random search</li>\n</ul>\n<p>We\'ll build these regularization and optimization techniques from scratch, implementing the mechanisms that improve generalization and training dynamics. This implementation approach reveals how seemingly simple modifications significantly impact neural network learning and performance.</p>\n\n<h3>5. Neural Network Applications</h3>\n<p>Neural network applications demonstrate how these models solve real-world problems across various domains. Implementing applications reveals the practical considerations beyond theoretical understanding.</p>\n<p>Key neural network applications include:</p>\n<ul>\n<li><strong>Classification Tasks</strong>: Image, text, and categorical data</li>\n<li><strong>Regression Problems</strong>: Continuous variable prediction</li>\n<li><strong>Sequence Learning</strong>: Time series and natural language</li>\n<li><strong>Function Approximation</strong>: Modeling complex relationships</li>\n<li><strong>Feature Learning</strong>: Representation extraction</li>\n</ul>\n<p>We\'ll implement neural networks for diverse applications from scratch, developing end-to-end systems that solve specific problems. This hands-on approach reveals how theoretical components combine into practical systems and how neural networks can be adapted to different types of data and tasks.</p>',
        parentId: 'ai_ml'
    },
    {
        id: 'deep_learning',
        title: 'Deep Learning Architectures',
        description: 'Advanced neural network structures and algorithms',
        content: '<h3>1. Convolutional Neural Networks</h3>\n<p>Convolutional Neural Networks (CNNs) excel at processing grid-like data such as images, leveraging local connectivity patterns and parameter sharing. These specialized architectures have revolutionized computer vision and related fields.</p>\n<p>Key components of CNNs include:</p>\n<ul>\n<li><strong>Convolutional Layers</strong>: Local feature extraction with filters</li>\n<li><strong>Pooling Operations</strong>: Downsampling for spatial reduction</li>\n<li><strong>Feature Hierarchies</strong>: Learning from simple to complex patterns</li>\n<li><strong>Transfer Learning</strong>: Reusing pre-trained representations</li>\n<li><strong>Visualization Techniques</strong>: Understanding network activations</li>\n</ul>\n<p>We\'ll implement convolutional neural networks from first principles, building the components that enable spatial pattern recognition. This hands-on approach reveals how parameter sharing and local connectivity create efficient and powerful vision models.</p>\n\n<h3>2. Recurrent Neural Networks</h3>\n<p>Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture historical information. These architectures excel at tasks involving time series, text, and other sequential patterns.</p>\n<p>Our implementation of RNNs will cover:</p>\n<ul>\n<li><strong>Basic RNN Cells</strong>: Capturing short-term dependencies</li>\n<li><strong>LSTM & GRU Architectures</strong>: Managing long-term dependencies</li>\n<li><strong>Bidirectional Networks</strong>: Processing sequences in both directions</li>\n<li><strong>Sequence-to-Sequence Models</strong>: Translation and transcription</li>\n<li><strong>Attention Mechanisms</strong>: Focusing on relevant information</li>\n</ul>\n<p>We\'ll build recurrent architectures from scratch, implementing the gating mechanisms and information flow that enable sequence learning. This implementation-focused approach reveals how networks maintain and use historical information for sequential prediction tasks.</p>\n\n<h3>3. Transformer Architectures</h3>\n<p>Transformer architectures use self-attention mechanisms to process sequential data without recurrence, enabling parallel processing and capturing long-range dependencies. These models have become the foundation of modern natural language processing.</p>\n<p>Key aspects of transformers include:</p>\n<ul>\n<li><strong>Self-Attention Mechanisms</strong>: Relating positions within sequences</li>\n<li><strong>Multi-Head Attention</strong>: Capturing different relationship types</li>\n<li><strong>Positional Encodings</strong>: Preserving sequence order information</li>\n<li><strong>Encoder-Decoder Structures</strong>: Processing and generating sequences</li>\n<li><strong>Pre-training and Fine-tuning</strong>: Transfer learning for language</li>\n</ul>\n<p>We\'ll implement transformer components from first principles, building attention mechanisms and network structures. This hands-on experience reveals how self-attention enables powerful language understanding and generation capabilities.</p>\n\n<h3>4. Generative Models</h3>\n<p>Generative models learn to create new data samples that resemble their training distribution. These architectures enable creative applications like image synthesis, text generation, and data augmentation.</p>\n<p>Our implementation of generative models will cover:</p>\n<ul>\n<li><strong>Variational Autoencoders (VAEs)</strong>: Probabilistic encodings</li>\n<li><strong>Generative Adversarial Networks (GANs)</strong>: Generator-discriminator dynamics</li>\n<li><strong>Diffusion Models</strong>: Gradual noise addition and removal</li>\n<li><strong>Flow-Based Models</strong>: Invertible transformations</li>\n<li><strong>Autoregressive Models</strong>: Sequential generation of complex data</li>\n</ul>\n<p>We\'ll build these generative architectures from scratch, implementing the training dynamics and sampling procedures. This implementation approach reveals how networks can learn to create new, realistic data rather than just recognize patterns in existing data.</p>\n\n<h3>5. Deep Reinforcement Learning</h3>\n<p>Deep Reinforcement Learning combines neural networks with reinforcement learning to solve complex decision-making problems. These architectures learn through interaction with environments, maximizing cumulative rewards.</p>\n<p>Key deep reinforcement learning methods include:</p>\n<ul>\n<li><strong>Deep Q-Networks (DQN)</strong>: Value function approximation</li>\n<li><strong>Policy Gradient Methods</strong>: Direct policy optimization</li>\n<li><strong>Actor-Critic Architectures</strong>: Combining value and policy learning</li>\n<li><strong>Model-Based RL</strong>: Learning environment dynamics</li>\n<li><strong>Multi-Agent RL</strong>: Coordinating multiple learning agents</li>\n</ul>\n<p>We\'ll implement deep reinforcement learning algorithms from first principles, building the neural networks that enable sophisticated decision-making. This hands-on approach reveals how deep learning can be applied to sequential decision problems, creating systems that learn to play games, control robots, or optimize complex processes through trial and error.</p>',
        parentId: 'ai_ml'
    },
    {
        id: 'reinforcement_learning',
        title: 'Reinforcement Learning',
        description: 'Building systems that learn through interaction',
        content: '<h3>1. Reinforcement Learning Fundamentals</h3>\n<p>Reinforcement learning operates at the intersection of optimization, dynamic programming, and sequential decision-making. Understanding these foundational elements is essential for developing effective RL systems.</p>\n<p>Key reinforcement learning fundamentals include:</p>\n<ul>\n<li><strong>Markov Decision Processes</strong>: Mathematical framework for sequential decisions</li>\n<li><strong>Rewards and Returns</strong>: Immediate vs. cumulative feedback</li>\n<li><strong>State-Value Functions</strong>: Expected returns from states</li>\n<li><strong>Action-Value Functions</strong>: Expected returns from state-action pairs</li>\n<li><strong>Policies</strong>: Strategies for selecting actions in states</li>\n</ul>\n<p>We\'ll implement these foundational components from first principles, developing the mathematical structures that enable learning through interaction. This hands-on approach reveals the theoretical framework that unifies diverse reinforcement learning algorithms.</p>\n\n<h3>2. Dynamic Programming Methods</h3>\n<p>Dynamic programming methods solve reinforcement learning problems when the environment model is completely known. These approaches provide the theoretical foundation for more complex algorithms that operate without perfect models.</p>\n<p>Our implementation of dynamic programming will cover:</p>\n<ul>\n<li><strong>Policy Evaluation</strong>: Computing value functions for fixed policies</li>\n<li><strong>Policy Improvement</strong>: Enhancing policies based on evaluations</li>\n<li><strong>Policy Iteration</strong>: Alternating evaluation and improvement</li>\n<li><strong>Value Iteration</strong>: Directly computing optimal values</li>\n<li><strong>Asynchronous Methods</strong>: Incrementally updating solutions</li>\n</ul>\n<p>We\'ll build these algorithms from scratch, implementing the iterative procedures that compute optimal policies. This implementation-focused approach reveals how dynamic programming enables exact solutions for reinforcement learning when complete environment models are available.</p>\n\n<h3>3. Monte Carlo & Temporal-Difference Methods</h3>\n<p>Monte Carlo and temporal-difference methods learn from experience without requiring complete environment models. These sample-based approaches enable reinforcement learning in complex, unknown environments.</p>\n<p>Key model-free learning methods include:</p>\n<ul>\n<li><strong>Monte Carlo Estimation</strong>: Learning from complete episodes</li>\n<li><strong>Temporal-Difference Learning</strong>: Bootstrapping from partial experiences</li>\n<li><strong>SARSA Algorithm</strong>: On-policy TD control</li>\n<li><strong>Q-Learning</strong>: Off-policy TD control</li>\n<li><strong>Eligibility Traces</strong>: Combining MC and TD approaches</li>\n</ul>\n<p>We\'ll implement these model-free algorithms from scratch, developing solutions that learn directly from interaction. This hands-on experience reveals how agents can discover optimal behaviors through trial and error, without prior knowledge of the environment dynamics.</p>\n\n<h3>4. Policy Gradient Methods</h3>\n<p>Policy gradient methods directly optimize policy parameters using gradient ascent on policy performance. These approaches enable reinforcement learning in continuous action spaces and with stochastic policies.</p>\n<p>Our implementation of policy gradients will cover:</p>\n<ul>\n<li><strong>REINFORCE Algorithm</strong>: Monte Carlo policy gradients</li>\n<li><strong>Actor-Critic Methods</strong>: Combining policy and value learning</li>\n<li><strong>Natural Policy Gradients</strong>: Fisher information and optimization</li>\n<li><strong>Trust Region Methods</strong>: Constrained policy optimization</li>\n<li><strong>Proximal Policy Optimization</strong>: Stable policy improvement</li>\n</ul>\n<p>We\'ll build these policy-based algorithms from first principles, implementing the gradient estimation and update procedures. This implementation approach reveals how reinforcement learning can directly optimize policies for complex tasks with large or continuous action spaces.</p>\n\n<h3>5. Exploration and Advanced Topics</h3>\n<p>Exploration strategies and advanced topics address specific challenges in reinforcement learning, from balancing exploration with exploitation to learning hierarchical behaviors and transferring knowledge between tasks.</p>\n<p>Key advanced topics include:</p>\n<ul>\n<li><strong>Exploration Strategies</strong>: ε-greedy, softmax, intrinsic motivation</li>\n<li><strong>Hierarchical RL</strong>: Learning at multiple temporal scales</li>\n<li><strong>Multi-Agent RL</strong>: Coordination and competition</li>\n<li><strong>Transfer Learning</strong>: Leveraging knowledge across tasks</li>\n<li><strong>Meta-Learning</strong>: Learning to learn reinforcement tasks</li>\n</ul>\n<p>We\'ll implement these advanced techniques from scratch, developing solutions to complex reinforcement learning challenges. This hands-on approach reveals the frontiers of reinforcement learning research and practice, providing insight into how agents can tackle increasingly sophisticated decision-making problems in diverse domains.</p>',
        parentId: 'ai_ml'
    },
    {
        id: 'ml_systems',
        title: 'Machine Learning Systems',
        description: 'Building complete ML pipelines and applications',
        content: '<h3>1. Data Infrastructure & Pipelines</h3>\n<p>Data infrastructure and pipelines form the foundation of machine learning systems, enabling efficient collection, storage, processing, and delivery of data. These components determine the scale, quality, and reliability of data for training and inference.</p>\n<p>Key data infrastructure components include:</p>\n<ul>\n<li><strong>Data Collection Systems</strong>: Sources, APIs, and instrumentation</li>\n<li><strong>Storage Solutions</strong>: Databases, data lakes, and file formats</li>\n<li><strong>ETL Processes</strong>: Extraction, transformation, loading</li>\n<li><strong>Data Validation</strong>: Schema enforcement and quality checks</li>\n<li><strong>Versioning & Lineage</strong>: Tracking data provenance</li>\n</ul>\n<p>We\'ll implement data infrastructure from first principles, building pipelines that handle the entire data lifecycle. This hands-on approach reveals how robust data systems form the foundation for successful machine learning applications.</p>\n\n<h3>2. Feature Engineering & Preprocessing</h3>\n<p>Feature engineering and preprocessing transform raw data into representations suitable for machine learning models. These processes significantly impact model performance, often determining success or failure more than model architecture.</p>\n<p>Our implementation of feature engineering will cover:</p>\n<ul>\n<li><strong>Feature Extraction</strong>: Deriving informative attributes</li>\n<li><strong>Transformation Pipelines</strong>: Coordinated data processing</li>\n<li><strong>Handling Missing Data</strong>: Imputation and indicator features</li>\n<li><strong>Categorical Encoding</strong>: One-hot, embedding, target encoding</li>\n<li><strong>Feature Selection</strong>: Identifying relevant attributes</li>\n</ul>\n<p>We\'ll build feature engineering pipelines from scratch, implementing transformations for various data types. This implementation-focused approach reveals how domain knowledge combines with data analysis to create representations that enable effective learning.</p>\n\n<h3>3. Model Lifecycle Management</h3>\n<p>Model lifecycle management encompasses the processes for developing, evaluating, deploying, and maintaining machine learning models. These practices enable systematic improvement and reliable operation of ML systems.</p>\n<p>Key aspects of model lifecycle management include:</p>\n<ul>\n<li><strong>Experiment Tracking</strong>: Recording training runs and parameters</li>\n<li><strong>Model Versioning</strong>: Managing model iterations</li>\n<li><strong>Evaluation Frameworks</strong>: Comprehensive performance assessment</li>\n<li><strong>A/B Testing</strong>: Comparing model versions in production</li>\n<li><strong>Model Registry</strong>: Cataloging and deploying models</li>\n</ul>\n<p>We\'ll implement model lifecycle tools from first principles, developing systems for tracking, evaluating, and deploying models. This hands-on experience reveals how systematic management practices transform machine learning from research to reliable production systems.</p>\n\n<h3>4. ML System Architecture</h3>\n<p>ML system architecture defines the structure and organization of machine learning applications, addressing requirements for scalability, reliability, latency, and maintainability. These architectural decisions determine how models integrate with larger software systems.</p>\n<p>Our exploration of ML architecture will cover:</p>\n<ul>\n<li><strong>Inference Servers</strong>: Model serving and API design</li>\n<li><strong>Batch vs. Real-time Processing</strong>: Deployment patterns</li>\n<li><strong>Monitoring & Alerting</strong>: Tracking system health</li>\n<li><strong>Scaling Strategies</strong>: Handling varying load</li>\n<li><strong>Microservice Integration</strong>: Composing ML components</li>\n</ul>\n<p>We\'ll build machine learning architectures from scratch, implementing the components that enable models to operate reliably in production. This implementation approach reveals how ML integrates with broader software engineering practices to create complete applications.</p>\n\n<h3>5. Responsible AI & ML Ethics</h3>\n<p>Responsible AI practices address the ethical, social, and governance implications of machine learning systems. These considerations ensure ML applications operate fairly, transparently, and in alignment with human values.</p>\n<p>Key aspects of responsible AI include:</p>\n<ul>\n<li><strong>Fairness Assessment</strong>: Measuring and mitigating bias</li>\n<li><strong>Model Explainability</strong>: Interpreting predictions</li>\n<li><strong>Privacy Protection</strong>: Differential privacy and secure ML</li>\n<li><strong>Robustness Evaluation</strong>: Testing against adversarial attacks</li>\n<li><strong>Impact Assessment</strong>: Evaluating ethical implications</li>\n</ul>\n<p>We\'ll implement responsible AI tools from first principles, developing metrics and methods for building ethical ML systems. This hands-on approach reveals how ethical considerations can be systematically integrated into the machine learning lifecycle, ensuring applications benefit users and society while minimizing potential harms.</p>',
        parentId: 'ai_ml'
    },
    
    // Theory of Computation
    {
        id: 'theory',
        title: 'Theory of Computation',
        description: 'Mathematical foundation of computation and algorithms',
        content: 'Theory of Computation is the mathematical study of the capabilities and limitations of computing systems. Our "Build from Scratch" approach means learning the formal models and mathematical proofs that define computation, then implementing these models yourself. This deep theoretical understanding provides insight into what can and cannot be computed, and forms the foundation for practical algorithm design and analysis.',
        parentId: null
    },
    {
        id: 'automata_theory',
        title: 'Automata Theory',
        description: 'Formal models of computation',
        content: '<h3>1. Finite Automata</h3>\n<p>Finite automata (FA) are the simplest models of computation, recognizing regular languages through state transitions. These fundamental models provide insight into the capabilities and limitations of the most basic computational devices.</p>\n<p>Key aspects of finite automata include:</p>\n<ul>\n<li><strong>Deterministic Finite Automata (DFA)</strong>: Unique next state for each input</li>\n<li><strong>Nondeterministic Finite Automata (NFA)</strong>: Multiple possible transitions</li>\n<li><strong>State Minimization</strong>: Finding equivalent minimal representations</li>\n<li><strong>Regular Expressions</strong>: Equivalent pattern representations</li>\n<li><strong>Closure Properties</strong>: Operations preserving regular languages</li>\n</ul>\n<p>We\'ll implement finite automata from first principles, building simulators that recognize regular languages. This hands-on approach reveals the mathematical foundation of pattern matching and lexical analysis in computing systems.</p>\n\n<h3>2. Pushdown Automata</h3>\n<p>Pushdown automata (PDA) extend finite automata with a stack memory, enabling recognition of context-free languages. These models correspond to the parsing capabilities needed for programming languages and other nested structures.</p>\n<p>Our implementation of pushdown automata will cover:</p>\n<ul>\n<li><strong>Stack Operations</strong>: Push, pop, and stack manipulation</li>\n<li><strong>Deterministic PDAs</strong>: Unique computation paths</li>\n<li><strong>Nondeterministic PDAs</strong>: Multiple possible computations</li>\n<li><strong>Context-Free Grammars</strong>: Equivalent grammar representations</li>\n<li><strong>Parsing Algorithms</strong>: Recognizing context-free languages</li>\n</ul>\n<p>We\'ll build pushdown automata from scratch, implementing the state transitions and stack operations that recognize nested structures. This implementation-focused approach reveals the theoretical foundations of parsers and the mathematical limits of context-free language recognition.</p>\n\n<h3>3. Turing Machines</h3>\n<p>Turing machines are the most powerful abstract computational model, capable of computing any algorithmically solvable problem. These theoretical devices establish the boundaries of what can be computed by any system.</p>\n<p>Key aspects of Turing machines include:</p>\n<ul>\n<li><strong>Tape and Head Operations</strong>: Read, write, and movement</li>\n<li><strong>Transition Functions</strong>: State changes based on input</li>\n<li><strong>Halting and Acceptance</strong>: Termination conditions</li>\n<li><strong>Multi-tape Machines</strong>: Equivalent extensions</li>\n<li><strong>Universal Turing Machines</strong>: Machines that simulate other machines</li>\n</ul>\n<p>We\'ll implement Turing machine simulators from first principles, developing systems that execute arbitrary Turing machine definitions. This hands-on experience reveals the theoretical model that defines computational possibility and establishes the concept of algorithmic computation.</p>\n\n<h3>4. Automata Transformations</h3>\n<p>Automata transformations convert between different computational models and representations, establishing equivalences and highlighting the relationships between various formalisms. These transformations reveal the structure of the automata hierarchy.</p>\n<p>Our exploration of automata transformations will cover:</p>\n<ul>\n<li><strong>NFA to DFA Conversion</strong>: Subset construction algorithm</li>\n<li><strong>Regular Expressions to Automata</strong>: Thompson\'s construction</li>\n<li><strong>Context-Free Grammar to PDA</strong>: Grammar-driven automaton construction</li>\n<li><strong>Two-Way Automata</strong>: Equivalence to standard models</li>\n<li><strong>Minimization Algorithms</strong>: Hopcroft\'s algorithm and its variants</li>\n</ul>\n<p>We\'ll implement these transformation algorithms from scratch, developing the procedures that convert between equivalent representations. This implementation approach reveals the deep connections between different computational models and the techniques for analyzing and optimizing automata.</p>\n\n<h3>5. Automata-Based Applications</h3>\n<p>Automata-based applications apply theoretical models to practical computing problems, from text processing to hardware design. These implementations transform abstract theory into concrete computational tools.</p>\n<p>Key applications of automata theory include:</p>\n<ul>\n<li><strong>Lexical Analyzers</strong>: Tokenizing programming languages</li>\n<li><strong>Pattern Matching</strong>: Text search and regular expressions</li>\n<li><strong>Protocol Verification</strong>: Modeling communication protocols</li>\n<li><strong>Hardware Circuit Design</strong>: State machine implementation</li>\n<li><strong>Computational Linguistics</strong>: Natural language processing models</li>\n</ul>\n<p>We\'ll implement practical applications of automata theory from first principles, developing tools for text processing, pattern matching, and language analysis. This hands-on approach reveals how abstract computational models translate into practical algorithms and systems used throughout computer science.</p>',
        parentId: 'theory'
    },
    {
        id: 'formal_languages',
        title: 'Formal Languages & Grammars',
        description: 'Mathematical systems for describing languages',
        content: '<h3>1. Formal Language Theory</h3>\n<p>Formal language theory provides the mathematical framework for defining and analyzing sets of strings and the rules that generate them. This fundamental theory establishes the foundation for understanding programming languages and parsing systems.</p>\n<p>Key aspects of formal language theory include:</p>\n<ul>\n<li><strong>Alphabets & Strings</strong>: Symbols and their combinations</li>\n<li><strong>Language Operations</strong>: Union, concatenation, Kleene star</li>\n<li><strong>Language Hierarchies</strong>: Classifications by complexity</li>\n<li><strong>Language Properties</strong>: Closure and decidability</li>\n<li><strong>Language Recognition</strong>: Machines that accept languages</li>\n</ul>\n<p>We\'ll implement language operations from first principles, building libraries for manipulating formal languages. This hands-on approach reveals the mathematical structures that make precise language definitions possible.</p>\n\n<h3>2. Regular Languages & Expressions</h3>\n<p>Regular languages represent the simplest class of formal languages, characterized by finite automata and regular expressions. These languages form the foundation for pattern matching and lexical analysis in computing.</p>\n<p>Our implementation of regular language tools will cover:</p>\n<ul>\n<li><strong>Regular Expression Syntax</strong>: Formal notation systems</li>\n<li><strong>Regex Engine Implementation</strong>: Building a matcher from scratch</li>\n<li><strong>Thompson\'s Construction</strong>: Converting regex to NFA</li>\n<li><strong>Regex Optimization</strong>: Improving matching efficiency</li>\n<li><strong>Regular Language Identities</strong>: Mathematical properties</li>\n</ul>\n<p>We\'ll build regular expression engines from scratch, implementing the algorithms for pattern matching. This implementation-focused approach reveals the theoretical connection between regular expressions, finite automata, and regular languages.</p>\n\n<h3>3. Context-Free Languages & Grammars</h3>\n<p>Context-free languages provide the formal foundation for most programming language syntax, capable of representing nested structures and recursion. These languages are more powerful than regular languages while still maintaining practical parsability.</p>\n<p>Key aspects of context-free languages include:</p>\n<ul>\n<li><strong>Context-Free Grammar Notation</strong>: Production rules and derivations</li>\n<li><strong>Parsing Algorithms</strong>: Recursive descent, LL, LR parsing</li>\n<li><strong>Grammar Transformations</strong>: Eliminating ambiguity and left recursion</li>\n<li><strong>Pushdown Automata</strong>: Theoretical recognition model</li>\n<li><strong>Chomsky Normal Form</strong>: Standardized grammar representation</li>\n</ul>\n<p>We\'ll implement context-free grammar tools from first principles, developing parsers and generators for these languages. This hands-on experience reveals how theoretical models of context-free languages translate into practical parsing techniques for programming languages.</p>\n\n<h3>4. Context-Sensitive & Unrestricted Grammars</h3>\n<p>Context-sensitive and unrestricted grammars represent the more powerful classes of formal languages, capable of expressing complex constraints and computations. These grammars correspond to more powerful computational models with greater expressiveness.</p>\n<p>Our exploration of advanced grammars will cover:</p>\n<ul>\n<li><strong>Context-Sensitive Rules</strong>: Grammar with contextual constraints</li>\n<li><strong>Linear Bounded Automata</strong>: Recognition models</li>\n<li><strong>Unrestricted Grammars</strong>: Turing-complete language definition</li>\n<li><strong>Chomsky Hierarchy</strong>: Classification of language complexity</li>\n<li><strong>Decidability Issues</strong>: Fundamental limitations</li>\n</ul>\n<p>We\'ll implement simple context-sensitive grammar systems from scratch, developing tools for these advanced language classes. This implementation approach reveals the theoretical boundaries between different language classes and their computational implications.</p>\n\n<h3>5. Applications in Language Processing</h3>\n<p>Formal language theory finds practical application in programming language design, compilers, natural language processing, and data validation. These applications transform theoretical concepts into practical tools for computing.</p>\n<p>Key applications of formal language theory include:</p>\n<ul>\n<li><strong>Lexer & Parser Generation</strong>: Building compiler front-ends</li>\n<li><strong>Domain-Specific Languages</strong>: Creating specialized notation</li>\n<li><strong>Grammar Inference</strong>: Learning grammars from examples</li>\n<li><strong>Syntax Highlighting</strong>: Visualizing language structure</li>\n<li><strong>Data Validation</strong>: Ensuring format correctness</li>\n</ul>\n<p>We\'ll implement practical applications of formal language theory from first principles, developing simple compilers, validators, and language processors. This hands-on approach reveals how theoretical language concepts translate into tools that process and analyze structured text in various domains.</p>',
        parentId: 'theory'
    },
    {
        id: 'computability',
        title: 'Computability Theory',
        description: 'The limits of what can be computed',
        content: '<h3>1. Models of Computation</h3>\n<p>Models of computation provide formal frameworks for defining what it means to compute something. These equivalent models establish the robust notion of algorithmic computability independent of specific implementation details.</p>\n<p>Key models of computation include:</p>\n<ul>\n<li><strong>Turing Machines</strong>: Theoretical abstraction of algorithmic processes</li>\n<li><strong>Lambda Calculus</strong>: Functional computation model</li>\n<li><strong>Recursive Functions</strong>: Mathematical definition of computation</li>\n<li><strong>Register Machines</strong>: Memory-based computation model</li>\n<li><strong>Cellular Automata</strong>: Grid-based parallel computation</li>\n</ul>\n<p>We\'ll implement examples of these computational models from first principles, building simulators for each formalism. This hands-on approach reveals the equivalence between seemingly different approaches to computation and establishes the robust foundation of computability theory.</p>\n\n<h3>2. Church-Turing Thesis</h3>\n<p>The Church-Turing thesis posits that every effectively calculable function can be computed by a Turing machine. This foundational principle establishes the boundaries of what can be computed by any algorithmic process.</p>\n<p>Our exploration of the Church-Turing thesis will cover:</p>\n<ul>\n<li><strong>Historical Development</strong>: Origins and refinements</li>\n<li><strong>Effective Calculability</strong>: Intuitive notion of computation</li>\n<li><strong>Equivalence Proofs</strong>: Connections between computational models</li>\n<li><strong>Physical Church-Turing Thesis</strong>: Physical computation limits</li>\n<li><strong>Quantum Computing</strong>: Potential challenges to the thesis</li>\n</ul>\n<p>We\'ll build cross-model simulators from scratch, implementing transformations between computational formalisms. This implementation-focused approach reveals how different models capture the same fundamental notion of algorithmic computation.</p>\n\n<h3>3. Decidability & Undecidability</h3>\n<p>Decidability theory categorizes computational problems based on whether they can be solved algorithmically. Understanding the boundary between decidable and undecidable problems reveals fundamental limitations of computing.</p>\n<p>Key aspects of decidability include:</p>\n<ul>\n<li><strong>Decision Problems</strong>: Yes/no questions for algorithms</li>\n<li><strong>The Halting Problem</strong>: Prototypical undecidable problem</li>\n<li><strong>Proof by Reduction</strong>: Demonstrating undecidability</li>\n<li><strong>Rice\'s Theorem</strong>: Properties of computable functions</li>\n<li><strong>Decidable Problem Classes</strong>: Regular and context-free languages</li>\n</ul>\n<p>We\'ll implement demonstrations of undecidability from first principles, developing concrete examples of unsolvable problems. This hands-on experience reveals the practical implications of theoretical limits and how they constrain algorithm design.</p>\n\n<h3>4. Recursion Theory</h3>\n<p>Recursion theory explores the mathematical structure of computable functions and the degrees of computability. This theory provides a rich framework for classifying computational problems by their relative difficulty.</p>\n<p>Our implementation of recursion theory will cover:</p>\n<ul>\n<li><strong>Primitive Recursive Functions</strong>: Basic computational building blocks</li>\n<li><strong>Recursive Enumerability</strong>: Listable but not decidable problems</li>\n<li><strong>Oracle Machines</strong>: Computation with additional power</li>\n<li><strong>Turing Degrees</strong>: Hierarchies of unsolvability</li>\n<li><strong>Recursive Theorems</strong>: Fixed-point results in computation</li>\n</ul>\n<p>We\'ll build implementations of recursive function theory from scratch, developing the mathematical foundations of computation. This implementation approach reveals the fine structure of computational complexity beyond the decidable/undecidable boundary.</p>\n\n<h3>5. Practical Implications</h3>\n<p>Computability theory has profound implications for practical computing, establishing fundamental limits on what can be automated and verified. These implications inform algorithm design, language development, and formal methods.</p>\n<p>Key practical implications include:</p>\n<ul>\n<li><strong>Program Verification</strong>: Limits of automated correctness checking</li>\n<li><strong>Type Systems</strong>: Static analysis and type checking</li>\n<li><strong>Software Testing</strong>: Theoretical boundaries of test coverage</li>\n<li><strong>Automated Reasoning</strong>: Constraints on theorem proving</li>\n<li><strong>Solvable Subproblems</strong>: Finding decidable special cases</li>\n</ul>\n<p>We\'ll implement practical applications of computability theory from first principles, developing tools that work within established theoretical constraints. This hands-on approach reveals how theoretical limits translate into engineering practices that work around fundamental computational boundaries.</p>',
        parentId: 'theory'
    },
    {
        id: 'complexity_theory',
        title: 'Complexity Theory',
        description: 'Measuring and classifying computational efficiency',
        content: '<h3>1. Computational Resources & Complexity Measures</h3>\n<p>Computational complexity measures quantify the resources required to solve problems algorithmically. These measures provide a theoretical foundation for understanding algorithm efficiency across different problem sizes.</p>\n<p>Key computational resources and measures include:</p>\n<ul>\n<li><strong>Time Complexity</strong>: Computational steps as function of input size</li>\n<li><strong>Space Complexity</strong>: Memory usage as function of input size</li>\n<li><strong>Circuit Complexity</strong>: Hardware resources required</li>\n<li><strong>Asymptotic Analysis</strong>: Big-O, Omega, and Theta notations</li>\n<li><strong>Amortized Analysis</strong>: Average performance over sequences</li>\n</ul>\n<p>We\'ll implement benchmarking tools from first principles, building frameworks for empirically measuring algorithmic resource usage. This hands-on approach reveals how theoretical complexity translates to practical performance and how to analyze algorithms both mathematically and experimentally.</p>\n\n<h3>2. Complexity Classes</h3>\n<p>Complexity classes group computational problems by their resource requirements, establishing a hierarchy of problem difficulty. These classifications provide a map of the computational universe from the easiest to the hardest problems.</p>\n<p>Our implementation of complexity classes will cover:</p>\n<ul>\n<li><strong>P (Polynomial Time)</strong>: Efficiently solvable problems</li>\n<li><strong>NP (Nondeterministic Polynomial)</strong>: Verifiable solutions</li>\n<li><strong>NP-Complete</strong>: The hardest problems in NP</li>\n<li><strong>PSPACE</strong>: Problems solvable with polynomial space</li>\n<li><strong>Exponential Time</strong>: Problems requiring exponential resources</li>\n</ul>\n<p>We\'ll build implementations of problems from different complexity classes from scratch, developing algorithms that showcase theoretical boundaries. This implementation-focused approach reveals the practical differences between complexity classes and why certain problems resist efficient solutions.</p>\n\n<h3>3. Reductions & Completeness</h3>\n<p>Reductions establish relationships between problems, showing how the solution to one problem can transform into a solution for another. These powerful techniques enable the classification of problems within the complexity hierarchy.</p>\n<p>Key aspects of reductions include:</p>\n<ul>\n<li><strong>Problem Transformations</strong>: Converting between problems</li>\n<li><strong>Polynomial-Time Reductions</strong>: Efficient transformations</li>\n<li><strong>NP-Completeness Proofs</strong>: Cook-Levin theorem and extensions</li>\n<li><strong>Completeness for Other Classes</strong>: PSPACE-complete, EXPTIME-complete</li>\n<li><strong>Approximation-Preserving Reductions</strong>: Transferring approximation properties</li>\n</ul>\n<p>We\'ll implement reductions between classic problems from first principles, developing the transformations that establish complexity relationships. This hands-on experience reveals the deep connections between seemingly different computational tasks and how hardness results propagate across problem domains.</p>\n\n<h3>4. The P vs. NP Question</h3>\n<p>The P vs. NP question asks whether problems with efficiently verifiable solutions can always be efficiently solved. This central unsolved problem in computer science has profound implications for algorithm design and computational limits.</p>\n<p>Our exploration of P vs. NP will cover:</p>\n<ul>\n<li><strong>Historical Context</strong>: Origins and significance</li>\n<li><strong>Consequences of P=NP</strong>: Implications for computation</li>\n<li><strong>Evidence for P≠NP</strong>: Why most researchers believe this</li>\n<li><strong>Approaches to Resolution</strong>: Mathematical techniques applied</li>\n<li><strong>Barriers to Proof</strong>: Relativization, natural proofs, algebrization</li>\n</ul>\n<p>We\'ll implement examples that illustrate the P vs. NP boundary, developing concrete problems that demonstrate the efficiency gap. This implementation approach reveals what makes NP-Complete problems challenging and why certain algorithmic techniques fail to bridge the apparent complexity divide.</p>\n\n<h3>5. Beyond Classic Complexity</h3>\n<p>Beyond classical time and space complexity, advanced theories provide additional perspectives on computational efficiency, including approximation, randomization, and quantum computation. These extended frameworks enrich our understanding of algorithmic capabilities.</p>\n<p>Key advanced topics include:</p>\n<ul>\n<li><strong>Approximation Algorithms</strong>: Trading accuracy for efficiency</li>\n<li><strong>Randomized Complexity</strong>: Probabilistic computation models</li>\n<li><strong>Quantum Complexity</strong>: Quantum algorithms and speedups</li>\n<li><strong>Parameterized Complexity</strong>: Fixed-parameter tractability</li>\n<li><strong>Average-Case Analysis</strong>: Typical rather than worst-case behavior</li>\n</ul>\n<p>We\'ll implement advanced complexity frameworks from first principles, developing approximation algorithms, randomized solutions, and simulations of quantum approaches. This hands-on approach reveals practical techniques for addressing inherently difficult problems and how alternative computational models affect problem solvability.</p>',
        parentId: 'theory'
    },
    {
        id: 'information_theory_cs',
        title: 'Information Theory & Coding',
        description: 'Mathematical foundations of data compression and transmission',
        content: '<h3>1. Information & Entropy</h3>\n<p>Information theory provides mathematical tools for quantifying information content and uncertainty. These fundamental concepts establish the theoretical foundations for data compression, transmission, and storage.</p>\n<p>Key information theory concepts include:</p>\n<ul>\n<li><strong>Entropy</strong>: Measure of information or uncertainty</li>\n<li><strong>Self-Information</strong>: Surprise value of individual events</li>\n<li><strong>Joint & Conditional Entropy</strong>: Information in multiple variables</li>\n<li><strong>Relative Entropy</strong>: Kullback-Leibler divergence</li>\n<li><strong>Mutual Information</strong>: Shared information between variables</li>\n</ul>\n<p>We\'ll implement information-theoretic measures from first principles, building libraries for calculating entropy and related quantities. This hands-on approach reveals the mathematical properties of information and how they guide the design of efficient data representations.</p>\n\n<h3>2. Source Coding & Compression</h3>\n<p>Source coding focuses on representing information efficiently, reducing redundancy while preserving essential content. These techniques form the foundation for data compression across various domains.</p>\n<p>Our implementation of source coding will cover:</p>\n<ul>\n<li><strong>Shannon\'s Source Coding Theorem</strong>: Fundamental limits</li>\n<li><strong>Huffman Coding</strong>: Optimal variable-length codes</li>\n<li><strong>Arithmetic Coding</strong>: Approaching entropy limits</li>\n<li><strong>Lempel-Ziv Algorithms</strong>: Dictionary-based compression</li>\n<li><strong>Transform Coding</strong>: Frequency-domain compression</li>\n</ul>\n<p>We\'ll build compression algorithms from scratch, implementing encoders and decoders for various methods. This implementation-focused approach reveals how theoretical information measures translate into practical compression techniques that approach fundamental limits.</p>\n\n<h3>3. Channel Coding & Error Correction</h3>\n<p>Channel coding adds controlled redundancy to protect information during transmission or storage. These techniques enable reliable communication over noisy channels and error-prone storage media.</p>\n<p>Key aspects of channel coding include:</p>\n<ul>\n<li><strong>Noisy Channel Theorem</strong>: Channel capacity limits</li>\n<li><strong>Error Detection Codes</strong>: Parity, checksums, CRC</li>\n<li><strong>Hamming Codes</strong>: Single-error correction</li>\n<li><strong>Reed-Solomon Codes</strong>: Burst error correction</li>\n<li><strong>LDPC & Turbo Codes</strong>: Modern capacity-approaching codes</li>\n</ul>\n<p>We\'ll implement error-correction codes from first principles, developing encoders, decoders, and channel simulators. This hands-on experience reveals how carefully designed redundancy enables reliable information transmission despite noise and interference.</p>\n\n<h3>4. Information Theory in Computing</h3>\n<p>Information theory has profound applications across computer science, from data structures to algorithms to machine learning. These applications leverage information-theoretic principles to optimize computational systems.</p>\n<p>Our exploration of computational applications will cover:</p>\n<ul>\n<li><strong>Algorithmic Information Theory</strong>: Kolmogorov complexity</li>\n<li><strong>Minimum Description Length</strong>: Model selection principle</li>\n<li><strong>Decision Trees & Information Gain</strong>: Machine learning applications</li>\n<li><strong>Hash Functions & Information</strong>: Collision probability</li>\n<li><strong>Cryptographic Applications</strong>: Information-theoretic security</li>\n</ul>\n<p>We\'ll implement information-theoretic algorithms from scratch, developing applications beyond traditional compression and coding. This implementation approach reveals how information concepts extend to diverse areas of computer science, from data structures to machine learning.</p>\n\n<h3>5. Network Information Theory</h3>\n<p>Network information theory extends to multi-user communication systems, establishing limits and strategies for information flow in networks. These advanced concepts address complex scenarios beyond point-to-point communication.</p>\n<p>Key network information theory topics include:</p>\n<ul>\n<li><strong>Multiple Access Channels</strong>: Many senders, one receiver</li>\n<li><strong>Broadcast Channels</strong>: One sender, many receivers</li>\n<li><strong>Relay Channels</strong>: Intermediary nodes in communication</li>\n<li><strong>Network Coding</strong>: Processing information at intermediate nodes</li>\n<li><strong>Distributed Source Coding</strong>: Compressing correlated sources</li>\n</ul>\n<p>We\'ll implement simple network coding schemes from first principles, developing algorithms for information sharing across networks. This hands-on approach reveals how information theory guides network protocol design and how distributed systems can optimally share information across multiple entities.</p>',
        parentId: 'theory'
    },
    
    // Software Engineering
    {
        id: 'software_eng',
        title: 'Software Engineering',
        description: 'Building quality software systems from first principles',
        content: 'Software Engineering is the systematic application of engineering approaches to software development. Our "Build from Scratch" approach means designing and implementing software systems yourself, understanding every component rather than relying on existing frameworks. You\'ll learn methodologies for building reliable, maintainable, and efficient software, with an emphasis on understanding why certain practices work rather than just following established processes.',
        parentId: null
    },
    {
        id: 'system_design',
        title: 'System Architecture & Design',
        description: 'Designing software systems from first principles',
        content: 'System architecture and design establish the overall structure and behavior of software systems. This topic explores methodologies for designing complex software from first principles. You\'ll learn about architectural patterns (layered, microservices, event-driven), design principles (SOLID, DRY, KISS), and modeling techniques. We\'ll implement architectural patterns from scratch, building simple but functional systems that demonstrate each approach. Rather than using existing frameworks that enforce specific architectures, you\'ll develop your own implementations, gaining insight into the tradeoffs between different architectural choices and how to design systems that meet both functional and non-functional requirements while remaining flexible enough to evolve over time.',
        parentId: 'software_eng'
    },
    {
        id: 'software_construction',
        title: 'Software Construction',
        description: 'Building robust software components and systems',
        content: 'Software construction covers the practical techniques for implementing reliable, maintainable code. This topic explores how to turn designs into working software through thoughtful construction practices. You\'ll learn about code organization, defensive programming, error handling, and program correctness. We\'ll implement common design patterns and programming idioms from scratch, understanding their benefits and appropriate use. Rather than relying on existing implementations or frameworks, you\'ll build your own versions of these patterns, gaining insight into how well-structured code supports maintainability, reliability, and efficiency. This approach emphasizes understanding why certain practices lead to better software, not just following conventions.',
        parentId: 'software_eng'
    },
    {
        id: 'testing_verification',
        title: 'Testing & Verification',
        description: 'Ensuring software correctness systematically',
        content: 'Testing and verification provide systematic approaches to ensuring that software behaves as expected. This topic explores different techniques for validating software behavior. You\'ll learn about unit testing, integration testing, system testing, and formal verification methods. We\'ll implement testing frameworks and verification tools from scratch, understanding their theoretical foundations. Rather than using existing testing libraries, you\'ll build your own test runners, assertion mechanisms, mocking frameworks, and simple formal verification tools. This approach provides deep insight into how testing and verification can discover different kinds of defects, the tradeoffs between different approaches, and how to design software that is inherently more testable and verifiable.',
        parentId: 'software_eng'
    },
    {
        id: 'performance_engineering',
        title: 'Performance Engineering',
        description: 'Building efficient software through measurement and optimization',
        content: 'Performance engineering focuses on making software systems faster, more efficient, and more scalable. This topic explores methodologies for understanding and improving system performance. You\'ll learn about profiling, benchmarking, bottleneck analysis, and optimization techniques. We\'ll implement performance measurement tools from scratch, developing profilers, memory analyzers, and benchmarking frameworks. Rather than using existing performance tools, you\'ll build your own instruments for understanding where time and resources are spent in a system. This approach provides deep insight into how algorithms, data structures, and system interactions affect performance, and how to make principled optimizations that improve efficiency without sacrificing maintainability.',
        parentId: 'software_eng'
    },
    {
        id: 'software_processes',
        title: 'Software Development Processes',
        description: 'Methodologies for effective software development',
        content: 'Software development processes provide structured approaches to building software in teams. This topic explores different methodologies for organizing software development activities. You\'ll learn about various process models (waterfall, iterative, agile) and practices (version control, code reviews, continuous integration). We\'ll implement simple tools that support these processes from scratch, including basic version control systems and continuous integration pipelines. Rather than just following established processes or using existing tools, you\'ll develop your own implementations, gaining insight into why certain approaches are effective for different kinds of projects and teams. This understanding allows you to adapt processes to specific contexts rather than applying them dogmatically.',
        parentId: 'software_eng'
    }
];

// Helper functions to work with topics
function getTopicsByParentId(parentId) {
    console.log(`getTopicsByParentId called with parentId: ${parentId}`);
    const result = topics.filter(topic => topic.parentId === parentId);
    console.log(`Found ${result.length} topics with parentId ${parentId}:`, result);
    return result;
}

function getTopicById(id) {
    return topics.find(topic => topic.id === id);
}

function searchTopics(query) {
    query = query.toLowerCase();
    return topics.filter(topic => 
        topic.title.toLowerCase().includes(query) || 
        topic.description.toLowerCase().includes(query) ||
        topic.content.toLowerCase().includes(query)
    );
}