// Define topics array with "Build from Scratch" philosophy
const topics = [
    // CS Foundations
    {
        id: 'cs_foundations',
        title: 'CS Foundations',
        description: 'Basic concepts and foundations of computer science',
        content: 'Computer Science foundations include the basic principles and concepts that underpin all of computing. These foundational elements provide the groundwork for understanding more complex computer systems and software development. Our approach follows the "Build from Scratch" philosophy, teaching you to understand and implement every layer of computing from first principles.',
        parentId: null
    },
    {
        id: 'boolean_logic',
        title: 'Boolean Logic & Digital Circuits',
        description: 'The fundamental building blocks of computation',
        content: '<h3>1. Boolean Algebra Fundamentals</h3>\n<p>Boolean algebra, developed by George Boole in the 19th century, provides the mathematical foundation for all digital systems. Unlike traditional algebra which works with continuous numerical values, Boolean algebra operates on binary values (true/false, 1/0) and defines operations that manipulate these values.</p>\n<p>The three fundamental Boolean operations are:</p>\n<ul>\n<li><strong>AND</strong>: Output is true only when both inputs are true</li>\n<li><strong>OR</strong>: Output is true when at least one input is true</li>\n<li><strong>NOT</strong>: Output is the opposite of the input (inverts the value)</li>\n</ul>\n<p>In Boolean algebra, these operations are rigorously defined with truth tables and algebraic laws that allow us to analyze and manipulate logical expressions. By understanding and applying Boolean algebra, we can design digital circuits that perform specific logical functions.</p>\n\n<h3>2. Implementing Logic with Transistors</h3>\n<p>Transistors are the physical devices that implement Boolean logic in hardware. A transistor can act as an electronically controlled switch, allowing or blocking the flow of current based on a control signal. By arranging transistors in specific configurations, we can create physical implementations of logical operations.</p>\n<p>The two main types of transistor logic families are:</p>\n<ul>\n<li><strong>TTL (Transistor-Transistor Logic)</strong>: Uses bipolar junction transistors</li>\n<li><strong>CMOS (Complementary Metal-Oxide-Semiconductor)</strong>: Uses field-effect transistors in complementary pairs</li>\n</ul>\n<p>When building from scratch, we\'ll examine how transistors function at the quantum level and how their physical properties enable them to implement logical operations. Rather than taking these components as black boxes, we\'ll understand exactly how electrons flow through semiconductor materials to create switching behavior.</p>\n\n<h3>3. Basic Logic Gates</h3>\n<p>Logic gates are the fundamental building blocks of digital circuits, directly implementing Boolean functions. Each type of gate performs a specific logical operation on its inputs.</p>\n<p>The basic gates include:</p>\n<ul>\n<li><strong>AND Gate</strong>: Output is high (1) only when all inputs are high</li>\n<li><strong>OR Gate</strong>: Output is high when at least one input is high</li>\n<li><strong>NOT Gate (Inverter)</strong>: Output is the opposite of the input</li>\n<li><strong>NAND Gate</strong>: Combines AND and NOT operations (universal gate)</li>\n<li><strong>NOR Gate</strong>: Combines OR and NOT operations (universal gate)</li>\n<li><strong>XOR Gate</strong>: Output is high when inputs are different</li>\n<li><strong>XNOR Gate</strong>: Output is high when inputs are the same</li>\n</ul>\n<p>We\'ll implement each of these gates from transistors, understanding their internal structure rather than treating them as prefabricated components. This approach reveals how complex logical functions can be built from simple electronic components.</p>\n\n<h3>4. Combinational Logic Circuits</h3>\n<p>Combinational circuits perform operations where the output depends only on the current input values, without any memory of previous states. These circuits combine multiple logic gates to implement more complex functions.</p>\n<p>Key combinational circuits include:</p>\n<ul>\n<li><strong>Multiplexers (MUX)</strong>: Select one of several input signals based on control inputs</li>\n<li><strong>Demultiplexers (DEMUX)</strong>: Route an input signal to one of several outputs</li>\n<li><strong>Encoders</strong>: Convert multiple input signals into a binary code</li>\n<li><strong>Decoders</strong>: Convert a binary code into multiple output signals</li>\n<li><strong>Adders</strong>: Perform binary addition on two or more numbers</li>\n<li><strong>Comparators</strong>: Compare two binary numbers and determine their relationship</li>\n</ul>\n<p>By building these circuits from basic gates (which we\'ve already built from transistors), we\'ll gain a comprehensive understanding of how complex logical operations can be implemented in hardware.</p>\n\n<h3>5. Sequential Logic and Memory</h3>\n<p>While combinational logic computes based solely on current inputs, sequential logic allows circuits to have memory and maintain state over time. This is achieved through feedback loops and storage elements.</p>\n<p>The fundamental sequential elements include:</p>\n<ul>\n<li><strong>Latches</strong>: Basic storage elements that hold a value as long as enabled</li>\n<li><strong>Flip-flops</strong>: Edge-triggered storage elements that change state only at specific clock transitions</li>\n<li><strong>Registers</strong>: Groups of flip-flops that store multiple bits of information</li>\n<li><strong>Counters</strong>: Sequential circuits that cycle through a sequence of states</li>\n</ul>\n<p>We\'ll build these sequential components from scratch using our fundamental gates, understanding how feedback and timing create memory in electronic systems. This forms the bridge between combinational logic and more complex computational systems with state.</p>',
        parentId: 'cs_foundations'
    },
    {
        id: 'information_theory',
        title: 'Information Theory & Representation',
        description: 'How information is quantified, stored, and processed',
        content: '<h3>1. Foundations of Information Theory</h3>\n<p>Information theory, developed by Claude Shannon in the 1940s, provides a mathematical framework for quantifying information and understanding the fundamental limits of data compression and transmission. This revolutionary framework bridges mathematics, physics, and computer science.</p>\n<p>Key concepts in information theory include:</p>\n<ul>\n<li><strong>Information and Uncertainty</strong>: Information resolves uncertainty; the more uncertain an outcome, the more information is gained when learning it</li>\n<li><strong>Probability Distributions</strong>: The likelihood of different messages or symbols appearing in a communication stream</li>\n<li><strong>Surprise Value</strong>: Rare events carry more information than common ones</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, deriving the mathematical relationships rather than accepting them as given, building a strong foundation for understanding how information can be measured, stored, and communicated efficiently.</p>\n\n<h3>2. Measuring Information: Entropy</h3>\n<p>Entropy is the fundamental measure of information content, quantifying the average amount of uncertainty in a random variable or data source. It establishes theoretical limits on data compression and transmission.</p>\n<p>We\'ll explore several key measures:</p>\n<ul>\n<li><strong>Shannon Entropy</strong>: The average information content per symbol in a source, measured in bits</li>\n<li><strong>Joint Entropy</strong>: The total entropy of two or more random variables considered together</li>\n<li><strong>Conditional Entropy</strong>: The remaining uncertainty in one variable when the other is known</li>\n<li><strong>Relative Entropy (KL Divergence)</strong>: A measure of the difference between two probability distributions</li>\n<li><strong>Mutual Information</strong>: The information shared between two variables, indicating their dependence</li>\n</ul>\n<p>By implementing these measures from scratch, we\'ll develop a deep understanding of how to quantify information in any system, from text documents to quantum states, without relying on existing information theory libraries or abstractions.</p>\n\n<h3>3. Binary Representation & Coding</h3>\n<p>All information in digital systems must ultimately be represented as sequences of binary digits (bits). This section explores how various types of information are encoded in binary and the fundamental trade-offs in different coding schemes.</p>\n<p>We\'ll examine various coding systems:</p>\n<ul>\n<li><strong>Fixed-Length Codes</strong>: Each symbol uses the same number of bits (e.g., ASCII, Unicode)</li>\n<li><strong>Variable-Length Codes</strong>: Different symbols use different numbers of bits based on their probability (e.g., Morse code, Huffman coding)</li>\n<li><strong>Prefix Codes</strong>: No codeword is a prefix of another, enabling unambiguous decoding</li>\n<li><strong>Source Coding</strong>: Techniques to represent information using fewer bits while preserving meaning</li>\n</ul>\n<p>We\'ll implement these coding schemes from scratch, building encoders and decoders that transform information between human-readable forms and efficient binary representations, developing the skills to create custom encoding systems for any information type.</p>\n\n<h3>4. Data Compression Algorithms</h3>\n<p>Data compression reduces the size of information for efficient storage and transmission, applying information theory principles to practical systems. This section focuses on lossless compression techniques that allow perfect reconstruction of the original data.</p>\n<p>We\'ll implement several key compression algorithms from scratch:</p>\n<ul>\n<li><strong>Run-Length Encoding</strong>: Compressing sequences of repeated values</li>\n<li><strong>Huffman Coding</strong>: Creating optimal variable-length prefix codes based on symbol frequencies</li>\n<li><strong>Arithmetic Coding</strong>: Encoding entire messages as fractional numbers for near-optimal compression</li>\n<li><strong>Dictionary Methods</strong>: LZ77, LZ78, and LZW algorithms that build dictionaries of repeated patterns</li>\n<li><strong>Context Modeling</strong>: Using previous symbols to predict and efficiently encode the next symbol</li>\n</ul>\n<p>By building these algorithms from their mathematical foundations rather than using existing libraries, we\'ll understand exactly how each technique works, their computational complexity, and how they approach the theoretical limits established by information theory.</p>\n\n<h3>5. Digital Media Representation</h3>\n<p>This section explores how complex forms of information like text, images, audio, and video are represented in digital systems, applying information theory principles to real-world media formats.</p>\n<p>We\'ll examine representation schemes for various media types:</p>\n<ul>\n<li><strong>Text Encoding</strong>: Character sets (ASCII, Unicode), text compression, and search structures</li>\n<li><strong>Image Representation</strong>: Bitmap formats, color spaces, spatial/frequency domain representations</li>\n<li><strong>Audio Encoding</strong>: Sampling, quantization, time and frequency domain representations</li>\n<li><strong>Video Formats</strong>: Frame-based representation, motion compensation, temporal compression</li>\n</ul>\n<p>For each media type, we\'ll implement basic encoders and decoders from scratch, understanding how information theory principles lead to efficient representations. This approach avoids relying on existing media libraries, instead building the fundamental algorithms that underlie all digital media systems.</p>',
        parentId: 'cs_foundations'
    },
    {
        id: 'computation_models',
        title: 'Models of Computation',
        description: 'Theoretical frameworks that define computation',
        content: '<h3>1. Foundations of Computation Theory</h3>\n<p>Computation theory examines what problems can be solved algorithmically and how efficiently they can be solved. This foundation is essential for understanding the fundamental capabilities and limitations of computing systems.</p>\n<p>Key concepts in computation theory include:</p>\n<ul>\n<li><strong>Algorithms</strong>: Step-by-step procedures for solving problems</li>\n<li><strong>Decidability</strong>: Whether a problem has an algorithmic solution</li>\n<li><strong>Tractability</strong>: Whether a problem can be solved efficiently</li>\n<li><strong>Reduction</strong>: Transforming one problem into another to relate their complexity</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, examining the nature of algorithms and computation before introducing formal models. This approach provides a philosophical and intuitive foundation for the more rigorous mathematical structures that follow.</p>\n\n<h3>2. Finite Automata & Regular Languages</h3>\n<p>Finite automata are the simplest models of computation, capable of recognizing regular languages. They provide a foundation for understanding more complex computational models and are widely used in lexical analysis, pattern matching, and digital circuit design.</p>\n<p>We\'ll explore several types of finite automata:</p>\n<ul>\n<li><strong>Deterministic Finite Automata (DFA)</strong>: Machines where each state has exactly one transition for each input symbol</li>\n<li><strong>Nondeterministic Finite Automata (NFA)</strong>: Machines that can have multiple possible transitions for a single input</li>\n<li><strong>Regular Expressions</strong>: A compact notation for describing regular languages</li>\n<li><strong>State Minimization</strong>: Techniques for finding the smallest equivalent automaton</li>\n</ul>\n<p>We\'ll implement simulators for these machines from scratch, building a library that can define, visualize, and execute finite automata on arbitrary inputs. This implementation will avoid existing automata libraries, instead developing the algorithms and data structures directly from their mathematical definitions.</p>\n\n<h3>3. Context-Free Languages & Pushdown Automata</h3>\n<p>Context-free languages extend regular languages with recursive structures, making them appropriate for modeling programming languages, nested expressions, and natural language syntax. Pushdown automata add a stack to finite automata, providing the memory needed to recognize these languages.</p>\n<p>Key aspects we\'ll explore include:</p>\n<ul>\n<li><strong>Context-Free Grammars (CFG)</strong>: Formal systems for generating languages with recursive structure</li>\n<li><strong>Derivations and Parse Trees</strong>: Representations of how strings are generated by a grammar</li>\n<li><strong>Pushdown Automata (PDA)</strong>: Finite automata augmented with an infinite stack</li>\n<li><strong>Parsing Algorithms</strong>: Top-down (recursive descent, LL) and bottom-up (LR) approaches</li>\n</ul>\n<p>We\'ll implement parsers and pushdown automata simulators from scratch, building both theoretical models and practical parsing tools without relying on parser generators or existing libraries. This approach provides deep insight into the structure of programming languages and compilers.</p>\n\n<h3>4. Turing Machines & Computability</h3>\n<p>Turing machines represent the most powerful general model of computation, capturing the full range of algorithmic processes. They define the boundary between computable and uncomputable problems and form the theoretical foundation of modern computers.</p>\n<p>We\'ll explore several aspects of Turing machines and computability:</p>\n<ul>\n<li><strong>Turing Machine Definition</strong>: Formal structure with unlimited memory tape and finite control</li>\n<li><strong>Universal Turing Machines</strong>: Machines that can simulate any other Turing machine</li>\n<li><strong>The Church-Turing Thesis</strong>: The hypothesis that Turing machines capture all effective computation</li>\n<li><strong>Undecidable Problems</strong>: The Halting Problem and other problems with no algorithmic solution</li>\n<li><strong>Reductions</strong>: Techniques for proving problems undecidable</li>\n</ul>\n<p>We\'ll implement Turing machine simulators from scratch, building systems that can define, visualize, and execute these theoretical machines on arbitrary inputs. This implementation will provide concrete insight into these abstract models, making their theoretical properties tangible through direct experimentation.</p>\n\n<h3>5. Computational Complexity</h3>\n<p>Computational complexity theory classifies problems based on the resources (time, space) required to solve them, establishing a hierarchy of complexity classes that organizes problems by their inherent difficulty.</p>\n<p>Key concepts in complexity theory include:</p>\n<ul>\n<li><strong>Time and Space Complexity</strong>: How resource requirements scale with input size</li>\n<li><strong>Complexity Classes</strong>: P, NP, PSPACE, and their relationships</li>\n<li><strong>NP-Completeness</strong>: Problems that are as hard as any in NP</li>\n<li><strong>Polynomial-Time Reductions</strong>: Transformations that preserve complexity</li>\n<li><strong>Approximation Algorithms</strong>: Efficient approaches for intractable problems</li>\n</ul>\n<p>We\'ll implement algorithms for classic problems from different complexity classes, analyzing their performance and understanding the fundamental reasons why some problems require exponentially more resources than others. This approach grounds abstract complexity theory in concrete implementations, providing insight into the practical implications of theoretical complexity barriers.</p>',
        parentId: 'cs_foundations'
    },
    
    // Computer Hardware
    {
        id: 'hardware',
        title: 'Computer Hardware',
        description: 'Physical components and architecture of computers',
        content: 'Computer hardware encompasses all the physical components that make up a computer system. Understanding hardware is essential for optimizing performance and troubleshooting issues in computing systems. Our "Build from Scratch" approach means learning about every layer of hardware, from transistors and logic gates up to complete computer systems, emphasizing how each component can be understood from first principles.',
        parentId: null
    },
    {
        id: 'transistors',
        title: 'Transistors & Logic Gates',
        description: 'The fundamental building blocks of digital systems',
        content: '<h3>1. Semiconductor Physics Fundamentals</h3>\n<p>At the heart of all modern computing lies semiconductor physics, which enables the creation of transistors. This section explores the physical principles that make computing possible at the atomic level.</p>\n<p>Key concepts in semiconductor physics include:</p>\n<ul>\n<li><strong>Atomic Structure</strong>: Electron shells, valence electrons, and how atoms bond together</li>\n<li><strong>Band Theory</strong>: Energy bands in solids, conduction bands, valence bands, and band gaps</li>\n<li><strong>Semiconductor Materials</strong>: Silicon, germanium, and compound semiconductors</li>\n<li><strong>Doping</strong>: Adding impurities to create n-type and p-type semiconductors</li>\n<li><strong>Carrier Transport</strong>: How electrons and holes move through semiconductor materials</li>\n</ul>\n<p>We\'ll explore these concepts from first principles, starting with quantum mechanics and atomic theory, building up to practical semiconductor behavior. This approach provides a strong foundation for understanding the physical layer of computing without relying on simplified abstractions.</p>\n\n<h3>2. Diodes and PN Junctions</h3>\n<p>Before understanding transistors, we must understand the PN junction, which forms the basis of semiconductor diodes and is a crucial component of transistors.</p>\n<p>Key aspects of PN junctions include:</p>\n<ul>\n<li><strong>Junction Formation</strong>: How p-type and n-type semiconductors interact when placed together</li>\n<li><strong>Depletion Region</strong>: The zone where free carriers are removed through diffusion</li>\n<li><strong>Forward and Reverse Bias</strong>: How applied voltage affects current flow through the junction</li>\n<li><strong>IV Characteristics</strong>: The nonlinear relationship between current and voltage</li>\n<li><strong>Junction Capacitance</strong>: How the junction stores charge and responds to changing voltages</li>\n</ul>\n<p>We\'ll analyze these junctions with mathematical models and build simple circuits to observe their behavior firsthand, understanding them from the atomic level up rather than as idealized components.</p>\n\n<h3>3. Transistor Types and Operation</h3>\n<p>Transistors are semiconductor devices that can amplify signals and act as electronic switches, forming the fundamental building blocks of all computing systems.</p>\n<p>We\'ll explore different transistor types and their operational principles:</p>\n<ul>\n<li><strong>Bipolar Junction Transistors (BJTs)</strong>: NPN and PNP structures, current amplification, operating regions</li>\n<li><strong>Field-Effect Transistors (FETs)</strong>: JFETs and MOSFETs, voltage control, channel formation</li>\n<li><strong>CMOS Technology</strong>: Complementary MOSFET pairs that form the basis of modern digital ICs</li>\n<li><strong>Transistor Parameters</strong>: Gain, frequency response, switching speed, power dissipation</li>\n<li><strong>Small-Signal Models</strong>: Mathematical representations for analyzing transistor behavior</li>\n</ul>\n<p>We\'ll implement circuits with discrete transistors, measuring their characteristics and understanding how their physical properties determine their electronic behavior, rather than simply using them as black-box components.</p>\n\n<h3>4. Digital Logic Implementation</h3>\n<p>This section explores how transistors are arranged to implement Boolean logic functions, creating the fundamental gates that form the basis of all digital systems.</p>\n<p>We\'ll examine different logic families and their implementations:</p>\n<ul>\n<li><strong>Resistor-Transistor Logic (RTL)</strong>: Simple but inefficient early implementation</li>\n<li><strong>Diode-Transistor Logic (DTL)</strong>: Improved speed with diodes for input processing</li>\n<li><strong>Transistor-Transistor Logic (TTL)</strong>: Faster switching with multiple transistors</li>\n<li><strong>CMOS Logic</strong>: Low power, high density implementation dominating modern design</li>\n<li><strong>Logic Levels and Noise Margins</strong>: How voltage ranges represent binary values reliably</li>\n</ul>\n<p>For each logic family, we\'ll build actual gates from individual transistors, measuring their performance characteristics and understanding the trade-offs between speed, power, and complexity, without relying on integrated circuits.</p>\n\n<h3>5. From Transistors to Integrated Circuits</h3>\n<p>Modern computing depends on the ability to integrate billions of transistors onto a single chip. This section explores how individual transistors evolved into large-scale integrated circuits.</p>\n<p>Key aspects of IC technology include:</p>\n<ul>\n<li><strong>Fabrication Processes</strong>: Photolithography, etching, doping, and metallization</li>\n<li><strong>Process Scaling</strong>: How transistors have shrunk from micrometers to nanometers</li>\n<li><strong>Moore\'s Law</strong>: The historical trend and physical limits of transistor scaling</li>\n<li><strong>Design Rules</strong>: Constraints that determine what can be physically manufactured</li>\n<li><strong>Packaging and Interconnects</strong>: How chips are connected to the outside world</li>\n</ul>\n<p>We\'ll examine detailed fabrication steps and design simple integrated circuits at the transistor level, understanding the physical and economic factors that shape modern chips rather than treating ICs as mysterious black boxes.</p>',
        parentId: 'hardware'
    },
    {
        id: 'digital_circuits',
        title: 'Digital Circuits & Components',
        description: 'Building useful circuits from basic logic gates',
        content: '<h3>1. Fundamentals of Digital Circuit Design</h3>\n<p>Digital circuit design is the process of creating electronic circuits that operate on binary signals, forming the foundation of all computing hardware. This section covers the essential principles and methodologies for designing reliable digital systems.</p>\n<p>Key concepts in digital design include:</p>\n<ul>\n<li><strong>Boolean Algebra in Practice</strong>: Applying Boolean algebra to simplify and optimize circuit designs</li>\n<li><strong>Circuit Minimization</strong>: Karnaugh maps and Quine-McCluskey algorithm for logic simplification</li>\n<li><strong>Timing Analysis</strong>: Propagation delay, setup/hold times, and critical paths</li>\n<li><strong>Noise and Interference</strong>: Design techniques to ensure signal integrity</li>\n<li><strong>Power Considerations</strong>: Static and dynamic power consumption optimization</li>\n</ul>\n<p>We\'ll apply these concepts by designing circuits on paper and then implementing them with physical components, focusing on understanding both theoretical and practical aspects of digital design from first principles.</p>\n\n<h3>2. Combinational Logic Circuits</h3>\n<p>Combinational circuits produce outputs that depend only on the current inputs, with no memory of past states. These circuits perform the fundamental data processing operations in a computer.</p>\n<p>We\'ll design and build key combinational components:</p>\n<ul>\n<li><strong>Adders</strong>: Half adders, full adders, and ripple-carry adders for binary arithmetic</li>\n<li><strong>Multiplexers (MUX)</strong>: Circuits that select one output from multiple inputs based on control signals</li>\n<li><strong>Demultiplexers (DEMUX)</strong>: Circuits that route a single input to one of multiple outputs</li>\n<li><strong>Encoders and Decoders</strong>: Circuits that convert between binary codes and one-hot representations</li>\n<li><strong>Comparators</strong>: Circuits that compare binary numbers and indicate relationships (equal, greater, less)</li>\n</ul>\n<p>For each component, we\'ll implement multiple design approaches using only basic gates (AND, OR, NOT) rather than using integrated circuit packages, understanding the trade-offs between design complexity, performance, and resource usage.</p>\n\n<h3>3. Sequential Logic Elements</h3>\n<p>Sequential circuits contain memory elements that store information over time, allowing systems to maintain state. These elements are essential for creating computers that can execute multi-step operations.</p>\n<p>We\'ll explore the fundamental sequential components:</p>\n<ul>\n<li><strong>Latches</strong>: SR, gated D, and other basic storage elements</li>\n<li><strong>Flip-flops</strong>: Edge-triggered storage devices (D, JK, T types)</li>\n<li><strong>Registers</strong>: Arrays of flip-flops that store multiple bits of information</li>\n<li><strong>Shift Registers</strong>: Serial-to-parallel and parallel-to-serial conversion</li>\n<li><strong>Master-Slave Designs</strong>: Preventing race conditions in sequential circuits</li>\n</ul>\n<p>We\'ll build each of these components from scratch using only basic gates, analyzing their timing behavior, metastability issues, and how their physical implementation affects their performance characteristics.</p>\n\n<h3>4. Synchronous Sequential Circuits</h3>\n<p>Synchronous sequential circuits use clock signals to coordinate state changes, enabling complex, deterministic behavior essential for computing systems. These circuits form the core of control units and data paths in processors.</p>\n<p>Key synchronous circuit types we\'ll implement include:</p>\n<ul>\n<li><strong>Counters</strong>: Asynchronous and synchronous designs (binary, decade, Johnson, etc.)</li>\n<li><strong>State Machines</strong>: Moore and Mealy machine implementations</li>\n<li><strong>Sequencers</strong>: Programmable circuits that generate control signals in sequence</li>\n<li><strong>Pulse Generators</strong>: Circuits for creating precisely timed control signals</li>\n<li><strong>Clock Distribution</strong>: Techniques for delivering reliable clock signals throughout a system</li>\n</ul>\n<p>Our implementations will focus on understanding timing requirements, hazard avoidance, and the relationship between state diagrams, state tables, and actual circuit implementations, all built from basic gates rather than using specialized ICs.</p>\n\n<h3>5. Advanced Digital Design Techniques</h3>\n<p>Beyond basic components, modern digital systems employ sophisticated techniques to improve performance, reliability, and functionality. This section explores design approaches used in complex digital systems.</p>\n<p>Advanced techniques we\'ll explore include:</p>\n<ul>\n<li><strong>Pipelining</strong>: Breaking operations into stages to increase throughput</li>\n<li><strong>Asynchronous Design</strong>: Creating systems without global clock signals</li>\n<li><strong>Hardware Description Languages</strong>: Using VHDL or Verilog to describe complex circuits</li>\n<li><strong>Fault-Tolerant Design</strong>: Creating circuits that can detect and correct errors</li>\n<li><strong>Testability</strong>: Designing circuits for ease of testing and verification</li>\n</ul>\n<p>We\'ll implement examples of these techniques with discrete components where possible and use hardware description languages for more complex designs, maintaining our focus on understanding principles rather than using pre-built components. This approach provides the foundation needed to design sophisticated digital systems from first principles.</p>',
        parentId: 'hardware'
    },
    {
        id: 'cpu_design',
        title: 'CPU Design & Architecture',
        description: 'Designing a functional CPU from basic components',
        content: '<h3>1. CPU Architecture Fundamentals</h3>\n<p>CPU architecture defines the organizational structure and operational principles of a processor. Understanding these foundational concepts is essential for designing functional processors from basic components.</p>\n<p>Key architectural concepts include:</p>\n<ul>\n<li><strong>Von Neumann Architecture</strong>: The classic stored-program computer with shared memory for instructions and data</li>\n<li><strong>Harvard Architecture</strong>: Systems with separate instruction and data memories for improved performance</li>\n<li><strong>CISC vs. RISC Designs</strong>: Trade-offs between complex and reduced instruction sets</li>\n<li><strong>Word Size and Address Space</strong>: How bit width affects addressable memory and processing capabilities</li>\n<li><strong>Execution Models</strong>: Sequential, pipelined, superscalar, and out-of-order execution approaches</li>\n</ul>\n<p>We\'ll explore these architectures by designing simple versions of each, comparing their characteristics and understanding the historical and technical factors that influenced their development, focusing on first principles rather than existing commercial architectures.</p>\n\n<h3>2. Instruction Set Architecture (ISA)</h3>\n<p>The instruction set architecture is the interface between hardware and software, defining the processor\'s capabilities from the programmer\'s perspective. Designing an effective ISA involves carefully balancing expressiveness, efficiency, and implementability.</p>\n<p>Our ISA design process will cover:</p>\n<ul>\n<li><strong>Instruction Formats</strong>: Designing fixed and variable-length encoding schemas</li>\n<li><strong>Addressing Modes</strong>: Methods for specifying operand locations (immediate, direct, indirect, indexed)</li>\n<li><strong>Operation Categories</strong>: Arithmetic, logical, data movement, control flow, and special instructions</li>\n<li><strong>Condition Codes</strong>: Status flags and their role in conditional operations</li>\n<li><strong>ISA Extensions</strong>: Specialized instructions for enhanced functionality (SIMD, crypto, etc.)</li>\n</ul>\n<p>Rather than copying existing instruction sets, we\'ll design our own minimal but complete ISA from first principles, carefully considering the trade-offs of each design decision and how it affects both hardware implementation and software development.</p>\n\n<h3>3. Arithmetic Logic Units (ALUs)</h3>\n<p>The ALU is the computational heart of a CPU, performing mathematical and logical operations on data. Creating an efficient ALU requires understanding how to implement various operations in digital logic.</p>\n<p>We\'ll design and implement ALU components including:</p>\n<ul>\n<li><strong>Integer Arithmetic</strong>: Adders, subtractors, and multipliers with various speed/area trade-offs</li>\n<li><strong>Logical Operations</strong>: AND, OR, XOR, and NOT circuits with bit-parallel processing</li>\n<li><strong>Shifters and Rotators</strong>: Barrel shifters and logic for bit manipulation operations</li>\n<li><strong>Comparators</strong>: Circuits for equality and magnitude comparison</li>\n<li><strong>Flags Generation</strong>: Circuits to detect zero, negative, carry, overflow, and parity conditions</li>\n</ul>\n<p>We\'ll build our ALU entirely from the digital components previously studied, avoiding pre-built ALU modules and understanding the trade-offs between different implementations in terms of speed, area, and power consumption.</p>\n\n<h3>4. Control Unit Design</h3>\n<p>The control unit orchestrates the operation of the CPU, generating the precise sequence of control signals needed to execute instructions. This component translates the ISA into the actual hardware behavior.</p>\n<p>We\'ll explore two main approaches to control unit design:</p>\n<ul>\n<li><strong>Hardwired Control</strong>: Implementing control logic directly with combinational circuits</li>\n<li><strong>Microprogrammed Control</strong>: Using a microcode ROM to store control sequences</li>\n<li><strong>Instruction Fetch and Decode</strong>: Circuits for retrieving and interpreting instructions</li>\n<li><strong>Pipeline Control</strong>: Managing the flow of instructions through multiple pipeline stages</li>\n<li><strong>Hazard Detection and Handling</strong>: Addressing data, control, and structural hazards</li>\n</ul>\n<p>Our implementation will include complete control units using both approaches, comparing their advantages and disadvantages, and understanding how they generate the precise timing and control signals required for correct CPU operation.</p>\n\n<h3>5. Building a Complete CPU</h3>\n<p>Bringing all components together to create a working CPU requires careful integration and validation. This final section focuses on system integration, testing, and validation of a complete processor.</p>\n<p>Key aspects of CPU integration include:</p>\n<ul>\n<li><strong>Data Path Design</strong>: Connecting ALU, registers, and memory interfaces</li>\n<li><strong>Register File Implementation</strong>: Creating efficient multi-port register storage</li>\n<li><strong>Memory Interfacing</strong>: Designing address decoders and data buses</li>\n<li><strong>System Timing</strong>: Ensuring proper clock distribution and signal synchronization</li>\n<li><strong>CPU Verification</strong>: Testing strategies to validate correctness at various abstraction levels</li>\n</ul>\n<p>We\'ll implement our complete CPU design using a hardware description language (HDL), following a systematic process from specification to simulation to synthesis. Unlike typical courses that use pre-designed CPUs, we\'ll build everything from the logic gates up, gaining deep insight into every aspect of processor operation.</p>',
        parentId: 'hardware'
    },
    {
        id: 'memory_systems',
        title: 'Memory Systems',
        description: 'From individual bits to complex memory hierarchies',
        content: '<h3>1. Memory Cell Technologies</h3>\n<p>At the heart of computer memory are the physical mechanisms for storing binary information. Different memory technologies offer various trade-offs in terms of speed, density, volatility, and cost.</p>\n<p>We\'ll explore several fundamental memory cell technologies:</p>\n<ul>\n<li><strong>Static RAM (SRAM)</strong>: Six-transistor cells that maintain state without refresh</li>\n<li><strong>Dynamic RAM (DRAM)</strong>: Single-transistor-capacitor cells requiring periodic refresh</li>\n<li><strong>Non-volatile Memories</strong>: Flash, EEPROM, and emerging technologies (MRAM, ReRAM, etc.)</li>\n<li><strong>Magnetic Storage</strong>: Hard disk drive technology and principles</li>\n<li><strong>Optical Storage</strong>: CD, DVD, and Blu-ray recording and reading mechanisms</li>\n</ul>\n<p>For each technology, we\'ll examine the physics of operation at the transistor or material level, implementing simple versions where possible and understanding their fundamental characteristics, limitations, and manufacturing processes rather than treating them as abstract components.</p>\n\n<h3>2. Memory Organization and Addressing</h3>\n<p>Memory organization determines how individual cells are arranged into addressable units and accessed by the processor. Effective organization balances capacity, speed, and power consumption.</p>\n<p>Key aspects of memory organization include:</p>\n<ul>\n<li><strong>Memory Arrays</strong>: Row and column decoders, sense amplifiers, and timing circuits</li>\n<li><strong>Address Mapping</strong>: How logical addresses translate to physical memory locations</li>\n<li><strong>Word Size and Alignment</strong>: Data organization and access restrictions</li>\n<li><strong>Error Detection and Correction</strong>: Parity, ECC, and other techniques</li>\n<li><strong>Multi-bank Organizations</strong>: Interleaved access for improved performance</li>\n</ul>\n<p>We\'ll design and implement memory subsystems from basic components, including address decoders, data buffers, and control logic, avoiding pre-built memory modules and understanding the detailed operation of memory arrays and their surrounding circuitry.</p>\n\n<h3>3. Memory Hierarchy and Caching</h3>\n<p>Memory hierarchies exploit locality of reference to bridge the speed gap between fast processors and slower main memory. Caching is the key technique that makes this hierarchy effective.</p>\n<p>Our implementation of memory hierarchies will cover:</p>\n<ul>\n<li><strong>Register Files</strong>: Processor-integrated multi-port memory systems</li>\n<li><strong>Cache Architectures</strong>: Direct-mapped, set-associative, and fully-associative designs</li>\n<li><strong>Cache Replacement Policies</strong>: LRU, FIFO, random, and other algorithms</li>\n<li><strong>Write Policies</strong>: Write-through vs. write-back strategies</li>\n<li><strong>Coherence Protocols</strong>: Maintaining consistency in multi-cache systems</li>\n</ul>\n<p>We\'ll implement complete cache controllers from digital logic components, simulating their behavior with realistic memory access patterns and measuring performance metrics like hit rate and average access time, building an intuitive understanding of locality and its exploitation.</p>\n\n<h3>4. Virtual Memory Systems</h3>\n<p>Virtual memory extends the memory hierarchy to disk storage, providing the illusion of a larger memory space than physically available and isolating processes from each other.</p>\n<p>Key aspects of virtual memory include:</p>\n<ul>\n<li><strong>Address Translation</strong>: Page tables, TLBs, and translation mechanisms</li>\n<li><strong>Page Replacement Algorithms</strong>: LRU, clock, working set, and other approaches</li>\n<li><strong>Memory Protection</strong>: Access rights enforcement and process isolation</li>\n<li><strong>Demand Paging</strong>: Loading pages only when needed</li>\n<li><strong>Memory-Mapped Files</strong>: Integrating file I/O with memory access</li>\n</ul>\n<p>We\'ll implement virtual memory controllers and memory management units (MMUs) from basic components, understanding the hardware-software interaction in modern memory systems and how operating systems use the hardware capabilities to provide protected, virtualized memory environments.</p>\n\n<h3>5. Memory Controllers and Interfaces</h3>\n<p>Memory controllers bridge the gap between processors and memory devices, handling the complex timing and protocol requirements of modern memory systems.</p>\n<p>Our exploration of memory interfaces will include:</p>\n<ul>\n<li><strong>DRAM Timing</strong>: RAS/CAS protocols, refresh scheduling, and timing parameters</li>\n<li><strong>Memory Bus Architectures</strong>: Parallel and serial interfaces (DDR, LPDDR, HBM)</li>\n<li><strong>DMA Controllers</strong>: Offloading memory transfers from the CPU</li>\n<li><strong>Memory-Mapped I/O</strong>: Interfacing with peripherals through memory addressing</li>\n<li><strong>Storage Controllers</strong>: Interfaces to persistent storage systems</li>\n</ul>\n<p>We\'ll implement memory controllers from digital logic, understanding the detailed timing requirements of various memory technologies and the complex trade-offs involved in modern memory system design. This hands-on approach reveals the challenges in bridging the speed gap between processors and memory while managing power, reliability, and cost constraints.</p>',
        parentId: 'hardware'
    },
    {
        id: 'io_systems',
        title: 'I/O Systems & Peripherals',
        description: 'Connecting computers to the outside world',
        content: '<h3>1. Fundamentals of I/O Interfaces</h3>\n<p>Input/Output interfaces form the boundary between a computer system and the external world. Understanding the principles of these interfaces is essential for designing systems that can effectively communicate with users and other devices.</p>\n<p>Key concepts in I/O interfaces include:</p>\n<ul>\n<li><strong>Signal Types and Levels</strong>: Digital vs. analog, voltage standards, and current loops</li>\n<li><strong>Data Transmission Modes</strong>: Serial, parallel, synchronous, and asynchronous</li>\n<li><strong>Addressing and Selection</strong>: Memory-mapped I/O vs. port-mapped I/O</li>\n<li><strong>Handshaking Protocols</strong>: Methods for coordinating data transfer between devices</li>\n<li><strong>Error Detection & Correction</strong>: Ensuring data integrity across interfaces</li>\n</ul>\n<p>We\'ll examine these concepts by designing and implementing basic I/O interfaces from discrete components, avoiding pre-built interface controllers and understanding how signals are physically transferred between digital systems.</p>\n\n<h3>2. Bus Architectures</h3>\n<p>Buses are the communication pathways that connect various components within a computer system. Understanding bus design involves balancing bandwidth, latency, cost, and compatibility requirements.</p>\n<p>Our exploration of bus architectures will include:</p>\n<ul>\n<li><strong>System Buses</strong>: Address, data, and control lines in computer architecture</li>\n<li><strong>Bus Topologies</strong>: Linear, star, hierarchical, and point-to-point connections</li>\n<li><strong>Arbitration Mechanisms</strong>: Resolving contention when multiple devices need access</li>\n<li><strong>Bus Timing</strong>: Synchronous vs. asynchronous protocols, wait states, and handshaking</li>\n<li><strong>Electrical Characteristics</strong>: Transmission line effects, termination, and signal integrity</li>\n</ul>\n<p>We\'ll implement simple but functional bus systems from basic digital components, understanding the timing diagrams, electrical requirements, and protocol specifications that govern reliable multi-device communication rather than using off-the-shelf bus controllers.</p>\n\n<h3>3. I/O Transaction Methods</h3>\n<p>Different methods for performing I/O transactions offer varying trade-offs between simplicity, efficiency, and CPU utilization. Understanding these methods is critical for designing systems with appropriate performance characteristics.</p>\n<p>We\'ll explore major I/O transaction methods:</p>\n<ul>\n<li><strong>Programmed I/O</strong>: CPU directly controlling data transfer through explicit instructions</li>\n<li><strong>Interrupt-Driven I/O</strong>: Devices signaling the CPU when they need service</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Peripheral-to-memory transfers without CPU intervention</li>\n<li><strong>Channel I/O</strong>: Specialized processors dedicated to managing I/O operations</li>\n<li><strong>Memory-Mapped I/O</strong>: Accessing device registers as memory locations</li>\n</ul>\n<p>For each method, we\'ll implement the necessary controllers and interface circuits from basic digital components, understanding the hardware-software interactions and measuring performance characteristics like throughput, latency, and CPU overhead.</p>\n\n<h3>4. Common Peripheral Interfaces</h3>\n<p>Modern computers support a wide range of peripherals through standardized interfaces. Understanding these interfaces involves analyzing their protocols, electrical specifications, and software interactions.</p>\n<p>We\'ll explore several key peripheral interfaces:</p>\n<ul>\n<li><strong>Serial Interfaces</strong>: UART, SPI, I2C, and USB implementation</li>\n<li><strong>Display Interfaces</strong>: VGA, DVI, HDMI, and DisplayPort protocols</li>\n<li><strong>Storage Interfaces</strong>: SATA, NVMe, SD, and storage protocol controllers</li>\n<li><strong>Network Interfaces</strong>: Ethernet, Wi-Fi, and basic physical layer implementation</li>\n<li><strong>Human Interface Devices</strong>: Keyboard, mouse, and game controller interfaces</li>\n</ul>\n<p>For each interface, we\'ll build simple controller circuits from digital logic components, understanding the precise timing requirements, protocol states, and physical layer considerations rather than using pre-built interface controllers.</p>\n\n<h3>5. I/O Software & Device Drivers</h3>\n<p>The hardware-software boundary in I/O systems requires carefully designed software that can efficiently manage devices while abstracting their complexities. This section bridges hardware design with systems programming.</p>\n<p>Key aspects of I/O software include:</p>\n<ul>\n<li><strong>Device Driver Architecture</strong>: Layered design and hardware abstraction</li>\n<li><strong>Polling vs. Interrupt Handling</strong>: Implementation strategies and trade-offs</li>\n<li><strong>Buffering Techniques</strong>: Single, double, and circular buffer implementations</li>\n<li><strong>Device Discovery & Configuration</strong>: Plug-and-play mechanisms and device enumeration</li>\n<li><strong>Error Handling & Recovery</strong>: Strategies for maintaining system reliability</li>\n</ul>\n<p>We\'ll implement basic device drivers for the peripheral controllers we\'ve built, writing software that directly manipulates hardware registers and responds to interrupts, avoiding high-level operating system APIs and understanding the complete hardware-software stack from first principles.</p>',
        parentId: 'hardware'
    },
    
    // Programming & Languages
    {
        id: 'programming',
        title: 'Programming & Languages',
        description: 'Programming concepts and language implementation',
        content: 'Programming is the process of creating instructions that tell a computer how to perform tasks. Rather than just learning to use existing programming languages, our "Build from Scratch" approach focuses on understanding how programming languages themselves work and are implemented. You\'ll learn to create your own languages, compilers, and interpreters, building a deep understanding of programming concepts from first principles.',
        parentId: null
    },
    {
        id: 'assembly',
        title: 'Assembly Language & Machine Code',
        description: 'The lowest level of programming',
        content: '<h3>1. Machine Code Fundamentals</h3>\n<p>Machine code is the raw binary instructions that a processor can directly execute. Understanding machine code requires knowledge of binary number systems, data representation, and processor architecture.</p>\n<p>Key aspects of machine code include:</p>\n<ul>\n<li><strong>Binary Representation</strong>: How instructions and data are encoded as bits</li>\n<li><strong>Instruction Formats</strong>: How operation codes, operands, and addressing modes are packed into binary</li>\n<li><strong>Endianness</strong>: Little-endian vs. big-endian byte ordering and its implications</li>\n<li><strong>Word Sizes</strong>: How the processor\'s bit width affects instruction and data formats</li>\n<li><strong>Instruction Pointer</strong>: How the processor keeps track of the next instruction to execute</li>\n</ul>\n<p>We\'ll explore these concepts by examining raw binary instructions, manually decoding them, and understanding how each bit influences processor behavior. Rather than using disassemblers or debuggers, we\'ll develop our own tools to visualize and manipulate machine code directly.</p>\n\n<h3>2. Assembly Language Syntax</h3>\n<p>Assembly language provides a human-readable representation of machine code, using mnemonics for instructions and symbolic names for memory locations. Understanding assembly syntax is the first step toward low-level programming.</p>\n<p>Our exploration of assembly syntax includes:</p>\n<ul>\n<li><strong>Instruction Mnemonics</strong>: Human-readable operation codes (e.g., MOV, ADD, JMP)</li>\n<li><strong>Operand Syntax</strong>: Register references, immediate values, and memory addressing modes</li>\n<li><strong>Directives</strong>: Special instructions to the assembler (e.g., for defining data, sections)</li>\n<li><strong>Labels</strong>: Symbolic names for memory locations and program branches</li>\n<li><strong>Comments and Documentation</strong>: Practices for making assembly code understandable</li>\n</ul>\n<p>We\'ll study multiple assembly language dialects, understanding both CISC and RISC assembly conventions, writing programs by hand rather than generating them from higher-level languages, and directly mapping the relationship between assembly instructions and the resulting machine code.</p>\n\n<h3>3. Programming in Assembly</h3>\n<p>Effective assembly programming requires understanding processor-specific instructions, registers, and memory management, as well as developing strategies for implementing higher-level constructs with low-level operations.</p>\n<p>Key assembly programming topics include:</p>\n<ul>\n<li><strong>Control Flow</strong>: Implementing conditionals, loops, and function calls</li>\n<li><strong>Data Structures</strong>: Arrays, records, stacks, and linked lists in assembly</li>\n<li><strong>Memory Management</strong>: Stack frames, heap allocation, and addressing modes</li>\n<li><strong>Bit Manipulation</strong>: Masking, shifting, and bitwise operations for efficient algorithms</li>\n<li><strong>Optimization Techniques</strong>: Instruction selection, register allocation, and pipeline awareness</li>\n</ul>\n<p>We\'ll develop complete programs in assembly language, implementing classic algorithms, data structures, and system utilities without relying on libraries or operating system services, gaining a deep understanding of how software interacts directly with hardware.</p>\n\n<h3>4. Building an Assembler</h3>\n<p>An assembler translates assembly language into machine code. By building an assembler from scratch, we gain insight into both language processing and the detailed structure of machine instructions.</p>\n<p>Our assembler implementation will cover:</p>\n<ul>\n<li><strong>Lexical Analysis</strong>: Breaking assembly source into tokens (mnemonics, registers, values)</li>\n<li><strong>Symbol Table Management</strong>: Tracking labels and their memory addresses</li>\n<li><strong>Two-Pass Assembly</strong>: Resolving forward references and calculating offsets</li>\n<li><strong>Instruction Encoding</strong>: Converting mnemonics and operands to binary opcodes</li>\n<li><strong>Output Formats</strong>: Generating executable binary files (e.g., ELF, flat binary)</li>\n</ul>\n<p>We\'ll implement a complete assembler in a high-level language, avoiding existing assembler tools or parser generators, understanding every step of the translation process and the challenges of mapping human-readable code to efficient machine instructions.</p>\n\n<h3>5. Interfacing with Hardware</h3>\n<p>Assembly language provides direct access to hardware features that are often inaccessible from higher-level languages. This capability is essential for systems programming and device control.</p>\n<p>Key aspects of hardware interfacing include:</p>\n<ul>\n<li><strong>I/O Port Access</strong>: Reading from and writing to hardware ports</li>\n<li><strong>Memory-Mapped I/O</strong>: Controlling devices through special memory addresses</li>\n<li><strong>Interrupt Handling</strong>: Writing interrupt service routines</li>\n<li><strong>DMA Control</strong>: Setting up direct memory access for efficient data transfer</li>\n<li><strong>Privileged Instructions</strong>: Managing processor modes and protection features</li>\n</ul>\n<p>We\'ll write assembly programs that directly control hardware components like displays, keyboards, and storage devices, without using operating system services or device drivers, understanding the full stack from processor instructions to physical devices.</p>',
        parentId: 'programming'
    },
    {
        id: 'language_design',
        title: 'Programming Language Design',
        description: 'Creating your own programming language',
        content: '<h3>1. Fundamental Language Concepts</h3>\n<p>Before designing a programming language, it\'s essential to understand the core concepts that underlie all languages. These fundamental principles inform the major design decisions and tradeoffs in language creation.</p>\n<p>Key language concepts include:</p>\n<ul>\n<li><strong>Syntax vs. Semantics</strong>: The distinction between how programs are written and what they mean</li>\n<li><strong>Binding and Scope</strong>: How names are associated with values and their visibility</li>\n<li><strong>Abstraction Mechanisms</strong>: Ways to hide implementation details (functions, classes, modules)</li>\n<li><strong>Evaluation Strategies</strong>: Eager vs. lazy, strict vs. non-strict evaluation</li>\n<li><strong>Memory Management Models</strong>: Manual, reference counting, garbage collection approaches</li>\n</ul>\n<p>We\'ll explore these concepts by analyzing existing languages, understanding their design decisions, and considering alternative approaches. This foundation will prepare us to make informed choices in our own language design rather than simply copying existing features.</p>\n\n<h3>2. Programming Paradigms</h3>\n<p>Programming paradigms represent fundamentally different approaches to structuring and organizing code. Understanding these paradigms is crucial for designing languages that effectively support specific styles of programming.</p>\n<p>We\'ll explore major paradigms including:</p>\n<ul>\n<li><strong>Imperative Programming</strong>: Step-by-step execution with mutable state (procedural, object-oriented)</li>\n<li><strong>Declarative Programming</strong>: Expressing logic without control flow (functional, logic-based)</li>\n<li><strong>Object-Oriented Programming</strong>: Encapsulating state and behavior in objects and classes</li>\n<li><strong>Functional Programming</strong>: Computing with pure functions and immutable data</li>\n<li><strong>Logic Programming</strong>: Specifying rules and relationships rather than algorithms</li>\n</ul>\n<p>For each paradigm, we\'ll implement small language prototypes that embody its core principles, understanding the strengths, weaknesses, and implementation challenges of each approach. This hands-on experience with multiple paradigms will inform the design of our own language, which might combine elements from different paradigms.</p>\n\n<h3>3. Syntax Design & Parsing</h3>\n<p>A language\'s syntax defines its concrete representation as text and significantly impacts readability, writability, and learnability. Well-designed syntax balances these factors while supporting the language\'s semantic model.</p>\n<p>Our syntax design process will cover:</p>\n<ul>\n<li><strong>Grammar Specification</strong>: Formal definition using BNF, EBNF, or similar notations</li>\n<li><strong>Lexical Structure</strong>: Tokens, identifiers, keywords, and literal values</li>\n<li><strong>Expression Syntax</strong>: Infix, prefix, postfix notations and precedence rules</li>\n<li><strong>Statement and Declaration Syntax</strong>: Block structure, termination, and nesting</li>\n<li><strong>Syntactic Sugar</strong>: Convenient notation that simplifies common patterns</li>\n</ul>\n<p>We\'ll design a syntax for our language from first principles, evaluating each decision against usability criteria rather than simply adopting conventions from existing languages. We\'ll also build parsers for our grammar, understanding both the theory of parsing and practical implementation techniques.</p>\n\n<h3>4. Type Systems & Semantic Analysis</h3>\n<p>Type systems provide a framework for classifying values and expressions, enabling compile-time verification of program properties. A well-designed type system balances safety, expressiveness, and simplicity.</p>\n<p>Our exploration of type systems will include:</p>\n<ul>\n<li><strong>Static vs. Dynamic Typing</strong>: Compile-time vs. run-time type checking trade-offs</li>\n<li><strong>Structural vs. Nominal Types</strong>: Classification based on structure or explicit declaration</li>\n<li><strong>Type Inference</strong>: Algorithms for deducing types without explicit annotations</li>\n<li><strong>Polymorphism</strong>: Parametric, subtype, and ad-hoc approaches to code reuse</li>\n<li><strong>Type Safety and Soundness</strong>: Ensuring program correctness through the type system</li>\n</ul>\n<p>We\'ll implement type checkers for different type systems, understanding their algorithmic foundations and how they enforce language semantics. This implementation experience will inform the design of our language\'s type system, ensuring it meets our goals for safety, expressiveness, and performance.</p>\n\n<h3>5. Language Implementation Strategies</h3>\n<p>How a language is implemented affects its performance characteristics, portability, and ease of development. Different implementation approaches offer various trade-offs that influence language design decisions.</p>\n<p>We\'ll explore several implementation strategies:</p>\n<ul>\n<li><strong>Interpreters</strong>: Direct execution of source or intermediate representations</li>\n<li><strong>Compilers</strong>: Translation to other languages or native machine code</li>\n<li><strong>Just-In-Time Compilation</strong>: Dynamic translation during program execution</li>\n<li><strong>Virtual Machines</strong>: Abstract execution environments with portable bytecode</li>\n<li><strong>Runtime Systems</strong>: Supporting libraries and services for executing programs</li>\n</ul>\n<p>We\'ll implement simple versions of different execution models, understanding their performance characteristics, implementation complexity, and suitability for different language features. This experience will prepare us to choose and develop an appropriate implementation strategy for our own language, ensuring our design decisions are practically implementable.</p>',
        parentId: 'programming'
    },
    {
        id: 'compiler_construction',
        title: 'Compiler & Interpreter Construction',
        description: 'Building tools that translate and execute code',
        content: '<h3>1. Compiler Architecture & Design</h3>\n<p>Compilers and interpreters are complex software systems that transform source code into executable forms. Understanding their overall architecture is essential for building these tools from scratch.</p>\n<p>Key architectural concepts include:</p>\n<ul>\n<li><strong>Compiler Phases</strong>: Front-end, middle-end, and back-end components and their interfaces</li>\n<li><strong>Intermediate Representations</strong>: Abstract syntax trees, three-address code, and static single assignment form</li>\n<li><strong>Symbol Tables</strong>: Data structures for managing identifiers and their attributes</li>\n<li><strong>Error Handling</strong>: Detection, reporting, and recovery strategies throughout compilation</li>\n<li><strong>Compilation Models</strong>: Single-pass vs. multi-pass, batch vs. incremental, just-in-time approaches</li>\n</ul>\n<p>We\'ll develop a modular compiler architecture from first principles, designing each component with clear interfaces and responsibilities, while avoiding existing compiler frameworks that hide these design decisions behind high-level abstractions.</p>\n\n<h3>2. Lexical Analysis & Scanning</h3>\n<p>Lexical analysis is the first phase of compilation, converting raw source text into a stream of tokens. A well-designed scanner balances correctness, error handling, and performance.</p>\n<p>Our implementation of lexical analysis will cover:</p>\n<ul>\n<li><strong>Regular Expressions</strong>: Formal notation for defining lexical patterns</li>\n<li><strong>Finite Automata</strong>: DFAs and NFAs for implementing efficient scanners</li>\n<li><strong>Scanner Construction</strong>: Manual and automated approaches to scanner implementation</li>\n<li><strong>Lexical Errors</strong>: Detection and recovery from invalid input characters</li>\n<li><strong>Lexical Conventions</strong>: Handling whitespace, comments, and language-specific rules</li>\n</ul>\n<p>We\'ll implement scanners using multiple techniques, including hand-written code and automata-based algorithms, while avoiding existing scanner generators. This approach provides deep insight into the trade-offs between different implementation strategies and the theory that underpins them.</p>\n\n<h3>3. Parsing & Syntax Analysis</h3>\n<p>Syntax analysis structures a token stream into a parse tree or abstract syntax tree according to a grammar. This phase checks that programs follow the language\'s syntactic rules and prepares them for semantic analysis.</p>\n<p>Our exploration of parsing will include:</p>\n<ul>\n<li><strong>Context-Free Grammars</strong>: Formal descriptions of language syntax</li>\n<li><strong>Top-Down Parsing</strong>: Recursive descent, LL(k), and predictive parsing techniques</li>\n<li><strong>Bottom-Up Parsing</strong>: Shift-reduce, LR, LALR, and precedence parsing approaches</li>\n<li><strong>Abstract Syntax Trees</strong>: Efficient representations of program structure</li>\n<li><strong>Error Recovery</strong>: Techniques to continue parsing after syntax errors</li>\n</ul>\n<p>We\'ll implement multiple parsers for our language, comparing different approaches and understanding their strengths and limitations. By building these parsers from scratch rather than using parser generators, we\'ll gain insight into the algorithms and data structures that drive syntax analysis.</p>\n\n<h3>4. Semantic Analysis & Type Checking</h3>\n<p>Semantic analysis verifies that programs obey the language\'s semantic rules, particularly type constraints. This phase ensures that operations are applied to appropriate operands and prepares for code generation.</p>\n<p>Key semantic analysis topics include:</p>\n<ul>\n<li><strong>Name Resolution</strong>: Connecting identifier uses with their declarations</li>\n<li><strong>Type Checking</strong>: Verifying type compatibility in expressions and statements</li>\n<li><strong>Type Inference</strong>: Deducing types without explicit annotations</li>\n<li><strong>Semantic Errors</strong>: Detecting and reporting violations of language rules</li>\n<li><strong>Static Analysis</strong>: Flow analysis, escape analysis, and other program properties</li>\n</ul>\n<p>We\'ll implement a complete semantic analyzer for our language, checking all language-specific rules and providing meaningful error messages. This implementation will avoid libraries or frameworks that hide the details of type checking, instead building understanding from first principles.</p>\n\n<h3>5. Code Generation & Execution</h3>\n<p>Code generation transforms a verified program representation into executable form, whether machine code, bytecode, or another target language. This phase determines the performance and behavior of compiled programs.</p>\n<p>Our code generation implementation will cover:</p>\n<ul>\n<li><strong>Target Architecture Selection</strong>: Native code, virtual machines, or translation targets</li>\n<li><strong>Instruction Selection</strong>: Mapping language operations to target instructions</li>\n<li><strong>Register Allocation</strong>: Efficiently assigning variables to registers</li>\n<li><strong>Memory Layout</strong>: Organizing data structures in memory</li>\n<li><strong>Runtime System Integration</strong>: Memory management, I/O, and other services</li>\n</ul>\n<p>We\'ll implement code generators for multiple targets, such as a custom virtual machine, x86 assembly, or WebAssembly, understanding the specific challenges of each. For interpreted languages, we\'ll build evaluators that directly execute abstract syntax trees or bytecode. This approach reveals the fundamental techniques of efficient code generation without relying on existing back-end frameworks.</p>',
        parentId: 'programming'
    },
    {
        id: 'systems_programming',
        title: 'Systems Programming',
        description: 'Creating software that interfaces with hardware',
        content: '<h3>1. Hardware Abstraction Layers</h3>\n<p>Hardware abstraction layers (HALs) provide a consistent interface to diverse hardware components, enabling portable and maintainable systems software. Creating effective abstractions requires understanding both hardware details and software design principles.</p>\n<p>Key aspects of hardware abstraction include:</p>\n<ul>\n<li><strong>Processor Abstraction</strong>: Managing CPU-specific features and instruction sets</li>\n<li><strong>Memory Management Units</strong>: Abstracting physical memory access and protection</li>\n<li><strong>Device Register Mapping</strong>: Creating software interfaces to hardware registers</li>\n<li><strong>Interrupt Controllers</strong>: Providing uniform access to diverse interrupt hardware</li>\n<li><strong>Platform-Specific Optimization</strong>: Balancing portability with performance</li>\n</ul>\n<p>We\'ll implement HALs for multiple hardware platforms, developing interfaces that hide hardware complexity while preserving essential capabilities. Rather than using existing HALs, we\'ll build our own from the hardware registers up, understanding the trade-offs between portability, performance, and maintainability.</p>\n\n<h3>2. Device Driver Development</h3>\n<p>Device drivers bridge the gap between hardware devices and software applications, translating abstract operations into device-specific commands. Effective driver development requires deep understanding of both hardware protocols and software architecture.</p>\n<p>Our exploration of driver development will include:</p>\n<ul>\n<li><strong>Character Devices</strong>: Implementing drivers for serial ports, keyboards, and similar devices</li>\n<li><strong>Block Devices</strong>: Creating drivers for storage systems with block-based access</li>\n<li><strong>Network Interfaces</strong>: Developing drivers for Ethernet and wireless devices</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Managing high-performance data transfers</li>\n<li><strong>Power Management</strong>: Implementing device sleep, hibernation, and wake-up</li>\n</ul>\n<p>We\'ll write complete device drivers from scratch for common hardware components, developing both the hardware-facing code and the software interfaces. This approach avoids using existing driver frameworks, instead building understanding of the entire driver stack from registers and interrupts to system calls.</p>\n\n<h3>3. Runtime Libraries & System Services</h3>\n<p>Runtime libraries provide essential services to applications, handling tasks like memory allocation, I/O, and process management. Building these libraries requires understanding both hardware capabilities and application needs.</p>\n<p>Key runtime components we\'ll implement include:</p>\n<ul>\n<li><strong>Memory Allocators</strong>: Building malloc, free, and garbage collectors from scratch</li>\n<li><strong>I/O Libraries</strong>: Implementing buffered I/O, formatting, and error handling</li>\n<li><strong>Thread Support</strong>: Creating thread creation, scheduling, and synchronization primitives</li>\n<li><strong>Time Services</strong>: Building timers, delays, and real-time scheduling support</li>\n<li><strong>Error Handling</strong>: Implementing exceptions, signals, and other error mechanisms</li>\n</ul>\n<p>We\'ll build complete runtime libraries from scratch, without using existing implementations like libc. This approach reveals the fundamental techniques required to support application execution, from low-level memory management to high-level abstractions like formatted I/O.</p>\n\n<h3>4. Embedded Systems Development</h3>\n<p>Embedded systems combine hardware and software to perform specific functions, often with constrained resources and real-time requirements. Programming these systems demands specialized techniques and careful resource management.</p>\n<p>Our embedded systems exploration will cover:</p>\n<ul>\n<li><strong>Bare-Metal Programming</strong>: Writing code that runs without an operating system</li>\n<li><strong>Real-Time Constraints</strong>: Meeting timing requirements and deadlines</li>\n<li><strong>Resource Optimization</strong>: Minimizing memory usage and power consumption</li>\n<li><strong>Firmware Development</strong>: Creating persistent code that runs on device boot</li>\n<li><strong>Hardware/Software Co-design</strong>: Balancing functionality between hardware and software</li>\n</ul>\n<p>We\'ll develop complete embedded applications for microcontroller platforms, writing code that runs directly on the hardware without operating system support. This approach builds understanding of the unique challenges in embedded development, from interrupt-driven programming to resource-constrained optimization.</p>\n\n<h3>5. Systems Programming Tools</h3>\n<p>Effective systems programming requires specialized tools for development, debugging, testing, and analysis. Understanding these toolsand sometimes building themis essential for creating reliable systems software.</p>\n<p>Key systems programming tools include:</p>\n<ul>\n<li><strong>Linkers and Loaders</strong>: Understanding and implementing the program loading process</li>\n<li><strong>Debuggers</strong>: Building tools to inspect running systems at the hardware level</li>\n<li><strong>Binary Analysis</strong>: Creating tools to analyze and manipulate compiled code</li>\n<li><strong>Profilers and Tracers</strong>: Developing performance analysis infrastructure</li>\n<li><strong>Cross-Development Environments</strong>: Setting up build systems that target different hardware</li>\n</ul>\n<p>We\'ll implement simplified versions of essential systems programming tools, understanding their internal operation rather than just using existing ones. This approach builds insight into the entire systems development lifecycle, from code creation to deployment and analysis, revealing the foundational technologies that make modern computing possible.</p>',
        parentId: 'programming'
    },
    
    // Operating Systems
    {
        id: 'systems',
        title: 'Operating Systems',
        description: 'Building computer operating systems from first principles',
        content: 'Operating systems are the fundamental software that manages computer hardware and provides common services for computer programs. Our "Build from Scratch" approach teaches you to design and implement every component of an operating system yourself, without relying on existing implementations or high-level abstractions. You\'ll learn how to create a functional OS from basic principles, understanding every layer from hardware interaction to user interfaces.',
        parentId: null
    },
    {
        id: 'boot_process',
        title: 'Boot Process & Kernel Initialization',
        description: 'How computers start up and load the operating system',
        content: '<h3>1. Firmware and Early Boot Sequence</h3>\n<p>When a computer is powered on, it begins executing firmware code stored in non-volatile memory. This firmware initializes the hardware and prepares the system for loading an operating system. Understanding this process is essential for developing low-level system software.</p>\n<p>Key aspects of the early boot sequence include:</p>\n<ul>\n<li><strong>BIOS vs. UEFI</strong>: Traditional and modern firmware approaches to system initialization</li>\n<li><strong>Power-On Self Test (POST)</strong>: Hardware verification and initialization sequence</li>\n<li><strong>Boot Device Selection</strong>: How firmware locates and loads the bootloader</li>\n<li><strong>Memory Detection</strong>: Discovering and initializing RAM before the OS is loaded</li>\n<li><strong>Firmware Services</strong>: Interfaces provided to bootloaders and early operating systems</li>\n</ul>\n<p>We\'ll explore firmware internals by examining firmware implementations and creating minimal test environments that simulate the early boot process. This approach provides insight into the critical but often invisible first steps of system startup, without relying on black-box firmware implementations.</p>\n\n<h3>2. Bootloader Design & Implementation</h3>\n<p>Bootloaders bridge the gap between firmware and the operating system kernel, loading the kernel into memory and preparing the environment for its execution. Implementing a bootloader requires understanding both hardware constraints and OS requirements.</p>\n<p>Our bootloader implementation will cover:</p>\n<ul>\n<li><strong>Multi-Stage Loading</strong>: Breaking the boot process into size-constrained stages</li>\n<li><strong>Disk Access</strong>: Reading kernel code from storage without OS services</li>\n<li><strong>Boot Protocols</strong>: Communication between bootloader and kernel</li>\n<li><strong>Memory Maps</strong>: Providing the kernel with information about available memory</li>\n<li><strong>Boot Configuration</strong>: Passing parameters and options to the kernel</li>\n</ul>\n<p>We\'ll implement a complete bootloader from scratch, capable of loading a simple kernel from disk to memory. Rather than using existing bootloaders like GRUB, we\'ll write our own assembly and C code, understanding every step from the initial firmware handoff to the final jump to kernel code.</p>\n\n<h3>3. Processor Mode Transitions</h3>\n<p>Modern processors support different operational modes with varying capabilities and protection levels. Transitioning between these modes is a critical part of the boot process, requiring careful management of processor state and memory structures.</p>\n<p>Key mode transitions include:</p>\n<ul>\n<li><strong>Real Mode to Protected Mode</strong>: Enabling memory protection and 32-bit operation</li>\n<li><strong>Protected Mode to Long Mode</strong>: Transitioning to 64-bit operation (on x86-64)</li>\n<li><strong>Global Descriptor Tables</strong>: Setting up memory segmentation structures</li>\n<li><strong>Paging Initialization</strong>: Establishing virtual memory translation structures</li>\n<li><strong>Privilege Levels</strong>: Configuring CPU protection rings for kernel and user code</li>\n</ul>\n<p>We\'ll implement these transitions step by step, writing code that properly configures processor registers, memory structures, and control flags. This hands-on approach reveals the intricate details of processor architecture and the foundations of system security and isolation.</p>\n\n<h3>4. Early Hardware Initialization</h3>\n<p>Before a kernel can provide full functionality, it must initialize critical hardware components and establish basic services. This early initialization occurs in a minimal environment without the support of many kernel subsystems.</p>\n<p>Our exploration of early hardware initialization will include:</p>\n<ul>\n<li><strong>Interrupt Controller Setup</strong>: Configuring interrupt handling for system events</li>\n<li><strong>Timer Initialization</strong>: Establishing system timing and scheduling capabilities</li>\n<li><strong>Memory Controller Configuration</strong>: Optimizing memory access patterns and caching</li>\n<li><strong>Basic I/O Setup</strong>: Initializing essential input/output devices for early debug output</li>\n<li><strong>Multiprocessor Initialization</strong>: Bringing secondary CPU cores online</li>\n</ul>\n<p>We\'ll implement initialization routines for these critical hardware components, developing a deep understanding of hardware interfaces and the delicate sequence of operations required to bring a system to life. This implementation will avoid using existing drivers or initialization code, instead developing the minimum necessary code from hardware documentation.</p>\n\n<h3>5. Kernel Environment Preparation</h3>\n<p>Before the main kernel code can execute, the boot process must establish a suitable execution environment, including memory layouts, data structures, and essential services. This preparation bridges the gap between the bare hardware and the structured world of the kernel.</p>\n<p>Key aspects of kernel environment preparation include:</p>\n<ul>\n<li><strong>Memory Layout Establishment</strong>: Setting up kernel and user address spaces</li>\n<li><strong>C Environment Initialization</strong>: Preparing for high-level language execution</li>\n<li><strong>Core Data Structures</strong>: Creating essential kernel management structures</li>\n<li><strong>Bootstrap Memory Allocation</strong>: Implementing early memory management before the full MM system</li>\n<li><strong>Debug and Logging Services</strong>: Establishing early diagnostic capabilities</li>\n</ul>\n<p>We\'ll implement the complete sequence of environment preparation, from the first instructions executed in kernel space to the point where high-level kernel initialization can begin. This approach provides insight into the hidden foundations that support the rest of the operating system, revealing the careful orchestration required to transform bare hardware into a platform ready for complex software execution.</p>',
        parentId: 'systems'
    },
    {
        id: 'process_management',
        title: 'Process Management',
        description: 'Creating and controlling execution units',
        content: '<h3>1. Process Abstraction & Representation</h3>\n<p>Processes are the fundamental execution units in operating systems, abstracting running programs from the underlying hardware. Effectively representing processes is essential for managing and controlling their execution.</p>\n<p>Key aspects of process abstraction include:</p>\n<ul>\n<li><strong>Process Control Blocks</strong>: Data structures for tracking process state and resources</li>\n<li><strong>Process Address Space</strong>: Memory organization for code, data, stack, and heap</li>\n<li><strong>Process Lifecycle</strong>: States from creation to termination and transitions between them</li>\n<li><strong>Process Hierarchy</strong>: Parent-child relationships and inheritance of attributes</li>\n<li><strong>Process Identity</strong>: Naming, identification, and security attributes</li>\n</ul>\n<p>We\'ll implement a complete process management infrastructure from scratch, developing data structures that can track all aspects of a running process. Rather than using existing process management code, we\'ll build our own abstractions, gaining insight into the fundamental building blocks of multitasking operating systems.</p>\n\n<h3>2. Context Switching & Dispatching</h3>\n<p>Context switching is the mechanism that allows multiple processes to share CPU resources, giving the illusion of parallel execution. This complex operation requires careful preservation and restoration of processor state.</p>\n<p>Our context switching implementation will cover:</p>\n<ul>\n<li><strong>Register State Saving</strong>: Preserving CPU registers during task switches</li>\n<li><strong>Memory Context Management</strong>: Switching address spaces and translation tables</li>\n<li><strong>CPU Mode Transitions</strong>: Managing privileged operations during context switches</li>\n<li><strong>Execution Stack Switching</strong>: Managing per-process kernel and user stacks</li>\n<li><strong>Context Switch Optimization</strong>: Minimizing the performance impact of task switches</li>\n</ul>\n<p>We\'ll implement context switching in assembly language for specific processor architectures, understanding the hardware-specific details and optimizations. This hands-on approach reveals the intricate dance required to juggle multiple execution contexts on a single processor, without relying on existing context switching code.</p>\n\n<h3>3. Scheduling Algorithms & Policies</h3>\n<p>Schedulers determine which process runs next and for how long, balancing responsiveness, fairness, and efficiency. Effective scheduling is critical for system performance and requires both algorithmic understanding and practical implementation skills.</p>\n<p>Our exploration of scheduling will include:</p>\n<ul>\n<li><strong>Preemptive vs. Cooperative Scheduling</strong>: Different approaches to allocating CPU time</li>\n<li><strong>Priority-Based Algorithms</strong>: Assigning and managing execution priorities</li>\n<li><strong>Round-Robin and Time-Slicing</strong>: Fairly sharing CPU time among processes</li>\n<li><strong>Real-Time Scheduling</strong>: Meeting timing constraints for time-critical tasks</li>\n<li><strong>Multiprocessor Scheduling</strong>: Distributing processes across multiple CPU cores</li>\n</ul>\n<p>We\'ll implement multiple scheduling algorithms from scratch, comparing their behavior and performance characteristics. By building a pluggable scheduler framework, we\'ll experience the tradeoffs between different scheduling policies firsthand, rather than just using existing schedulers or simulating their behavior.</p>\n\n<h3>4. Thread Implementation & Management</h3>\n<p>Threads provide lightweight concurrency within processes, sharing an address space while executing independently. Implementing threads requires careful coordination of shared resources and execution state.</p>\n<p>Key thread implementation topics include:</p>\n<ul>\n<li><strong>User vs. Kernel Threads</strong>: Different models for thread implementation</li>\n<li><strong>Thread Control Blocks</strong>: Lightweight process-like structures for threads</li>\n<li><strong>Thread Local Storage</strong>: Per-thread data in a shared address space</li>\n<li><strong>Thread Scheduling</strong>: Balancing execution among threads within processes</li>\n<li><strong>Thread Creation and Termination</strong>: Efficiently spawning and cleaning up threads</li>\n</ul>\n<p>We\'ll implement both user-level and kernel-level threading systems from scratch, understanding the advantages and tradeoffs of each approach. This implementation will demonstrate how threads can provide concurrency while minimizing the overhead associated with full processes.</p>\n\n<h3>5. Synchronization & Communication</h3>\n<p>Processes and threads must coordinate their actions when sharing resources or exchanging information. Effective synchronization and communication mechanisms are essential for correct concurrent operation.</p>\n<p>Our implementation of synchronization will cover:</p>\n<ul>\n<li><strong>Mutual Exclusion Primitives</strong>: Locks, mutexes, and critical sections</li>\n<li><strong>Condition Synchronization</strong>: Semaphores, condition variables, and barriers</li>\n<li><strong>Message Passing</strong>: Inter-process communication via structured messages</li>\n<li><strong>Shared Memory</strong>: Directly sharing memory regions between processes</li>\n<li><strong>Deadlock Prevention</strong>: Detecting and avoiding synchronization deadlocks</li>\n</ul>\n<p>We\'ll implement these mechanisms from first principles, building synchronization primitives at both the hardware and software levels. Rather than using existing libraries, we\'ll develop a complete suite of synchronization tools, understanding the algorithms and hardware features that make safe concurrent execution possible.</p>',
        parentId: 'systems'
    },
    {
        id: 'memory_management',
        title: 'Memory Management',
        description: 'Efficiently allocating and protecting memory resources',
        content: '<h3>1. Physical Memory Management</h3>\n<p>Physical memory is the actual RAM hardware in a computer system. Managing this resource effectively requires tracking available memory, allocating it to processes, and reclaiming it when no longer needed.</p>\n<p>Key aspects of physical memory management include:</p>\n<ul>\n<li><strong>Memory Detection</strong>: Discovering and mapping available physical memory at boot time</li>\n<li><strong>Memory Partitioning</strong>: Organizing physical memory into manageable regions</li>\n<li><strong>Physical Allocation Algorithms</strong>: First-fit, best-fit, buddy system approaches</li>\n<li><strong>Physical Address Alignment</strong>: Meeting hardware constraints for memory access</li>\n<li><strong>Fragmentation Management</strong>: Strategies to combat memory fragmentation over time</li>\n</ul>\n<p>We\'ll implement a physical memory manager from scratch, developing data structures to track memory regions and algorithms for efficient allocation. This implementation will avoid using existing memory management libraries, instead building understanding from the hardware interface upward.</p>\n\n<h3>2. Virtual Memory Systems</h3>\n<p>Virtual memory creates an abstraction layer between logical addresses used by processes and physical addresses in RAM. This abstraction enables memory protection, overcommitment, and simplifies application development.</p>\n<p>Our virtual memory implementation will cover:</p>\n<ul>\n<li><strong>Address Translation</strong>: Mapping virtual addresses to physical locations</li>\n<li><strong>Page Tables</strong>: Data structures for efficient address mapping</li>\n<li><strong>Translation Lookaside Buffers (TLBs)</strong>: Hardware caching of address translations</li>\n<li><strong>Demand Paging</strong>: Loading memory contents only when needed</li>\n<li><strong>Memory-Mapped Files</strong>: Accessing files through the virtual memory system</li>\n</ul>\n<p>We\'ll build a complete virtual memory system from first principles, including page table structures optimized for different architectures. Rather than using existing virtual memory implementations, we\'ll develop our own system that interacts directly with both the MMU hardware and the physical memory manager.</p>\n\n<h3>3. Paging & Segmentation</h3>\n<p>Paging and segmentation are two approaches to implementing virtual memory, each with distinct characteristics and trade-offs. Understanding both approaches provides insight into memory system design evolution.</p>\n<p>Key aspects of paging and segmentation include:</p>\n<ul>\n<li><strong>Page Tables</strong>: Single-level, multi-level, and inverted page table designs</li>\n<li><strong>Segment Descriptors</strong>: Defining variable-sized memory regions</li>\n<li><strong>Protection Bits</strong>: Controlling read, write, and execute permissions</li>\n<li><strong>Hybrid Approaches</strong>: Combining paging and segmentation benefits</li>\n<li><strong>Hardware Support</strong>: Processor features that accelerate virtual memory</li>\n</ul>\n<p>We\'ll implement both paging and segmentation systems from scratch, comparing their performance, complexity, and memory efficiency. This hands-on experience with multiple memory organization approaches builds a deeper understanding of virtual memory concepts than studying a single implementation.</p>\n\n<h3>4. Memory Allocation & Heap Management</h3>\n<p>Memory allocators provide controlled access to memory resources, managing the details of finding and reserving memory blocks of appropriate sizes. Efficient allocators balance speed, memory utilization, and fragmentation resistance.</p>\n<p>Our allocator implementation will include:</p>\n<ul>\n<li><strong>Kernel Allocators</strong>: Memory management for operating system needs</li>\n<li><strong>User-Space Allocators</strong>: Implementing malloc, free, and related functions</li>\n<li><strong>Memory Pools</strong>: Optimized allocation for fixed-size objects</li>\n<li><strong>Garbage Collection</strong>: Automatic memory reclamation techniques</li>\n<li><strong>Memory Debugging</strong>: Tools for detecting leaks and corruption</li>\n</ul>\n<p>We\'ll build multiple memory allocators from scratch, evaluating their performance characteristics and suitability for different workloads. Rather than using existing allocation libraries, we\'ll implement our own algorithms, gaining insight into the complex trade-offs involved in heap management.</p>\n\n<h3>5. Swapping & Memory Compression</h3>\n<p>When physical memory is exhausted, systems can extend available memory by using disk space or compression techniques. These approaches trade performance for capacity, enabling systems to handle larger workloads than would fit in RAM.</p>\n<p>Key aspects of memory extension include:</p>\n<ul>\n<li><strong>Page Replacement Algorithms</strong>: Selecting which pages to evict (LRU, Clock, etc.)</li>\n<li><strong>Swap Space Management</strong>: Organizing and accessing disk-based memory extensions</li>\n<li><strong>Memory Compression</strong>: Trading CPU time for memory capacity</li>\n<li><strong>Prefetching and Anticipation</strong>: Reducing the performance impact of paging</li>\n<li><strong>Out-of-Memory Management</strong>: Handling resource exhaustion gracefully</li>\n</ul>\n<p>We\'ll implement page replacement algorithms and swapping systems from first principles, including the policy decisions, data structures, and I/O operations involved. This implementation will provide insight into how modern operating systems balance memory capacity and performance constraints, managing limited physical resources to support demanding workloads.</p>',
        parentId: 'systems'
    },
    {
        id: 'filesystem',
        title: 'File Systems',
        description: 'Organizing and storing persistent data',
        content: '<h3>1. File System Architecture & Design</h3>\n<p>File systems organize and manage persistent storage, providing a stable, hierarchical interface to an otherwise raw block device. Designing effective file systems requires balancing performance, reliability, and usability considerations.</p>\n<p>Key aspects of file system architecture include:</p>\n<ul>\n<li><strong>Overall Structure</strong>: On-disk layout, partitioning, and volume management</li>\n<li><strong>Disk Block Management</strong>: Managing physical storage units and addressing</li>\n<li><strong>Metadata Organization</strong>: Strategies for storing file and directory information</li>\n<li><strong>File System Interfaces</strong>: System calls and operations for accessing storage</li>\n<li><strong>Performance Optimizations</strong>: Caching, prefetching, and layout strategies</li>\n</ul>\n<p>We\'ll develop a file system architecture from first principles, designing the on-disk structures and in-memory representations needed for efficient storage. Rather than using existing file system implementations, we\'ll build our own from scratch, gaining insight into the fundamental trade-offs in storage system design.</p>\n\n<h3>2. Block Allocation & Management</h3>\n<p>Block allocation is how file systems assign disk space to files. The choice of allocation strategy significantly impacts performance, space efficiency, and resistance to fragmentation over time.</p>\n<p>Our exploration of block allocation will include:</p>\n<ul>\n<li><strong>Contiguous Allocation</strong>: Storing files in sequential blocks</li>\n<li><strong>Linked Allocation</strong>: Using pointers to connect blocks belonging to a file</li>\n<li><strong>Indexed Allocation</strong>: Tracking blocks through index structures (inodes)</li>\n<li><strong>Extent-Based Allocation</strong>: Managing ranges of contiguous blocks</li>\n<li><strong>Free Space Management</strong>: Bitmap, linked list, and other free block tracking methods</li>\n</ul>\n<p>We\'ll implement multiple allocation strategies from scratch, comparing their performance in different scenarios. This hands-on approach provides deeper understanding than simply studying existing file systems, revealing how allocation decisions affect file system behavior over time.</p>\n\n<h3>3. Directory Organization & Name Resolution</h3>\n<p>Directories provide the hierarchical structure and naming system that makes file systems navigable by humans. Effective directory implementation balances lookup speed, modification efficiency, and robustness.</p>\n<p>Key aspects of directory implementation include:</p>\n<ul>\n<li><strong>Directory Entry Formats</strong>: Storing filename-to-file mappings efficiently</li>\n<li><strong>Path Traversal</strong>: Resolving hierarchical paths to specific files</li>\n<li><strong>Directory Search Optimization</strong>: Hash tables, B-trees for fast lookups</li>\n<li><strong>Namespace Management</strong>: Symbolic links, hard links, and mount points</li>\n<li><strong>Directory Consistency</strong>: Maintaining integrity during updates</li>\n</ul>\n<p>We\'ll build a complete directory subsystem from scratch, including both the on-disk structures and in-memory caching. This implementation will explore the trade-offs between different directory organizations, such as linear lists versus tree structures, without relying on existing directory implementations.</p>\n\n<h3>4. File Operations & Caching</h3>\n<p>File operations translate application requests into disk operations, managing the details of reading, writing, and manipulating files. Effective implementations balance correctness, performance, and resource usage.</p>\n<p>Our file operations implementation will cover:</p>\n<ul>\n<li><strong>Basic Operations</strong>: Open, close, read, write, and seek</li>\n<li><strong>Advanced Operations</strong>: Memory mapping, truncation, and extended attributes</li>\n<li><strong>Buffer Cache</strong>: Caching disk blocks for performance</li>\n<li><strong>Read-Ahead</strong>: Prefetching blocks based on access patterns</li>\n<li><strong>Write-Behind</strong>: Delaying and coalescing writes for efficiency</li>\n</ul>\n<p>We\'ll implement a complete file operations layer from scratch, building both the system call interface and the underlying mechanisms. This approach provides insight into how abstract file operations translate to concrete disk operations, without using existing file system code that obscures these details.</p>\n\n<h3>5. Journaling & Recovery</h3>\n<p>File system reliability requires mechanisms to recover from crashes and unexpected shutdowns. Journaling provides a way to maintain consistency by tracking changes before they\'re applied to the main file system structures.</p>\n<p>Key aspects of journaling and recovery include:</p>\n<ul>\n<li><strong>Transaction Logging</strong>: Recording changes before committing them</li>\n<li><strong>Metadata Journaling</strong>: Protecting file system structures while optimizing data handling</li>\n<li><strong>Checkpointing</strong>: Periodically synchronizing journal and main file system</li>\n<li><strong>Recovery Procedures</strong>: Restoring consistency after crashes</li>\n<li><strong>Alternative Approaches</strong>: Copy-on-write, log-structured file systems</li>\n</ul>\n<p>We\'ll implement a journaling mechanism from first principles, developing both the on-disk journal format and the recovery procedures. This hands-on experience with crash recovery provides insight into the delicate balance between performance and reliability in modern file systems, revealing the complex trade-offs that existing implementations make.</p>',
        parentId: 'systems'
    },
    {
        id: 'device_drivers',
        title: 'Device Drivers & I/O',
        description: 'Interfacing with hardware devices',
        content: '<h3>1. I/O Subsystem Architecture</h3>\n<p>The I/O subsystem provides a framework for managing diverse hardware devices through consistent interfaces. Designing this architecture requires balancing abstraction with performance while supporting a wide range of device types.</p>\n<p>Key aspects of I/O subsystem architecture include:</p>\n<ul>\n<li><strong>Device Abstraction Models</strong>: Character, block, and network device paradigms</li>\n<li><strong>Driver Frameworks</strong>: Organizing driver code for modularity and maintainability</li>\n<li><strong>Device Discovery</strong>: Identifying and initializing hardware at boot and runtime</li>\n<li><strong>I/O Scheduling</strong>: Managing concurrent access to shared devices</li>\n<li><strong>Error Handling</strong>: Detecting and recovering from hardware failures</li>\n</ul>\n<p>We\'ll design and implement a complete I/O subsystem from scratch, building the framework that organizes device access. This implementation will avoid using existing driver frameworks, instead developing the architecture from first principles to gain insight into the trade-offs involved in system-level I/O management.</p>\n\n<h3>2. Device Communication Mechanisms</h3>\n<p>Hardware devices communicate with the CPU through various mechanisms, each with different performance characteristics and complexity. Understanding these communication channels is essential for effective driver development.</p>\n<p>Our exploration of device communication will include:</p>\n<ul>\n<li><strong>Port-Mapped I/O</strong>: Communicating through special CPU instructions (in/out)</li>\n<li><strong>Memory-Mapped I/O</strong>: Accessing device registers through memory addresses</li>\n<li><strong>Interrupt Handling</strong>: Responding to asynchronous device signals</li>\n<li><strong>Direct Memory Access (DMA)</strong>: Enabling devices to access memory directly</li>\n<li><strong>Bus Protocols</strong>: Understanding PCI, USB, I2C, and other communication standards</li>\n</ul>\n<p>We\'ll implement drivers using each of these communication mechanisms, understanding their hardware interfaces and software requirements. Rather than using existing driver libraries that hide these details, we\'ll write code that directly communicates with hardware, developing a deep understanding of the CPU-device interface.</p>\n\n<h3>3. Block Device Drivers</h3>\n<p>Block devices manage data in fixed-size blocks and typically support random access. Storage devices like disks are the primary example of block devices, requiring drivers that balance performance, data integrity, and error handling.</p>\n<p>Key aspects of block driver implementation include:</p>\n<ul>\n<li><strong>Block Addressing</strong>: Mapping logical block addresses to physical locations</li>\n<li><strong>Request Queuing</strong>: Managing and optimizing I/O requests</li>\n<li><strong>Read-Ahead and Caching</strong>: Improving performance through prediction</li>\n<li><strong>Error Detection and Recovery</strong>: Handling media defects and hardware failures</li>\n<li><strong>Advanced Storage Features</strong>: TRIM, secure erase, and health monitoring</li>\n</ul>\n<p>We\'ll implement a complete block device driver from scratch, developing each component from the hardware interface upward. This approach avoids using existing block drivers, instead building understanding of the complex interactions between the block layer and both the hardware below and the file system above.</p>\n\n<h3>4. Character and Input Device Drivers</h3>\n<p>Character devices handle data as streams of bytes and often interface with input/output peripherals. These drivers must balance responsiveness with efficient resource usage while handling unpredictable timing.</p>\n<p>Our character driver implementation will cover:</p>\n<ul>\n<li><strong>Terminal Devices</strong>: Managing console and serial communication</li>\n<li><strong>Keyboard and Pointing Devices</strong>: Processing human input events</li>\n<li><strong>Buffering Strategies</strong>: Handling bursty input and output</li>\n<li><strong>Line Discipline</strong>: Processing and editing input streams</li>\n<li><strong>Device Control</strong>: Setting parameters and modes through ioctl operations</li>\n</ul>\n<p>We\'ll build drivers for common character devices from the hardware level up, implementing the complete stack from interrupt handlers to user-space interfaces. This hands-on approach reveals the unique challenges of character devices, such as real-time constraints and variable data rates, without using existing driver implementations that hide these complexities.</p>\n\n<h3>5. Graphics and Display Drivers</h3>\n<p>Graphics devices present unique challenges due to their high bandwidth requirements, complex hardware, and performance sensitivity. Implementing graphics drivers requires understanding both hardware acceleration and software rendering paths.</p>\n<p>Key aspects of graphics driver development include:</p>\n<ul>\n<li><strong>Framebuffer Management</strong>: Controlling display memory and refresh</li>\n<li><strong>Mode Setting</strong>: Configuring resolution, color depth, and timing</li>\n<li><strong>2D Acceleration</strong>: Hardware-assisted rendering operations</li>\n<li><strong>GPU Programming</strong>: Interfacing with graphics processing units</li>\n<li><strong>Multi-Display Support</strong>: Managing multiple output devices</li>\n</ul>\n<p>We\'ll implement graphics drivers from scratch, starting with basic framebuffer support and progressing to hardware acceleration features. Rather than using existing graphics stacks, we\'ll develop our own driver that directly controls the display hardware, gaining insight into the critical but often opaque world of graphics programming. This approach reveals the fundamental techniques that enable modern graphical interfaces, without relying on complex existing implementations.</p>',
        parentId: 'systems'
    },

    // Networking
    {
        id: 'networks',
        title: 'Networking',
        description: 'Building network systems and protocols from scratch',
        content: 'Computer networking is the practice of connecting computers to share resources and communicate. Our "Build from Scratch" approach means learning about every layer of the network stack, from the physical transmission of signals to application protocols, implementing each component yourself. This comprehensive understanding allows you to build, optimize, and troubleshoot network systems with insight that\'s impossible to gain by just using existing implementations.',
        parentId: null
    },
    {
        id: 'physical_layer',
        title: 'Physical Layer & Signal Transmission',
        description: 'How bits become signals on physical media',
        content: '<h3>1. Signal Theory & Electromagnetic Fundamentals</h3>\n<p>At its core, all digital communication relies on the physics of electromagnetic signals. Understanding these fundamentals is essential for designing, troubleshooting, and optimizing physical layer communication systems.</p>\n<p>Key aspects of signal theory include:</p>\n<ul>\n<li><strong>Waveform Properties</strong>: Frequency, amplitude, phase, and their manipulation</li>\n<li><strong>Spectrum Analysis</strong>: Time and frequency domain representations of signals</li>\n<li><strong>Signal Propagation</strong>: How electromagnetic waves travel through different media</li>\n<li><strong>Noise and Interference</strong>: Sources of signal degradation and their effects</li>\n<li><strong>Signal-to-Noise Ratio</strong>: Measuring and improving communication quality</li>\n</ul>\n<p>We\'ll explore these principles through hands-on experiments, using basic electronic components to generate and measure signals. This approach builds understanding from fundamental physics rather than treating signals as abstract entities, providing insight into the physical constraints that shape all communication systems.</p>\n\n<h3>2. Transmission Media & Channel Characteristics</h3>\n<p>Different transmission media have unique properties that affect how signals propagate and degrade. Understanding these characteristics is crucial for designing appropriate signaling methods for each medium.</p>\n<p>Our exploration of transmission media will include:</p>\n<ul>\n<li><strong>Copper-Based Media</strong>: Twisted pair, coaxial, and power line characteristics</li>\n<li><strong>Fiber Optic Systems</strong>: Light propagation, attenuation, and dispersion</li>\n<li><strong>Wireless Propagation</strong>: Radio waves, microwave, and infrared transmission</li>\n<li><strong>Channel Impairments</strong>: Attenuation, distortion, and multi-path effects</li>\n<li><strong>Channel Capacity</strong>: Shannon\'s theorem and fundamental limits</li>\n</ul>\n<p>We\'ll conduct experiments with different media types, measuring their properties and observing how they affect signal transmission. This hands-on approach provides direct experience with the physical constraints of communication channels, rather than just studying theoretical models.</p>\n\n<h3>3. Digital Modulation Techniques</h3>\n<p>Modulation is the process of encoding digital information onto analog carrier signals. Different modulation schemes offer various trade-offs between bandwidth efficiency, power requirements, and noise resistance.</p>\n<p>Key digital modulation techniques include:</p>\n<ul>\n<li><strong>Amplitude Shift Keying (ASK)</strong>: Encoding bits by varying signal amplitude</li>\n<li><strong>Frequency Shift Keying (FSK)</strong>: Using different frequencies to represent bits</li>\n<li><strong>Phase Shift Keying (PSK)</strong>: Encoding data through phase variations</li>\n<li><strong>Quadrature Amplitude Modulation (QAM)</strong>: Combining amplitude and phase modulation</li>\n<li><strong>Spread Spectrum Techniques</strong>: DSSS and FHSS for noise resistance</li>\n</ul>\n<p>We\'ll implement these modulation schemes from scratch using basic signal processing components, understanding their mathematical foundations and practical implementations. Rather than using existing modulation libraries, we\'ll build our own modulators and demodulators, gaining insight into the algorithms that convert between bits and physical signals.</p>\n\n<h3>4. Line Coding & Signal Encoding</h3>\n<p>Line coding translates binary data into signal patterns suitable for specific transmission media. Effective encoding schemes balance multiple objectives including clock recovery, error detection, and spectral efficiency.</p>\n<p>Our study of line coding will cover:</p>\n<ul>\n<li><strong>Non-Return to Zero (NRZ)</strong>: Basic binary voltage level representations</li>\n<li><strong>Manchester Encoding</strong>: Self-clocking schemes for synchronization</li>\n<li><strong>Multi-Level Transmissions</strong>: 2B1Q, PAM-n, and other bandwidth-efficient codes</li>\n<li><strong>Run-Length Limited Codes</strong>: Controlling signal patterns for physical constraints</li>\n<li><strong>8b/10b and Similar Encodings</strong>: Balancing DC levels and error detection</li>\n</ul>\n<p>We\'ll implement these encoding schemes from first principles, building encoders and decoders that translate between raw binary data and physical signal representations. This implementation-focused approach reveals the practical challenges of reliable bit transmission, without relying on existing encoding libraries.</p>\n\n<h3>5. Physical Layer Protocols & Interfaces</h3>\n<p>Physical layer protocols define the electrical, mechanical, and procedural standards for activating, maintaining, and deactivating physical connections. These standards ensure interoperability between equipment from different vendors.</p>\n<p>Key physical layer standards and interfaces include:</p>\n<ul>\n<li><strong>Ethernet Physical Layer</strong>: 10BASE-T, 100BASE-TX, 1000BASE-T standards</li>\n<li><strong>Serial Communication</strong>: RS-232, RS-485, UART, SPI, and I2C interfaces</li>\n<li><strong>Wireless PHY Layers</strong>: 802.11 (Wi-Fi), Bluetooth, and cellular protocols</li>\n<li><strong>Digital Subscriber Line (DSL)</strong>: Techniques for high-speed transmission over copper</li>\n<li><strong>Clock Recovery</strong>: Synchronizing transmitter and receiver timing</li>\n</ul>\n<p>We\'ll implement simplified versions of these physical layer protocols, focusing on the signaling, synchronization, and timing aspects. Rather than using existing protocol stacks, we\'ll build our own physical layer interfaces from basic components, developing insight into how abstract bits become reliable physical signals in real-world communication systems.</p>',
        parentId: 'networks'
    },
    {
        id: 'data_link_layer',
        title: 'Data Link Layer & MAC Protocols',
        description: 'Creating reliable links between directly connected devices',
        content: '<h3>1. Framing & Packet Structure</h3>\n<p>Framing is the process of organizing raw bits into structured data units that can be reliably transmitted and received. Effective frame designs balance efficiency, reliability, and compatibility with physical layer constraints.</p>\n<p>Key aspects of framing include:</p>\n<ul>\n<li><strong>Frame Delimiting</strong>: Marking the beginning and end of frames</li>\n<li><strong>Character/Bit Stuffing</strong>: Preventing delimiter confusion in data</li>\n<li><strong>Frame Synchronization</strong>: Aligning receiver with incoming frame boundaries</li>\n<li><strong>Address Fields</strong>: Identifying source and destination devices</li>\n<li><strong>Control Information</strong>: Frame types, sequence numbers, and flags</li>\n</ul>\n<p>We\'ll implement multiple framing methods from scratch, developing software that transforms raw data streams into properly structured frames. Rather than using existing framing libraries, we\'ll build our own implementations, gaining insight into the challenges of maintaining frame integrity across noisy channels.</p>\n\n<h3>2. Error Detection & Correction</h3>\n<p>Communication channels introduce errors that corrupt transmitted data. Error detection and correction codes allow receivers to identify and potentially repair these corruptions, ensuring data reliability.</p>\n<p>Our exploration of error control will include:</p>\n<ul>\n<li><strong>Parity Checks</strong>: Simple single-bit error detection</li>\n<li><strong>Cyclic Redundancy Checks (CRC)</strong>: Polynomial division for robust detection</li>\n<li><strong>Checksums</strong>: Addition-based error detection methods</li>\n<li><strong>Hamming Codes</strong>: Detecting and correcting single-bit errors</li>\n<li><strong>Forward Error Correction</strong>: Reed-Solomon and other correction techniques</li>\n</ul>\n<p>We\'ll implement these error detection and correction algorithms from first principles, building encoders and decoders that can protect data from corruption. This approach reveals the mathematical foundations and practical trade-offs in error control, without relying on existing implementations that hide these details.</p>\n\n<h3>3. Medium Access Control Protocols</h3>\n<p>When multiple devices share a communication medium, they need protocols to coordinate access and prevent interference. MAC protocols balance fairness, efficiency, and determinism in channel sharing.</p>\n<p>Key MAC protocols we\'ll explore include:</p>\n<ul>\n<li><strong>CSMA/CD</strong>: Ethernet\'s approach to shared medium coordination</li>\n<li><strong>CSMA/CA</strong>: Collision avoidance techniques used in wireless networks</li>\n<li><strong>Token Passing</strong>: Deterministic approaches to medium access</li>\n<li><strong>Time Division Multiple Access (TDMA)</strong>: Scheduled time slot allocation</li>\n<li><strong>Frequency Division Multiple Access (FDMA)</strong>: Dividing the frequency spectrum</li>\n</ul>\n<p>We\'ll implement these protocols from scratch, developing systems that can coordinate multiple devices on a shared channel. This hands-on approach provides insight into the fundamental problem of distributed coordination without centralized control, revealing how network designs balance performance and reliability.</p>\n\n<h3>4. Link Control Protocols</h3>\n<p>Link control protocols manage the communication flow between directly connected devices, handling acknowledgments, retransmissions, and sequencing to ensure reliable data transfer.</p>\n<p>Our implementation of link control will cover:</p>\n<ul>\n<li><strong>Stop-and-Wait ARQ</strong>: Simple acknowledgment-based reliability</li>\n<li><strong>Sliding Window Protocols</strong>: Go-Back-N and Selective Repeat</li>\n<li><strong>Flow Control</strong>: Preventing receiver overflow</li>\n<li><strong>Connection Management</strong>: Establishing and terminating links</li>\n<li><strong>Protocol Efficiency Analysis</strong>: Throughput and utilization optimization</li>\n</ul>\n<p>We\'ll build these protocols from first principles, creating software that can reliably transfer data over unreliable links. This implementation-focused approach reveals the algorithmic and engineering challenges in ensuring delivery guarantees, without using existing protocol stacks that hide the underlying mechanisms.</p>\n\n<h3>5. Switching & Bridging</h3>\n<p>Switches and bridges extend the data link layer beyond point-to-point connections, connecting multiple network segments while operating at the MAC address level. These devices form the foundation of local area networks.</p>\n<p>Key switching and bridging topics include:</p>\n<ul>\n<li><strong>MAC Address Learning</strong>: Building and maintaining forwarding tables</li>\n<li><strong>Frame Forwarding</strong>: Directing frames to appropriate ports</li>\n<li><strong>Spanning Tree Protocol</strong>: Preventing loops in redundant topologies</li>\n<li><strong>VLAN Implementation</strong>: Logical network segmentation</li>\n<li><strong>Switch Architectures</strong>: Cut-through vs. store-and-forward designs</li>\n</ul>\n<p>We\'ll implement switching algorithms from scratch, building software that can intelligently forward frames between network segments. Rather than using existing switch firmware or simulators, we\'ll develop our own switching logic, gaining insight into the self-learning mechanisms that allow switches to automatically build network topologies. This approach reveals the elegant solutions that enable modern local networks to scale efficiently while maintaining plug-and-play simplicity.</p>',
        parentId: 'networks'
    },
    {
        id: 'network_layer',
        title: 'Network Layer & Routing',
        description: 'Finding paths through interconnected networks',
        content: '<h3>1. Network Layer Architecture & Protocols</h3>\n<p>The network layer provides end-to-end packet delivery across multiple network segments, creating the illusion of a unified network despite physical boundaries. Designing effective network layer protocols requires balancing scalability, efficiency, and robustness.</p>\n<p>Key aspects of network layer architecture include:</p>\n<ul>\n<li><strong>Connectionless vs. Connection-Oriented Service</strong>: Delivery guarantees and state management</li>\n<li><strong>Network Layer Addressing</strong>: Global identification of hosts across interconnected networks</li>\n<li><strong>Protocol Structures</strong>: Packet formats, header designs, and extension mechanisms</li>\n<li><strong>Fragmentation and Reassembly</strong>: Handling different maximum transfer units</li>\n<li><strong>Error Handling and Reporting</strong>: Detecting and responding to delivery failures</li>\n</ul>\n<p>We\'ll design and implement a complete network layer protocol from scratch, developing the packet formats, addressing schemes, and core services. This approach avoids existing implementations like IPv4/IPv6, instead building understanding of fundamental design decisions and trade-offs in internetworking protocols.</p>\n\n<h3>2. Addressing & Subnetting</h3>\n<p>Network addressing schemes allow devices to be uniquely identified across vast interconnected systems. Subnetting divides address spaces into manageable segments, enabling efficient routing and address allocation.</p>\n<p>Our exploration of addressing will include:</p>\n<ul>\n<li><strong>Address Structure</strong>: Designing hierarchical addressing schemes</li>\n<li><strong>Subnet Division</strong>: CIDR notation and subnet mask calculations</li>\n<li><strong>Address Resolution</strong>: Mapping between network and link layer addresses</li>\n<li><strong>Address Assignment</strong>: Static, dynamic, and automatic configuration</li>\n<li><strong>Special Purpose Addresses</strong>: Broadcast, multicast, and reserved ranges</li>\n</ul>\n<p>We\'ll implement addressing and subnetting from first principles, building tools that can divide address spaces, perform subnet calculations, and manage address allocation. This hands-on approach reveals the mathematical and design foundations of network addressing, without relying on existing tools that hide these mechanisms.</p>\n\n<h3>3. Routing Algorithms</h3>\n<p>Routing algorithms determine the best paths for packets to travel through a network. Effective algorithms balance optimality, convergence speed, and resource usage while adapting to changing network conditions.</p>\n<p>Key routing algorithms we\'ll implement include:</p>\n<ul>\n<li><strong>Distance Vector Routing</strong>: Bellman-Ford approach to distributed path finding</li>\n<li><strong>Link State Routing</strong>: Dijkstra-based approaches with complete topology knowledge</li>\n<li><strong>Path Vector Routing</strong>: BGP-like policy-based routing with path attributes</li>\n<li><strong>Hierarchical Routing</strong>: Area-based approaches for large-scale networks</li>\n<li><strong>Multicast Routing</strong>: Efficiently delivering to multiple destinations</li>\n</ul>\n<p>We\'ll implement these algorithms from scratch, building routing protocols that can dynamically discover network topologies and compute optimal paths. This implementation approach provides insight into the distributed algorithms that make global communication possible, revealing how millions of routers coordinate without centralized control.</p>\n\n<h3>4. Packet Forwarding & Switching</h3>\n<p>Packet forwarding is the process of moving packets from input to output interfaces based on routing decisions. Efficient forwarding implementations balance speed, flexibility, and resource utilization.</p>\n<p>Our implementation of forwarding will cover:</p>\n<ul>\n<li><strong>Forwarding Table Organization</strong>: Data structures for rapid lookup</li>\n<li><strong>Longest Prefix Matching</strong>: Algorithms for finding the most specific route</li>\n<li><strong>Switching Fabrics</strong>: Hardware architectures for high-speed forwarding</li>\n<li><strong>Quality of Service</strong>: Prioritization and traffic management</li>\n<li><strong>Network Address Translation</strong>: Mapping between address domains</li>\n</ul>\n<p>We\'ll build packet forwarding engines from first principles, implementing the algorithms and data structures used in modern routers. Rather than using existing forwarding implementations, we\'ll develop our own, gaining insight into the performance optimizations and design trade-offs in high-speed networking equipment.</p>\n\n<h3>5. Internet Protocol Implementation</h3>\n<p>The Internet Protocol (IP) is the foundation of the global internet, providing a standardized internetworking layer that enables worldwide communication. Understanding IP requires implementing its key mechanisms and behaviors.</p>\n<p>Key aspects of IP implementation include:</p>\n<ul>\n<li><strong>IPv4 and IPv6 Packet Structure</strong>: Header formats and extension mechanisms</li>\n<li><strong>IP Routing Tables</strong>: Managing forwarding information</li>\n<li><strong>ICMP Implementation</strong>: Error reporting and network diagnostics</li>\n<li><strong>IP Security</strong>: Authentication and encryption at the network layer</li>\n<li><strong>IP Mobility</strong>: Maintaining connections across network changes</li>\n</ul>\n<p>We\'ll implement a simplified but functional version of IP from scratch, including packet handling, routing integration, and support services. This approach avoids using existing IP stacks, instead building deep understanding of how the internet\'s fundamental protocol works by creating our own implementation. This reveals the elegant design decisions that have allowed IP to scale from a small research network to a global communication system connecting billions of devices.</p>',
        parentId: 'networks'
    },
    {
        id: 'transport_layer',
        title: 'Transport Layer Protocols',
        description: 'Creating reliable end-to-end communication',
        content: 'The transport layer provides end-to-end communication services for applications, including reliability, flow control, and congestion management. This topic covers protocol design for both reliable (TCP-like) and unreliable (UDP-like) transport. You\'ll implement your own transport protocols from scratch, including connection establishment, acknowledgment mechanisms, retransmission strategies, and congestion control algorithms. Rather than using existing TCP/UDP implementations, we\'ll build a complete transport layer that applications can use to communicate reliably over unreliable networks. This hands-on approach provides deep insight into the challenges and solutions of internet-scale communication.',
        parentId: 'networks'
    },
    {
        id: 'application_protocols',
        title: 'Application Layer Protocols',
        description: 'Building protocols for specific applications',
        content: 'Application layer protocols define how applications communicate over a network. This topic explores the design and implementation of protocols for specific purposes like file transfer, web access, email, and real-time communication. You\'ll implement your own application protocols from scratch, developing clients and servers that can interoperate using well-defined message formats and sequences. Rather than using existing libraries or frameworks, we\'ll build complete application-level communication systems that solve real-world problems. This approach provides insight into protocol design considerations, state management, and the tradeoffs involved in networked application development.',
        parentId: 'networks'
    },

    // Algorithms & Data Structures
    {
        id: 'algorithms',
        title: 'Algorithms & Data Structures',
        description: 'Implementing efficient solutions from first principles',
        content: 'Algorithms are step-by-step procedures for solving problems, while data structures organize data for efficient access and modification. Our "Build from Scratch" approach means implementing every algorithm and data structure yourself, understanding their mathematical foundations, and analyzing their efficiency. Rather than using existing libraries, you\'ll gain the skills to design and optimize solutions for any computational problem.',
        parentId: null
    },
    {
        id: 'basic_data_structures',
        title: 'Basic Data Structures',
        description: 'Fundamental ways to organize data',
        content: 'Basic data structures provide the foundation for efficiently organizing and accessing data in memory. This topic covers the implementation of fundamental structures like arrays, linked lists, stacks, queues, and hash tables. You\'ll build each of these structures from scratch, starting with primitive memory operations and implementing all the necessary operations and algorithms. Rather than using language-provided containers or libraries, we\'ll develop our own implementations, gaining insight into memory management, pointers, and the tradeoffs between different ways of organizing data. This approach builds a deep understanding of how data is stored and manipulated in computer systems.',
        parentId: 'algorithms'
    },
    {
        id: 'advanced_data_structures',
        title: 'Advanced Data Structures',
        description: 'Sophisticated structures for complex problems',
        content: 'Advanced data structures provide efficient solutions for more complex data management problems. This topic covers the implementation of sophisticated structures like balanced trees (AVL, Red-Black), B-trees, heaps, tries, and graphs. You\'ll build each of these structures from scratch, including all the balancing operations, traversal algorithms, and optimizations that make them effective. Instead of using existing libraries, we\'ll develop our own implementations, gaining insight into the mathematical properties and algorithmic techniques that enable efficient operations on large datasets. This approach builds the skills to design custom data structures for specific problem domains.',
        parentId: 'algorithms'
    },
    {
        id: 'searching_sorting',
        title: 'Searching & Sorting Algorithms',
        description: 'Finding and organizing data efficiently',
        content: 'Searching and sorting are fundamental operations in computing that arrange and locate data. This topic covers algorithms for efficiently searching through data (linear, binary, hashing) and sorting collections (bubble, insertion, selection, merge, quick, heap). You\'ll implement each algorithm from scratch, analyzing their time and space complexity mathematically. Rather than using language-provided functions or libraries, we\'ll develop our own implementations, gaining insight into algorithm design, correctness proofs, and optimization techniques. This approach builds a deep understanding of algorithmic efficiency and the tradeoffs involved in different approaches.',
        parentId: 'algorithms'
    },
    {
        id: 'graph_algorithms',
        title: 'Graph Algorithms',
        description: 'Solving problems on interconnected data',
        content: 'Graph algorithms provide solutions for problems involving relationships between objects. This topic covers algorithms for traversal (BFS, DFS), shortest paths (Dijkstra\'s, Bellman-Ford), minimum spanning trees (Prim\'s, Kruskal\'s), and network flow. You\'ll implement each algorithm from scratch, representing graphs using appropriate data structures and developing the necessary algorithmic logic. Rather than using existing graph libraries, we\'ll build complete implementations that can solve real-world problems in areas like social networks, transportation systems, and computer networks. This approach provides deep insight into the mathematical properties of graphs and the algorithmic techniques that exploit their structure.',
        parentId: 'algorithms'
    },
    {
        id: 'algorithm_design',
        title: 'Algorithm Design Techniques',
        description: 'Strategies for creating efficient algorithms',
        content: 'Algorithm design techniques provide systematic approaches to solving computational problems. This topic explores paradigms like divide-and-conquer, dynamic programming, greedy algorithms, and backtracking. You\'ll implement algorithms based on each technique from scratch, applying them to solve complex problems. Rather than memorizing existing solutions, we\'ll develop the skills to recognize which technique is appropriate for a given problem and how to apply it effectively. This approach builds analytical thinking and problem-solving abilities, enabling you to design efficient algorithms for novel problems rather than just using pre-built solutions.',
        parentId: 'algorithms'
    },

    // Databases
    {
        id: 'databases',
        title: 'Databases',
        description: 'Building database systems from first principles',
        content: 'Database systems provide organized mechanisms to store, manage, and retrieve information. Our "Build from Scratch" approach means implementing every component of a database system yourself, from storage engines and query processors to transaction managers. Rather than just using existing database systems, you\'ll understand how they work internally and be able to design storage solutions for any data management problem.',
        parentId: null
    },
    {
        id: 'storage_engines',
        title: 'Storage Engines & Data Organization',
        description: 'How databases physically store and retrieve data',
        content: 'Storage engines are the components of database systems that manage how data is stored on disk and in memory. This topic explores the physical organization of data, including file formats, page layouts, and access methods. You\'ll implement your own storage engine from scratch, designing data structures for efficient storage and retrieval of records. We\'ll cover indexing techniques like B-trees and hash indexes, implementing these structures ourselves rather than using existing libraries. This approach provides deep insight into the fundamental challenges of persistent storage, including durability, concurrency, and performance optimization.',
        parentId: 'databases'
    },
    {
        id: 'query_processing',
        title: 'Query Processing & Execution',
        description: 'Translating queries into efficient operations',
        content: 'Query processing is how database systems translate user queries into execution plans that retrieve or modify data. This topic covers query parsing, optimization, and execution strategies. You\'ll implement your own query processor from scratch, including a parser for a SQL-like language, an optimizer that can rewrite queries for efficiency, and an execution engine that carries out the operations. Rather than using existing query engines, we\'ll build a complete system that can understand and efficiently execute complex queries. This approach provides insight into the algorithms and techniques that enable databases to answer questions about large datasets quickly.',
        parentId: 'databases'
    },
    {
        id: 'transaction_management',
        title: 'Transaction Management',
        description: 'Ensuring data consistency during concurrent operations',
        content: 'Transaction management ensures that database operations maintain consistency even during concurrent access and system failures. This topic covers ACID properties, concurrency control mechanisms, isolation levels, and recovery techniques. You\'ll implement your own transaction manager from scratch, including locking protocols, deadlock detection, write-ahead logging, and crash recovery. Rather than relying on existing transaction systems, we\'ll build a complete implementation that can correctly handle multiple clients modifying data simultaneously. This approach provides deep insight into the theoretical and practical challenges of maintaining data integrity in multi-user systems.',
        parentId: 'databases'
    },
    {
        id: 'database_design',
        title: 'Database Design & Normalization',
        description: 'Structuring data for consistency and efficiency',
        content: 'Database design is the process of structuring data to represent real-world entities and relationships efficiently. This topic covers data modeling techniques, normalization theory, and schema design. You\'ll learn how to analyze application requirements and create appropriate database schemas that minimize redundancy while supporting required queries. We\'ll implement integrity constraints and explore the mathematical foundations of the relational model. Rather than using automated design tools, you\'ll develop the skills to manually design optimal database structures, gaining insight into the theoretical principles that guide schema design decisions.',
        parentId: 'databases'
    },
    {
        id: 'distributed_databases',
        title: 'Distributed Database Systems',
        description: 'Building databases that span multiple machines',
        content: 'Distributed database systems store and process data across multiple computers for improved scalability, availability, and reliability. This topic explores the challenges and solutions for distributing data, including partitioning strategies, replication protocols, distributed transactions, and consensus algorithms. You\'ll implement components of a distributed database from scratch, including mechanisms for data distribution, consistency maintenance, and fault tolerance. Rather than using existing distributed systems, we\'ll build our own implementations of key protocols like two-phase commit and Paxos. This approach provides deep insight into the fundamental trade-offs in distributed systems and the techniques used to overcome the challenges of network partitions and node failures.',
        parentId: 'databases'
    },

    // Cybersecurity
    {
        id: 'security',
        title: 'Cybersecurity',
        description: 'Building security systems and controls from first principles',
        content: 'Cybersecurity is the practice of protecting systems, networks, and programs from digital attacks. Our "Build from Scratch" approach means implementing security mechanisms yourself, understanding the mathematical foundations of cryptography, and learning to think like both defenders and attackers. Rather than just using existing security tools, you\'ll gain the knowledge to design secure systems from the ground up and evaluate the security of any digital system.',
        parentId: null
    },
    {
        id: 'crypto_fundamentals',
        title: 'Cryptographic Fundamentals',
        description: 'Mathematical foundations of secure communication',
        content: 'Cryptography provides the mathematical tools to secure digital information and communications. This topic covers the fundamental concepts and algorithms that enable secure systems. You\'ll learn about number theory, modular arithmetic, and information theory that form the foundation of cryptography. We\'ll implement classical ciphers (substitution, transposition) and modern symmetric algorithms (AES) from scratch, understanding their mathematical properties and security guarantees. Rather than using existing cryptographic libraries, you\'ll build your own implementations, gaining insight into how security primitives work at a fundamental level and why they\'re secure against various attacks.',
        parentId: 'security'
    },
    {
        id: 'public_key_crypto',
        title: 'Public Key Cryptography',
        description: 'Building asymmetric encryption systems',
        content: 'Public key cryptography enables secure communication without a shared secret key. This topic covers the mathematics and algorithms of asymmetric encryption. You\'ll implement essential public key systems like RSA and elliptic curve cryptography from scratch, building on number theory principles such as modular exponentiation and the difficulty of factoring large numbers. We\'ll develop digital signature algorithms and key exchange protocols, understanding their security properties. Rather than using existing libraries, you\'ll build these systems yourself, gaining deep insight into the mathematical foundations that enable secure internet communication, from HTTPS to cryptocurrency.',
        parentId: 'security'
    },
    {
        id: 'secure_systems',
        title: 'Secure Systems Design',
        description: 'Building systems with security from the ground up',
        content: 'Secure systems design applies security principles throughout the architecture and implementation of software and hardware. This topic covers methodologies for creating systems that are secure by design, not as an afterthought. You\'ll learn about threat modeling, principle of least privilege, defense in depth, and secure coding practices. We\'ll implement security mechanisms like access control systems, authentication frameworks, and secure communication channels from scratch. Rather than using existing solutions, you\'ll build your own security controls, developing a deep understanding of how to translate security requirements into concrete implementations that protect against real-world threats.',
        parentId: 'security'
    },
    {
        id: 'security_analysis',
        title: 'Security Analysis & Penetration Testing',
        description: 'Finding and exploiting vulnerabilities',
        content: 'Security analysis examines systems to identify and address vulnerabilities before they can be exploited. This topic covers techniques for analyzing systems from an attacker\'s perspective. You\'ll learn about common vulnerability classes (buffer overflows, SQL injection, XSS) and how to find them through static analysis, fuzzing, and penetration testing. We\'ll implement our own analysis tools from scratch, including simple static analyzers and fuzzers. Rather than using existing security scanners, you\'ll develop your own testing methodologies and tools, gaining insight into how attackers think and how to build more resilient systems that can withstand sophisticated attacks.',
        parentId: 'security'
    },
    {
        id: 'network_security',
        title: 'Network Security',
        description: 'Protecting communications and network infrastructure',
        content: 'Network security focuses on protecting data in transit and securing network infrastructure. This topic explores threats to networks and the mechanisms to defend against them. You\'ll learn about secure protocols, firewalls, intrusion detection systems, and virtual private networks. We\'ll implement our own network security tools from scratch, including packet filters, simple intrusion detection systems, and VPN protocols based on the cryptographic primitives you\'ve already built. Rather than using existing security appliances or software, you\'ll develop your own network security solutions, gaining deep insight into how attacks propagate through networks and how defenses can be structured to prevent unauthorized access and protect data confidentiality and integrity.',
        parentId: 'security'
    },
    
    // AI & Machine Learning
    {
        id: 'ai_ml',
        title: 'AI & Machine Learning',
        description: 'Building intelligent systems from mathematical foundations',
        content: 'Artificial Intelligence enables machines to simulate human intelligence, while Machine Learning allows systems to learn from data without explicit programming. Our "Build from Scratch" approach means implementing AI algorithms and ML models yourself, starting from their mathematical foundations. Rather than using existing frameworks and libraries, you\'ll gain the knowledge to design, train, and evaluate intelligent systems from first principles.',
        parentId: null
    },
    {
        id: 'ml_fundamentals',
        title: 'Machine Learning Fundamentals',
        description: 'Mathematical foundations of learning from data',
        content: 'Machine learning fundamentals establish the mathematical and statistical principles that enable computers to learn from data. This topic covers the core concepts and techniques that form the basis of ML. You\'ll learn about linear algebra, probability theory, and optimization methods essential for machine learning. We\'ll implement fundamental algorithms from scratch, including linear regression, logistic regression, and k-means clustering, understanding their mathematical derivations and properties. Rather than using existing ML libraries, you\'ll build your own implementations, developing a deep understanding of how learning algorithms extract patterns from data and make predictions or decisions based on those patterns.',
        parentId: 'ai_ml'
    },
    {
        id: 'neural_networks',
        title: 'Neural Networks From Scratch',
        description: 'Building brain-inspired learning systems',
        content: 'Neural networks are computational models inspired by the human brain that can learn complex patterns from data. This topic explores the mathematics, architecture, and training of neural networks. You\'ll implement feed-forward networks from scratch, starting with neurons as simple computational units and building up to multi-layer networks. We\'ll develop backpropagation and gradient descent algorithms to train these networks, understanding their mathematical foundations. Rather than using existing frameworks like TensorFlow or PyTorch, you\'ll build your own neural network library, gaining insight into every aspect of how these powerful models learn and make predictions.',
        parentId: 'ai_ml'
    },
    {
        id: 'deep_learning',
        title: 'Deep Learning Architectures',
        description: 'Advanced neural network structures and algorithms',
        content: 'Deep learning extends neural networks with advanced architectures that can learn highly complex patterns. This topic covers specialized network structures for different types of data and problems. You\'ll implement convolutional neural networks (CNNs) for image processing, recurrent neural networks (RNNs) and transformers for sequential data, and generative adversarial networks (GANs) for generating new content. We\'ll build these architectures from scratch, understanding their mathematical foundations and training algorithms. Rather than using existing deep learning libraries, you\'ll implement your own versions of these cutting-edge models, developing insight into how they achieve state-of-the-art performance on complex tasks like image recognition, natural language processing, and content generation.',
        parentId: 'ai_ml'
    },
    {
        id: 'reinforcement_learning',
        title: 'Reinforcement Learning',
        description: 'Building systems that learn through interaction',
        content: 'Reinforcement learning is a paradigm where agents learn to make decisions by interacting with an environment. This topic explores how systems can learn optimal behavior through trial, error, and reward. You\'ll implement fundamental RL algorithms from scratch, including dynamic programming methods, Monte Carlo methods, and temporal-difference learning (Q-learning). We\'ll develop more advanced techniques like policy gradients and deep Q-networks, understanding their mathematical foundations. Rather than using existing RL libraries, you\'ll build your own implementations, gaining insight into how agents can learn complex behaviors without explicit programming, from game playing to robotic control to resource allocation.',
        parentId: 'ai_ml'
    },
    {
        id: 'ml_systems',
        title: 'Machine Learning Systems',
        description: 'Building complete ML pipelines and applications',
        content: 'Machine learning systems integrate ML models into larger software systems to solve real-world problems. This topic explores the architecture and engineering of complete ML applications. You\'ll learn about data pipelines, feature engineering, model selection, evaluation metrics, and deployment strategies. We\'ll implement a complete ML system from scratch, from data collection and preprocessing to model training, evaluation, and serving. Rather than using existing ML platforms, you\'ll build your own infrastructure for managing the ML lifecycle, gaining insight into the challenges and solutions for creating reliable, scalable, and maintainable intelligent systems that can operate in production environments.',
        parentId: 'ai_ml'
    },
    
    // Theory of Computation
    {
        id: 'theory',
        title: 'Theory of Computation',
        description: 'Mathematical foundation of computation and algorithms',
        content: 'Theory of Computation is the mathematical study of the capabilities and limitations of computing systems. Our "Build from Scratch" approach means learning the formal models and mathematical proofs that define computation, then implementing these models yourself. This deep theoretical understanding provides insight into what can and cannot be computed, and forms the foundation for practical algorithm design and analysis.',
        parentId: null
    },
    {
        id: 'automata_theory',
        title: 'Automata Theory',
        description: 'Formal models of computation',
        content: 'Automata theory studies abstract mathematical models of computation. This topic explores different classes of automata and their computational power. You\'ll learn about finite automata, pushdown automata, and Turing machines, understanding their formal definitions and properties. We\'ll implement these models from scratch, building simulators that can execute automata on given inputs and determine whether they accept or reject. Rather than using existing libraries, you\'ll develop your own representations and algorithms for these fundamental computational models, gaining insight into the theoretical foundation of what computers can and cannot compute and how computational problems can be classified based on the simplest machine that can solve them.',
        parentId: 'theory'
    },
    {
        id: 'formal_languages',
        title: 'Formal Languages & Grammars',
        description: 'Mathematical systems for describing languages',
        content: 'Formal languages and grammars provide mathematical frameworks for defining sets of strings (languages) and the rules that generate them. This topic explores the Chomsky hierarchy of grammars and their corresponding automata. You\'ll learn about regular grammars, context-free grammars, context-sensitive grammars, and unrestricted grammars. We\'ll implement parsers and generators for these grammar types from scratch, understanding their computational properties. Rather than using existing parsing tools, you\'ll build your own implementations of regular expressions, context-free parsers, and other language processing tools, gaining insight into the theoretical foundation of programming languages, compilers, and text processing systems.',
        parentId: 'theory'
    },
    {
        id: 'computability',
        title: 'Computability Theory',
        description: 'The limits of what can be computed',
        content: 'Computability theory investigates which problems can be solved algorithmically and which cannot. This topic explores the boundary between computability and non-computability. You\'ll learn about the Church-Turing thesis, decidable and undecidable problems, reductions, and the halting problem. We\'ll implement simulations of Universal Turing Machines from scratch, demonstrating their ability to compute any computable function. Rather than accepting computability results as abstract concepts, you\'ll develop concrete implementations that illustrate these fundamental limits, gaining insight into why certain problems cannot be solved by any algorithm regardless of computational resources, with implications for program verification, automated reasoning, and artificial intelligence.',
        parentId: 'theory'
    },
    {
        id: 'complexity_theory',
        title: 'Complexity Theory',
        description: 'Measuring and classifying computational efficiency',
        content: 'Complexity theory studies the resources (time, space) required to solve computational problems. This topic explores how to classify problems based on their inherent difficulty. You\'ll learn about complexity classes (P, NP, NP-complete, PSPACE), reduction techniques, and the relationships between different classes. We\'ll implement algorithms for classic problems in various complexity classes from scratch, understanding their time and space requirements. Rather than treating complexity results as theoretical abstractions, you\'ll develop concrete implementations that demonstrate why some problems are fundamentally harder than others, gaining insight into the P vs. NP question and its implications for algorithm design, cryptography, and optimization.',
        parentId: 'theory'
    },
    {
        id: 'information_theory_cs',
        title: 'Information Theory & Coding',
        description: 'Mathematical foundations of data compression and transmission',
        content: 'Information theory quantifies the amount of information in data and establishes fundamental limits on data compression and transmission. This topic explores the mathematical principles that govern how information can be efficiently represented and communicated. You\'ll learn about entropy, mutual information, channel capacity, and error-correcting codes. We\'ll implement compression algorithms and error-correction schemes from scratch, understanding their theoretical foundations. Rather than using existing libraries, you\'ll build your own implementations of Huffman coding, Hamming codes, and other information-theoretic algorithms, gaining insight into the fundamental limits of data storage and communication and how systems can approach these theoretical limits in practice.',
        parentId: 'theory'
    },
    
    // Software Engineering
    {
        id: 'software_eng',
        title: 'Software Engineering',
        description: 'Building quality software systems from first principles',
        content: 'Software Engineering is the systematic application of engineering approaches to software development. Our "Build from Scratch" approach means designing and implementing software systems yourself, understanding every component rather than relying on existing frameworks. You\'ll learn methodologies for building reliable, maintainable, and efficient software, with an emphasis on understanding why certain practices work rather than just following established processes.',
        parentId: null
    },
    {
        id: 'system_design',
        title: 'System Architecture & Design',
        description: 'Designing software systems from first principles',
        content: 'System architecture and design establish the overall structure and behavior of software systems. This topic explores methodologies for designing complex software from first principles. You\'ll learn about architectural patterns (layered, microservices, event-driven), design principles (SOLID, DRY, KISS), and modeling techniques. We\'ll implement architectural patterns from scratch, building simple but functional systems that demonstrate each approach. Rather than using existing frameworks that enforce specific architectures, you\'ll develop your own implementations, gaining insight into the tradeoffs between different architectural choices and how to design systems that meet both functional and non-functional requirements while remaining flexible enough to evolve over time.',
        parentId: 'software_eng'
    },
    {
        id: 'software_construction',
        title: 'Software Construction',
        description: 'Building robust software components and systems',
        content: 'Software construction covers the practical techniques for implementing reliable, maintainable code. This topic explores how to turn designs into working software through thoughtful construction practices. You\'ll learn about code organization, defensive programming, error handling, and program correctness. We\'ll implement common design patterns and programming idioms from scratch, understanding their benefits and appropriate use. Rather than relying on existing implementations or frameworks, you\'ll build your own versions of these patterns, gaining insight into how well-structured code supports maintainability, reliability, and efficiency. This approach emphasizes understanding why certain practices lead to better software, not just following conventions.',
        parentId: 'software_eng'
    },
    {
        id: 'testing_verification',
        title: 'Testing & Verification',
        description: 'Ensuring software correctness systematically',
        content: 'Testing and verification provide systematic approaches to ensuring that software behaves as expected. This topic explores different techniques for validating software behavior. You\'ll learn about unit testing, integration testing, system testing, and formal verification methods. We\'ll implement testing frameworks and verification tools from scratch, understanding their theoretical foundations. Rather than using existing testing libraries, you\'ll build your own test runners, assertion mechanisms, mocking frameworks, and simple formal verification tools. This approach provides deep insight into how testing and verification can discover different kinds of defects, the tradeoffs between different approaches, and how to design software that is inherently more testable and verifiable.',
        parentId: 'software_eng'
    },
    {
        id: 'performance_engineering',
        title: 'Performance Engineering',
        description: 'Building efficient software through measurement and optimization',
        content: 'Performance engineering focuses on making software systems faster, more efficient, and more scalable. This topic explores methodologies for understanding and improving system performance. You\'ll learn about profiling, benchmarking, bottleneck analysis, and optimization techniques. We\'ll implement performance measurement tools from scratch, developing profilers, memory analyzers, and benchmarking frameworks. Rather than using existing performance tools, you\'ll build your own instruments for understanding where time and resources are spent in a system. This approach provides deep insight into how algorithms, data structures, and system interactions affect performance, and how to make principled optimizations that improve efficiency without sacrificing maintainability.',
        parentId: 'software_eng'
    },
    {
        id: 'software_processes',
        title: 'Software Development Processes',
        description: 'Methodologies for effective software development',
        content: 'Software development processes provide structured approaches to building software in teams. This topic explores different methodologies for organizing software development activities. You\'ll learn about various process models (waterfall, iterative, agile) and practices (version control, code reviews, continuous integration). We\'ll implement simple tools that support these processes from scratch, including basic version control systems and continuous integration pipelines. Rather than just following established processes or using existing tools, you\'ll develop your own implementations, gaining insight into why certain approaches are effective for different kinds of projects and teams. This understanding allows you to adapt processes to specific contexts rather than applying them dogmatically.',
        parentId: 'software_eng'
    }
];

// Helper functions to work with topics
function getTopicsByParentId(parentId) {
    console.log(`getTopicsByParentId called with parentId: ${parentId}`);
    const result = topics.filter(topic => topic.parentId === parentId);
    console.log(`Found ${result.length} topics with parentId ${parentId}:`, result);
    return result;
}

function getTopicById(id) {
    return topics.find(topic => topic.id === id);
}

function searchTopics(query) {
    query = query.toLowerCase();
    return topics.filter(topic => 
        topic.title.toLowerCase().includes(query) || 
        topic.description.toLowerCase().includes(query) ||
        topic.content.toLowerCase().includes(query)
    );
}